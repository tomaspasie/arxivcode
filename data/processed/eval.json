[
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "What paper has ArXiv ID 2405.04324?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the microsoft/Swin-Transformer repository written in?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation is written in Python."
  },
  {
    "input": "What topics does the SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation cover?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What programming language is BERT: Pre-training of Deep Bidirectional Transformers implemented in?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers implementation is written in Python."
  },
  {
    "input": "Explain the salesforce/LAVIS implementation of BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What language is the bowang-lab/MedSAM repository written in?",
    "output": "The Segment Anything in Medical Images implementation is written in Jupyter Notebook."
  },
  {
    "input": "What category is the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper in?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "How popular is the MetaGPT: Meta Programming for Multi-Agent Systems implementation?",
    "output": "The FoundationAgents/MetaGPT repository has 60,328 stars and 7,416 forks on GitHub."
  },
  {
    "input": "What does the dmlc/dgl repository do?",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "What language is the zihangdai/xlnet repository written in?",
    "output": "The XLNet: Generalized Autoregressive Pretraining implementation is written in Python."
  },
  {
    "input": "What category is the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper in?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the AlpacaEval: An Automatic Evaluator for Instruction-following paper.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,924 stars, indicating high community interest."
  },
  {
    "input": "When was the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper published?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "How popular is the Grounding DINO: Marrying DINO with Grounded Pre-Training implementation?",
    "output": "The IDEA-Research/GroundingDINO repository has 9,401 stars and 976 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is available at https://github.com/OpenRLHF/OpenRLHF (OpenRLHF/OpenRLHF)."
  },
  {
    "input": "Give me an overview of Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,671 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 1609.02907?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "Explain the pyg-team/pytorch_geometric implementation of PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "What is Universal Language Model Fine-tuning for Text Classification about?",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,653 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ViT: An Image is Worth 16x16 Words implementation?",
    "output": "The google-research/vision_transformer repository has 12,102 stars and 1,428 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What programming language is Know What You Don't Know: Unanswerable Questions for SQuAD implemented in?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD implementation is written in JavaScript."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 248 stars, indicating high community interest."
  },
  {
    "input": "What is Dense Passage Retrieval for Open-Domain Question Answering about?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "Which field does InstructBLIP: Towards General-purpose Vision-Language Models belong to?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Where can I find the code for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "Where can I find the code for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "Describe the google-deepmind/dqn repository that implements Playing Atari with Deep Reinforcement Learning.",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "Give me an overview of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 362 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is available at https://github.com/RLHFlow/Directional-Preference-Alignment (RLHFlow/Directional-Preference-Alignment)."
  },
  {
    "input": "What is SQuAD: 100,000+ Questions for Machine Comprehension of Text about?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Explain the JAX: Composable transformations of Python+NumPy programs paper.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,268 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "In what year did GPT-3: Language Models are Few-Shot Learners come out?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What's the implementation repository for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "How many stars does the ALBERT: A Lite BERT repository have?",
    "output": "The google-research/albert repository has 3,275 stars and 573 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the noahshinn/reflexion repository written in?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "What programming language is Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implemented in?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "In what year did Densely Connected Convolutional Networks come out?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Universal Language Model Fine-tuning for Text Classification implemented in?",
    "output": "The Universal Language Model Fine-tuning for Text Classification implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 2002.05202?",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the tatp22/linformer-pytorch repository?",
    "output": "The tatp22/linformer-pytorch repository covers: artificial-intelligence, deep-learning, attention-mechanism, pytorch, machine-learning, linformer, paper."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Adam: A Method for Stochastic Optimization released?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2401.01335?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10"
  },
  {
    "input": "Explain the google-research/electra implementation of ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "What's the implementation repository for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.03744?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "What paper has ArXiv ID 2012.00413?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10"
  },
  {
    "input": "Explain the princeton-nlp/SimCSE implementation of SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "In what year did Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning come out?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "Explain the PKU-Alignment/align-anything implementation of arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10.",
    "output": "Align Anything: Training All-modality Model with Feedback"
  },
  {
    "input": "What is Graph Attention Networks about?",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,469 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the openai/gpt-3 repository do?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What does the google-research-datasets/natural-questions repository do?",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "What programming language is LIMA: Less Is More for Alignment implemented in?",
    "output": "The LIMA: Less Is More for Alignment implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "What is the GitHub repository for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "What category is the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper in?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is Denoising Diffusion Probabilistic Models about?",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,906 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "Give me an overview of LLaMA: Open and Efficient Foundation Language Models.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models repository have?",
    "output": "The deepspeedai/DeepSpeed repository has 40,956 stars and 4,657 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.03511.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the HuggingGPT: Solving AI Tasks with ChatGPT paper in?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is available at https://github.com/salesforce/booksum (salesforce/booksum)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What is InstructBLIP: Towards General-purpose Vision-Language Models about?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "Describe the hendrycks/test repository that implements MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "Where can I find the code for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "Which field does Time-LLM: Time Series Forecasting by Reprogramming Large Language Models belong to?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1904.12848?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "Explain the GanjinZero/RRHF implementation of arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10.",
    "output": "[NIPS2023] RRHF & Wombat"
  },
  {
    "input": "How many stars does the Llama 2: Open Foundation and Fine-Tuned Chat Models repository have?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "What category is the StarCoder: May the source be with you! paper in?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.20201?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 210 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is available at https://github.com/ZHZisZZ/modpo (ZHZisZZ/modpo)."
  },
  {
    "input": "Where can I find the code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "How popular is the Densely Connected Convolutional Networks implementation?",
    "output": "The liuzhuang13/DenseNet repository has 4,848 stars and 1,071 forks on GitHub."
  },
  {
    "input": "What language is the google-research/bert repository written in?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers implementation is written in Python."
  },
  {
    "input": "What programming language is LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implemented in?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation is written in Python."
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is available at https://github.com/zai-org/ImageReward (zai-org/ImageReward)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.13545.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the uclaml/SPIN repository do?",
    "output": "The official implementation of Self-Play Fine-Tuning (SPIN)"
  },
  {
    "input": "What is BERT: Pre-training of Deep Bidirectional Transformers about?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Generating Wikipedia by Summarizing Long Sequences paper published?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "What paper has ArXiv ID 2504.00698?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the lucidrains/muse-maskgit-pytorch implementation of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "Explain the Kwai-YuanQi/MM-RLHF implementation of arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10.",
    "output": "The Next Step Forward in Multimodal LLM Alignment"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the The Power of Scale for Parameter-Efficient Prompt Tuning implementation?",
    "output": "The google-research/prompt-tuning repository has 699 stars and 61 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 repository have?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "Give me an overview of RAG: Retrieval-Augmented Generation.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the sylinrl/TruthfulQA repository that implements arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10.",
    "output": "TruthfulQA: Measuring How Models Imitate Human Falsehoods"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Give me an overview of Decoupled Weight Decay Regularization.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What language is the rbgirshick/py-faster-rcnn repository written in?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the LIMA: Less Is More for Alignment paper published?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "Explain the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,076 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research/text-to-text-transfer-transformer repository written in?",
    "output": "The T5: Exploring the Limits of Transfer Learning implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2306.01693?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1906.08237.",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What does the siddk/voltron-robotics repository do?",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.01108?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "What is the GitHub repository for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What is the GitHub repository for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "What programming language is RetNet: Retentive Network: A Successor to Transformer implemented in?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tomekkorbak/pretraining-with-human-feedback which code accompanying the paper pretraining language models with human preferences It is written in Python. The repository has 180 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What is Billion-scale similarity search with GPUs about?",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,366 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is YOLOX: Exceeding YOLO Series in 2021 implemented in?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the allenai/RL4LMs repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "What language is the yafuly/TPO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "What year was GPT-3: Language Models are Few-Shot Learners released?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the ArXiv category of Linformer: Self-Attention with Linear Complexity?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Whisper: Robust Speech Recognition via Large-Scale Weak Supervision released?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2204.05862.",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the S4: Efficiently Modeling Long Sequences implementation cover?",
    "output": "The state-spaces/s4 repository covers: state-space-models, sequence-models, pytorch."
  },
  {
    "input": "What paper has ArXiv ID 2303.02913?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Masked Autoencoders Are Scalable Vision Learners repository have?",
    "output": "The facebookresearch/mae repository has 8,127 stars and 1,338 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.11067?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "Summarize the Horovod: fast and easy distributed deep learning in TensorFlow paper.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,642 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.00445.",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "What is Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks about?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,978 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Lookahead Optimizer: k steps forward, 1 step back implementation?",
    "output": "The alphadl/lookahead.pytorch repository has 338 stars and 64 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2108.12409?",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "Explain the FLAN: Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "What category is the SmoothQuant: Accurate and Efficient Post-Training Quantization paper in?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the horovod/horovod implementation of Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "Explain the salesforce/GeDi implementation of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "What year was Sequence to Sequence Learning with Neural Networks released?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at allenai/FineGrainedRLHF It is written in Python. The repository has 281 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.00735.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "Describe the facebookresearch/CovidPrognosis repository that implements arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "Tell me the title of ArXiv paper 1603.04467.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "What year was Neural Machine Translation by Jointly Learning to Align and Translate released?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Horovod: fast and easy distributed deep learning in TensorFlow paper.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,642 stars, indicating high community interest."
  },
  {
    "input": "Explain the LLaVA: Large Language and Vision Assistant paper.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "When was the Neural Machine Translation by Jointly Learning to Align and Translate paper published?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "Explain the microsoft/Swin-Transformer implementation of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "What language is the OptimalScale/LMFlow repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.00027?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What year was MMLU: Measuring Massive Multitask Language Understanding released?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Med-PaLM 2: Towards Expert-Level Medical Question Answering released?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "In what year did Feature Pyramid Networks for Object Detection come out?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "What's the implementation repository for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What does the NVlabs/prismer repository do?",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What category is the Universal Language Model Fine-tuning for Text Classification paper in?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is available at https://github.com/deepseek-ai/DeepSeek-Coder-V2 (deepseek-ai/DeepSeek-Coder-V2)."
  },
  {
    "input": "How many stars does the Linformer: Self-Attention with Linear Complexity repository have?",
    "output": "The tatp22/linformer-pytorch repository has 421 stars and 37 forks on GitHub."
  },
  {
    "input": "Explain the facebookresearch/faiss implementation of Billion-scale similarity search with GPUs.",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the ermongroup/ddim implementation of Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What year was SQuAD: 100,000+ Questions for Machine Comprehension of Text released?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "Give me an overview of AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,924 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Lookahead Optimizer: k steps forward, 1 step back paper in?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2111.00396.",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "What category is the Mistral 7B paper in?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2302.14233?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What language is the GAIR-NLP/LIMO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 implementation?",
    "output": "The tgxs002/align_sd repository has 293 stars and 10 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.02506.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "When was the Wide & Deep Learning for Recommender Systems paper published?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "What's the implementation repository for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.01952?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What language is the langchain-ai/langchain repository written in?",
    "output": "The MRKL Systems: A modular approach to AGI implementation is written in Python."
  },
  {
    "input": "What does the tkipf/gcn repository do?",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "What is the ArXiv category of The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the huawei-noah/Pretrained-Language-Model repository written in?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation?",
    "output": "The XiangLi1999/PrefixTuning repository has 956 stars and 164 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 implementation?",
    "output": "The deepseek-ai/DeepSeek-Coder-V2 repository has 6,281 stars and 1,015 forks on GitHub."
  },
  {
    "input": "How many stars does the Multi-Task Deep Neural Networks for Natural Language Understanding repository have?",
    "output": "The namisan/mt-dnn repository has 2,258 stars and 413 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1902.05188?",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What is the GitHub repository for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.00355?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "What language is the PKU-Alignment/align-anything repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of Enriching Word Vectors with Subword Information.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,445 stars, indicating high community interest."
  },
  {
    "input": "What is Question and Answer Test-Train Overlap in Open-Domain QA Datasets about?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,076 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 210 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What is High-Resolution Image Synthesis with Latent Diffusion Models about?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,646 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "Where can I find the code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "What is the ArXiv category of Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,680 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1609.02907.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,735 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Competition-Level Code Generation with AlphaCode paper.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.05237?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "How many stars does the Reflexion: Language Agents with Verbal Reinforcement Learning repository have?",
    "output": "The noahshinn/reflexion repository has 2,981 stars and 288 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 repository have?",
    "output": "The ContextualAI/HALOs repository has 894 stars and 50 forks on GitHub."
  },
  {
    "input": "When was the ALBERT: A Lite BERT paper published?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "Explain the Very Deep Convolutional Networks for Large-Scale Image Recognition paper.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,240 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What is the GitHub repository for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "Explain the GAIR-NLP/LIMO implementation of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What category is the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper in?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "Give me an overview of DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,785 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "Describe the stanford-futuredata/ColBERT repository that implements ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "Explain the openai/whisper implementation of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Describe the PKU-Alignment/align-anything repository that implements arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10.",
    "output": "Align Anything: Training All-modality Model with Feedback"
  },
  {
    "input": "What is the GitHub repository for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.19393.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "Describe the state-spaces/mamba repository that implements Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "Summarize the LIMA: Less Is More for Alignment paper.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.05237.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.01469?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "When was the InPars: Data Augmentation for Information Retrieval paper published?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Generating Wikipedia by Summarizing Long Sequences implemented in?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences implementation is written in Python."
  },
  {
    "input": "Which field does Longformer: The Long-Document Transformer belong to?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the jasonwei20/eda_nlp repository that implements EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "How many stars does the Segment Anything in Medical Images repository have?",
    "output": "The bowang-lab/MedSAM repository has 3,974 stars and 546 forks on GitHub."
  },
  {
    "input": "What are the key topics in the google/trax repository?",
    "output": "The google/trax repository covers: jax, numpy, deep-learning, deep-reinforcement-learning, machine-learning, transformer, reinforcement-learning."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "When was the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper published?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper in?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What category is the Self-RAG: Learning to Retrieve, Generate, and Critique paper in?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the facebookresearch/contriever implementation of Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "Tell me the title of ArXiv paper 1909.11942.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Playing Atari with Deep Reinforcement Learning paper.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "When was the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper published?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "Summarize the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "Give me an overview of FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2204.02311.",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "What paper has ArXiv ID 2501.11067?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the THUDM/P-tuning-v2 repository do?",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "What topics does the TensorFlow: A System for Large-Scale Machine Learning implementation cover?",
    "output": "The tensorflow/tensorflow repository covers: tensorflow, machine-learning, python, deep-learning, deep-neural-networks, neural-network, ml, distributed."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.04023.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "How many stars does the Vicuna: An Open-Source Chatbot repository have?",
    "output": "The lm-sys/FastChat repository has 39,298 stars and 4,778 forks on GitHub."
  },
  {
    "input": "Give me an overview of ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,415 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation?",
    "output": "The uber/orbit repository has 2,021 stars and 141 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2403.17887.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "Describe the AlibabaResearch/DAMO-ConvAI repository that implements arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10.",
    "output": "DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI."
  },
  {
    "input": "What language is the clarkkev/attention-analysis repository written in?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model implementation is written in Jupyter Notebook."
  },
  {
    "input": "What programming language is RoBERTa: A Robustly Optimized BERT Pretraining Approach implemented in?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "In what year did Finetuned Language Models Are Zero-Shot Learners come out?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1606.05250?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "What does the KaimingHe/deep-residual-networks repository do?",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What year was EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification released?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "In what year did LLaMA-Adapter: Efficient Fine-tuning of Language Models come out?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "What paper has ArXiv ID 2006.04768?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "Describe the tomekkorbak/pretraining-with-human-feedback repository that implements arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10.",
    "output": "Code accompanying the paper Pretraining Language Models with Human Preferences"
  },
  {
    "input": "What's the implementation repository for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "What language is the rajpurkar/SQuAD-explorer repository written in?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD implementation is written in JavaScript."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Dense Passage Retrieval for Open-Domain Question Answering belong to?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01751.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "What category is the Dense Passage Retrieval for Open-Domain Question Answering paper in?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the HuggingGPT: Solving AI Tasks with ChatGPT paper published?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "What is Gorilla: Large Language Model Connected with Massive APIs about?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,603 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the microsoft/unilm repository?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.02506?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "Which field does JAX: Composable transformations of Python+NumPy programs belong to?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Describe the facebookresearch/contriever repository that implements Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "What year was Question and Answer Test-Train Overlap in Open-Domain QA Datasets released?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of ViT: An Image is Worth 16x16 Words?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ZHZisZZ/modpo which [acl'24] beyond one-preference-fits-all alignment: multi-objective direct preference optimization It is written in Python. The repository has 93 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1903.07293.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "Describe the deepspeedai/DeepSpeed repository that implements ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Explain the tatp22/linformer-pytorch implementation of Linformer: Self-Attention with Linear Complexity.",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "What category is the AudioPaLM: A Large Language Model for Speech Understanding paper in?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the frankxu2004/knnlm-why repository do?",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What does the uber/orbit repository do?",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the microsoft/LoRA repository?",
    "output": "The microsoft/LoRA repository covers: gpt-2, adaptation, language-model, gpt-3, low-rank, pytorch, deep-learning, roberta, deberta, lora."
  },
  {
    "input": "What's the implementation repository for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "In what year did Mistral 7B come out?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation is written in Jupyter Notebook."
  },
  {
    "input": "When was the Deep Residual Learning for Image Recognition paper published?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "Where can I find the code for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2412.15838?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10"
  },
  {
    "input": "Which field does Efficient Estimation of Word Representations in Vector Space belong to?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.00923.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "Explain the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1707.06347?",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "What is the GitHub repository for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "How popular is the Reformer: The Efficient Transformer implementation?",
    "output": "The google/trax repository has 8,294 stars and 827 forks on GitHub."
  },
  {
    "input": "What programming language is MMLU: Measuring Massive Multitask Language Understanding implemented in?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Denoising Diffusion Implicit Models?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "What is the ArXiv category of FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was ColBERT: Efficient Passage Search via Contextualized Late Interaction released?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 1804.09301?",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What is the GitHub repository for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2208.01618?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "What language is the rail-berkeley/softlearning repository written in?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.13545?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "How popular is the Contriever: Unsupervised Dense Information Retrieval implementation?",
    "output": "The facebookresearch/contriever repository has 766 stars and 69 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.13007.",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "What paper has ArXiv ID 2406.07394?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "Explain the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "In what year did Reflexion: Language Agents with Verbal Reinforcement Learning come out?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.14815?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation?",
    "output": "The JohnNay/llm-lobbyist repository has 172 stars and 15 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is available at https://github.com/yafuly/TPO (yafuly/TPO)."
  },
  {
    "input": "What does the anthropics/hh-rlhf repository do?",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What programming language is InPars: Data Augmentation for Information Retrieval implemented in?",
    "output": "The InPars: Data Augmentation for Information Retrieval implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2308.12950.",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the DreamFusion: Text-to-3D using 2D Diffusion paper in?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What are the key topics in the tensorflow/tensorflow repository?",
    "output": "The tensorflow/tensorflow repository covers: tensorflow, machine-learning, python, deep-learning, deep-neural-networks, neural-network, ml, distributed."
  },
  {
    "input": "Where can I find the code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 233 stars, indicating high community interest."
  },
  {
    "input": "Explain the Dream to Control: Learning Behaviors by Latent Imagination paper.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.02155?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10"
  },
  {
    "input": "What programming language is Feature Pyramid Networks for Object Detection implemented in?",
    "output": "The Feature Pyramid Networks for Object Detection implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "What year was Faster R-CNN: Towards Real-Time Object Detection released?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "What programming language is Sequence to Sequence Learning with Neural Networks implemented in?",
    "output": "The Sequence to Sequence Learning with Neural Networks implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Inductive Representation Learning on Large Graphs paper published?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "What year was LLaVA-1.5: Improved Baselines with Visual Instruction Tuning released?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What category is the Dream to Control: Learning Behaviors by Latent Imagination paper in?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Flamingo: A Visual Language Model for Few-Shot Learning implemented in?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the princeton-nlp/SimCSE repository do?",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "Give me an overview of Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2302.13971?",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2404.03592?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 implementation?",
    "output": "The AlibabaResearch/DAMO-ConvAI repository has 1,508 stars and 236 forks on GitHub."
  },
  {
    "input": "Give me an overview of Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,150 stars, indicating high community interest."
  },
  {
    "input": "What does the lllyasviel/ControlNet repository do?",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "Describe the Megvii-BaseDetection/YOLOX repository that implements YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2212.03533?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "What year was YOLOX: Exceeding YOLO Series in 2021 released?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation?",
    "output": "The linkedin/gdmix repository has 133 stars and 18 forks on GitHub."
  },
  {
    "input": "What programming language is Denoising Diffusion Implicit Models implemented in?",
    "output": "The Denoising Diffusion Implicit Models implementation is written in Python."
  },
  {
    "input": "Where can I find the code for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.06161?",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at sylinrl/TruthfulQA which truthfulqa: measuring how models imitate human falsehoods It is written in Jupyter Notebook. The repository has 854 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at salesforce/booksum It is written in Python. The repository has 195 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.01061?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "What year was StarCoder: May the source be with you! released?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "What category is the YOLOX: Exceeding YOLO Series in 2021 paper in?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Summarize the Feature Pyramid Networks for Object Detection paper.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Inductive Representation Learning on Large Graphs implementation?",
    "output": "The williamleif/GraphSAGE repository has 3,636 stars and 849 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was Proximal Policy Optimization Algorithms released?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "Explain the BLIP-2: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "What language is the haotian-liu/LLaVA repository written in?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "What does the google-research/bert repository do?",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What is the GitHub repository for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "Describe the huggingface/sentence-transformers repository that implements Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "How popular is the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation?",
    "output": "The openai/whisper repository has 91,793 stars and 11,506 forks on GitHub."
  },
  {
    "input": "In what year did DistilBERT: a distilled version of BERT come out?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 2102.01192.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "Explain the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,794 stars, indicating high community interest."
  },
  {
    "input": "What category is the MRKL Systems: A modular approach to AGI paper in?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What is the GitHub repository for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Explain the The Power of Scale for Parameter-Efficient Prompt Tuning paper.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 699 stars, indicating high community interest."
  },
  {
    "input": "When was the Multi-Task Deep Neural Networks for Natural Language Understanding paper published?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "Give me an overview of Attention Is All You Need.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,800 stars, indicating high community interest."
  },
  {
    "input": "Which field does Flamingo: A Visual Language Model for Few-Shot Learning belong to?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the rajpurkar/SQuAD-explorer repository that implements SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 repository have?",
    "output": "The PKU-Alignment/align-anything repository has 4,601 stars and 506 forks on GitHub."
  },
  {
    "input": "When was the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper published?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "What year was Reflexion: Language Agents with Verbal Reinforcement Learning released?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 repository have?",
    "output": "The GAIR-NLP/LIMO repository has 1,054 stars and 52 forks on GitHub."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the ArXiv category of RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1603.04467?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "Explain the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,410 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Online-RLHF which a recipe for online rlhf and online iterative dpo. It is written in Python. The repository has 537 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 repository have?",
    "output": "The google-deepmind/tracr repository has 548 stars and 47 forks on GitHub."
  },
  {
    "input": "Explain the allenai/longformer implementation of Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "How many stars does the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows repository have?",
    "output": "The microsoft/Swin-Transformer repository has 15,517 stars and 2,202 forks on GitHub."
  },
  {
    "input": "What year was Depthwise Separable Convolutions for Neural Machine Translation released?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/evals The repository has 315 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "What paper has ArXiv ID 2302.09778?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "What does the oughtinc/ice repository do?",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "In what year did EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification come out?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "Give me an overview of DreamBooth: Fine Tuning Text-to-Image Diffusion Models.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 1,005 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.07459?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "Summarize the Neural Machine Translation by Jointly Learning to Align and Translate paper.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.11511?",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "What category is the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper in?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.11305.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 repository have?",
    "output": "The tomekkorbak/pretraining-with-human-feedback repository has 180 stars and 13 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "Explain the siddk/voltron-robotics implementation of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "What is the GitHub repository for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2107.07346.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is available at https://github.com/anthropics/evals (anthropics/evals)."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 1708.05031?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What language is the lucidrains/muse-maskgit-pytorch repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "Give me an overview of HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,488 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the nyu-mll/jiant repository written in?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "When was the Linformer: Self-Attention with Linear Complexity paper published?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What paper has ArXiv ID 2212.09251?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,415 stars, indicating high community interest."
  },
  {
    "input": "Describe the google-research/prompt-tuning repository that implements The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "Summarize the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Explain the nlpxucan/WizardLM implementation of WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is available at https://github.com/anthropics/evals (anthropics/evals)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the Linear95/APO repository do?",
    "output": "Code for ACL2024 paper - Adversarial Preference Optimization (APO)."
  },
  {
    "input": "What programming language is Dream to Control: Learning Behaviors by Latent Imagination implemented in?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2107.07346?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 repository have?",
    "output": "The AlibabaResearch/DAMO-ConvAI repository has 1,508 stars and 236 forks on GitHub."
  },
  {
    "input": "What programming language is Reformer: The Efficient Transformer implemented in?",
    "output": "The Reformer: The Efficient Transformer implementation is written in Python."
  },
  {
    "input": "Give me an overview of Adam: A Method for Stochastic Optimization.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "How many stars does the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What category is the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper in?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Where can I find the code for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "When was the LoRA: Low-Rank Adaptation of Large Language Models paper published?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "Describe the state-spaces/s4 repository that implements S4: Efficiently Modeling Long Sequences.",
    "output": "Structured state space sequence models"
  },
  {
    "input": "What is the GitHub repository for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What's the implementation repository for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "What is Enriching Word Vectors with Subword Information about?",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,445 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did MRKL Systems: A modular approach to AGI come out?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.22708.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "What year was WizardCoder: Empowering Code LLMs with Evol-Instruct released?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "What is LLaVA: Large Language and Vision Assistant about?",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.06326?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What is the GitHub repository for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "How many stars does the AudioPaLM: A Large Language Model for Speech Understanding repository have?",
    "output": "The lucidrains/audiolm-pytorch repository has 2,610 stars and 280 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What programming language is S4: Efficiently Modeling Long Sequences implemented in?",
    "output": "The S4: Efficiently Modeling Long Sequences implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 2211.10438?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1608.06993?",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the oughtinc/ice implementation of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does SmoothQuant: Accurate and Efficient Post-Training Quantization belong to?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 repository have?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.03059?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  }
]