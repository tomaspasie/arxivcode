[
  {
    "input": "What is the title of the paper with ArXiv ID 1301.3781?",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "What is Mamba: Linear-Time Sequence Modeling with Selective State Spaces about?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,662 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the ReAct: Synergizing Reasoning and Acting in Language Models repository have?",
    "output": "The ysymyth/ReAct repository has 3,264 stars and 331 forks on GitHub."
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.11366.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 implementation?",
    "output": "The openai/following-instructions-human-feedback repository has 1,251 stars and 146 forks on GitHub."
  },
  {
    "input": "How many stars does the Wide & Deep Learning for Recommender Systems repository have?",
    "output": "The tensorflow/models repository has 77,680 stars and 45,382 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2505.23006?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "Describe the allenai/codescientist repository that implements arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What programming language is Gorilla: Large Language Model Connected with Massive APIs implemented in?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 1801.01290?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "Explain the ReAct: Synergizing Reasoning and Acting in Language Models paper.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,264 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is LLM.int8(): 8-bit Matrix Multiplication for Transformers implemented in?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation is written in Python."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the FlashAttention: Fast and Memory-Efficient Exact Attention paper published?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.04104?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "Describe the microsoft/unilm repository that implements Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2204.02311?",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "What topics does the Universal Language Model Fine-tuning for Text Classification implementation cover?",
    "output": "The fastai/fastai repository covers: deep-learning, machine-learning, pytorch, python, gpu, fastai, notebooks, colab."
  },
  {
    "input": "What paper has ArXiv ID 2107.03374?",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.06767.",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10"
  },
  {
    "input": "Describe the lucidrains/PaLM-pytorch repository that implements PaLM: Scaling Language Modeling with Pathways.",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Linformer: Self-Attention with Linear Complexity paper in?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.19413?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Mistral 7B?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the rajpurkar/SQuAD-explorer implementation of Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "When was the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper published?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2503.22708?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "What category is the Attention Is All You Need paper in?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the bigcode-project/starcoder repository that implements StarCoder: May the source be with you!.",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "Explain the QLoRA: Efficient Finetuning of Quantized LLMs paper.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,782 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Denoising Diffusion Implicit Models paper.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,752 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "In what year did Time-LLM: Time Series Forecasting by Reprogramming Large Language Models come out?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "Describe the microsoft/torchscale repository that implements RetNet: Retentive Network: A Successor to Transformer.",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "What category is the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper in?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation?",
    "output": "The google-research/pegasus repository has 1,645 stars and 317 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "When was the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper published?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "What are the key topics in the uclaml/SPIN repository?",
    "output": "The uclaml/SPIN repository covers: deep-learning, fine-tuning, large-language-models, self-play."
  },
  {
    "input": "How popular is the Deep Residual Learning for Image Recognition implementation?",
    "output": "The KaimingHe/deep-residual-networks repository has 6,671 stars and 2,232 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 repository have?",
    "output": "The salesforce/GeDi repository has 210 stars and 45 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is available at https://github.com/yafuly/TPO (yafuly/TPO)."
  },
  {
    "input": "What are the key topics in the princeton-nlp/SimCSE repository?",
    "output": "The princeton-nlp/SimCSE repository covers: nlp, sentence-embeddings."
  },
  {
    "input": "Describe the google-research/google-research repository that implements Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Google Research"
  },
  {
    "input": "Where can I find the code for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What is HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction about?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Analyzing the Structure of Attention in a Transformer Language Model implementation?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "In what year did SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding come out?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What topics does the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation cover?",
    "output": "The microsoft/Swin-Transformer repository covers: swin-transformer, image-classification, object-detection, semantic-segmentation, imagenet, mscoco, ade20k, mask-rcnn."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The LIMA: Less Is More for Alignment implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was High-Resolution Image Synthesis with Latent Diffusion Models released?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 repository have?",
    "output": "The rll-research/BPref repository has 133 stars and 31 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "When was the Efficient Estimation of Word Representations in Vector Space paper published?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 repository have?",
    "output": "The allenai/codescientist repository has 303 stars and 40 forks on GitHub."
  },
  {
    "input": "Which field does Neural Message Passing for Quantum Chemistry belong to?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at zai-org/ImageReward which [neurips 2023] imagereward: learning and evaluating human preferences for text-to-image generation It is written in Python. The repository has 1,594 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "When was the Semi-Supervised Classification with Graph Convolutional Networks paper published?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "Explain the Synthetic Data Generation with Large Language Models paper.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "In what year did SQuAD: 100,000+ Questions for Machine Comprehension of Text come out?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "What category is the BLIP: Bootstrapping Language-Image Pre-training paper in?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Give me an overview of InPars: Data Augmentation for Information Retrieval.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "What category is the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper in?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Finetuned Language Models Are Zero-Shot Learners belong to?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Billion-scale similarity search with GPUs paper in?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What year was Time-LLM: Time Series Forecasting by Reprogramming Large Language Models released?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "What are the key topics in the microsoft/torchscale repository?",
    "output": "The microsoft/torchscale repository covers: computer-vision, machine-learning, multimodal, natural-language-processing, pretrained-language-model, speech-processing, transformer, translation."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.10862?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.07830.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "When was the MetaGPT: Meta Programming for Multi-Agent Systems paper published?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2001.04451?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00704?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2505.22954?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "Describe the QingruZhang/AdaLoRA repository that implements AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What language is the TsinghuaAI/CPM-1-Generate repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What year was Attention Is All You Need released?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "In what year did Self-RAG: Learning to Retrieve, Generate, and Critique come out?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "Give me an overview of ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,264 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "What paper has ArXiv ID 2302.13007?",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What paper has ArXiv ID 2006.16779?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "Describe the rajpurkar/SQuAD-explorer repository that implements Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.17492?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the haotian-liu/LLaVA repository?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "What programming language is Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implemented in?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "What paper has ArXiv ID 2504.20734?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.15334?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What's the implementation repository for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "What year was BERT Rediscovers the Classical NLP Pipeline released?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2502.19328?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01181.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "Explain the sylinrl/TruthfulQA implementation of arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10.",
    "output": "TruthfulQA: Measuring How Models Imitate Human Falsehoods"
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.14165.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2304.15004?",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "Summarize the BERT: Pre-training of Deep Bidirectional Transformers paper.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "Explain the zihangdai/xlnet implementation of XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "Explain the RLHFlow/Online-RLHF implementation of arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10.",
    "output": "A recipe for online RLHF and online iterative DPO."
  },
  {
    "input": "What paper has ArXiv ID 2007.16122?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "Explain the facebookresearch/CovidPrognosis implementation of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "How many stars does the Gorilla: Large Language Model Connected with Massive APIs repository have?",
    "output": "The ShishirPatil/gorilla repository has 12,603 stars and 1,294 forks on GitHub."
  },
  {
    "input": "Explain the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,674 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at PKU-Alignment/align-anything which align anything: training all-modality model with feedback It is written in Python. The repository has 4,601 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "Explain the IDEA-Research/GroundingDINO implementation of Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation?",
    "output": "The sentient-agi/OpenDeepSearch repository has 3,735 stars and 343 forks on GitHub."
  },
  {
    "input": "Explain the lm-sys/FastChat implementation of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1807.11164?",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "Summarize the Very Deep Convolutional Networks for Large-Scale Image Recognition paper.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,240 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What paper has ArXiv ID 2305.07185?",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "What year was Gorilla: Large Language Model Connected with Massive APIs released?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.00687.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "What category is the EfficientDet: Scalable and Efficient Object Detection paper in?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "When was the Adam: A Method for Stochastic Optimization paper published?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "What language is the facebookresearch/contriever repository written in?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval implementation is written in Python."
  },
  {
    "input": "Give me an overview of Competition-Level Code Generation with AlphaCode.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "Explain the deepseek-ai/DeepSeek-Coder-V2 implementation of arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10.",
    "output": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"
  },
  {
    "input": "What paper has ArXiv ID 1312.5602?",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "What paper has ArXiv ID 2301.00303?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Feature Pyramid Networks for Object Detection implementation?",
    "output": "The facebookresearch/Detectron repository has 26,396 stars and 5,435 forks on GitHub."
  },
  {
    "input": "Describe the KimMeen/Time-LLM repository that implements Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2402.18571.",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What is the GitHub repository for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,021 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "How popular is the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation?",
    "output": "The google-research/vision_transformer repository has 12,102 stars and 1,428 forks on GitHub."
  },
  {
    "input": "How many stars does the Competition-Level Code Generation with AlphaCode repository have?",
    "output": "The google-deepmind/code_contests repository has 2,177 stars and 223 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.09685.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "In what year did DreamFusion: Text-to-3D using 2D Diffusion come out?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the High-Resolution Image Synthesis with Latent Diffusion Models paper in?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.12115?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.09778.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "What language is the Crista23/goal_directedness_llms repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the google-research/vision_transformer repository written in?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation is written in Jupyter Notebook."
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What category is the Semi-Supervised Classification with Graph Convolutional Networks paper in?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the facebookresearch/DPR implementation of RAG: Retrieval-Augmented Generation.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "Where can I find the code for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is available at https://github.com/arcee-ai/mergekit (arcee-ai/mergekit)."
  },
  {
    "input": "What language is the facebookresearch/TransCoder repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at salesforce/booksum It is written in Python. The repository has 195 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01181?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "What year was Playing Atari with Deep Reinforcement Learning released?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "What paper has ArXiv ID 1910.13461?",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "When was the Reformer: The Efficient Transformer paper published?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,377 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What's the implementation repository for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "What paper has ArXiv ID 2210.01241?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implemented in?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "In what year did Zephyr: Direct Distillation of LM Alignment come out?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 919 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the RoFormer: Enhanced Transformer with Rotary Position Embedding repository have?",
    "output": "The ZhuiyiTechnology/roformer repository has 1,059 stars and 59 forks on GitHub."
  },
  {
    "input": "What is Mistral 7B about?",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,577 stars, indicating high community interest."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What programming language is QLoRA: Efficient Finetuning of Quantized LLMs implemented in?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ZHZisZZ/modpo which [acl'24] beyond one-preference-fits-all alignment: multi-objective direct preference optimization It is written in Python. The repository has 93 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.12652?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Inductive Representation Learning on Large Graphs repository have?",
    "output": "The williamleif/GraphSAGE repository has 3,636 stars and 849 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What programming language is PyTorch Geometric: Library for Geometric Deep Learning implemented in?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning implementation is written in Python."
  },
  {
    "input": "Which field does PyTorch: An Imperative Style, High-Performance Deep Learning Library belong to?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.02783?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "What category is the MetaGPT: Meta Programming for Multi-Agent Systems paper in?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "Describe the zai-org/ImageReward repository that implements arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10.",
    "output": "[NeurIPS 2023] ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation"
  },
  {
    "input": "What is FlashAttention: Fast and Memory-Efficient Exact Attention about?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "In what year did MMLU: Measuring Massive Multitask Language Understanding come out?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "What category is the InPars: Data Augmentation for Information Retrieval paper in?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.08774.",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is available at https://github.com/OptimalScale/LMFlow (OptimalScale/LMFlow)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the hojonathanho/diffusion repository that implements Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the BERT: Pre-training of Deep Bidirectional Transformers implementation cover?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "What paper has ArXiv ID 2311.03099?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10"
  },
  {
    "input": "Describe the google-research/google-research repository that implements Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Google Research"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,021 stars, indicating high community interest."
  },
  {
    "input": "Describe the allenai/RL4LMs repository that implements arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10.",
    "output": "A modular RL library to fine-tune language models to human preferences"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.09778?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the ashawkey/stable-dreamfusion repository that implements DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "What year was CLIP: Learning Transferable Visual Models From Natural Language released?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 repository have?",
    "output": "The deepseek-ai/DeepSeek-Coder-V2 repository has 6,281 stars and 1,015 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "When was the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper published?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.09864?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "What does the google/BIG-bench repository do?",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the ZHZisZZ/modpo repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.12597.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What topics does the PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is TinyBERT: Distilling BERT for Natural Language Understanding about?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,150 stars, indicating high community interest."
  },
  {
    "input": "Explain the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,069 stars, indicating high community interest."
  },
  {
    "input": "Which field does Deep Graph Library: A Graph-Centric, Highly-Performant Package belong to?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the FoundationAgents/MetaGPT repository do?",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What language is the allenai/longformer repository written in?",
    "output": "The Longformer: The Long-Document Transformer implementation is written in Python."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2206.01861?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the facebookresearch/Detectron repository that implements Feature Pyramid Networks for Object Detection.",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at openai/following-instructions-human-feedback The repository has 1,251 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Wide & Deep Learning for Recommender Systems come out?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "What is the GitHub repository for ChemCrow: Augmenting large-language models with chemistry tools?",
    "output": "The code for ChemCrow: Augmenting large-language models with chemistry tools is available at https://github.com/ur-whitelab/chemcrow-public (ur-whitelab/chemcrow-public)."
  },
  {
    "input": "Summarize the Synthetic Data Generation with Large Language Models paper.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What paper has ArXiv ID 2504.16084?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale come out?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.07394?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is available at https://github.com/sylinrl/TruthfulQA (sylinrl/TruthfulQA)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the FMInference/FlexLLMGen repository?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What is the ArXiv category of MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Explain the google-research/albert implementation of ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 919 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2405.02803?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "What does the tatsu-lab/alpaca_eval repository do?",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "What does the haotian-liu/LLaVA repository do?",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The Adam: A Method for Stochastic Optimization implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.00445?",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "In what year did InstructBLIP: Towards General-purpose Vision-Language Models come out?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "Summarize the LLaVA: Large Language and Vision Assistant paper.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "When was the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper published?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "What is the ArXiv category of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is GPT-3: Language Models are Few-Shot Learners about?",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,774 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the allenai/RL4LMs repository?",
    "output": "The allenai/RL4LMs repository covers: language-modeling, nlp, reinforcement-learning, dialogue-generation, machine-translation, natural-language-processing, summarization, table-to-text, text-generation."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.01947.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2005.11401?",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "Give me an overview of Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,848 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1412.6980?",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "How popular is the Llama 2: Open Foundation and Fine-Tuned Chat Models implementation?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.00355.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.13188.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "What programming language is HuggingGPT: Solving AI Tasks with ChatGPT implemented in?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2305.18290?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the rail-berkeley/softlearning repository?",
    "output": "The rail-berkeley/softlearning repository covers: reinforcement-learning, soft-actor-critic, deep-learning, deep-reinforcement-learning, deep-neural-networks, machine-learning."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 implementation?",
    "output": "The OptimalScale/LMFlow repository has 8,489 stars and 834 forks on GitHub."
  },
  {
    "input": "What category is the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper in?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is available at https://github.com/PKU-Alignment/align-anything (PKU-Alignment/align-anything)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is available at https://github.com/Kwai-YuanQi/MM-RLHF (Kwai-YuanQi/MM-RLHF)."
  },
  {
    "input": "What is Neural Message Passing for Quantum Chemistry about?",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 344 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the LoRA: Low-Rank Adaptation of Large Language Models paper.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 13,038 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Codex: Evaluating Large Language Models Trained on Code?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What does the QingruZhang/AdaLoRA repository do?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "Describe the facebookresearch/segment-anything repository that implements Segment Anything.",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.23006?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 1905.07830?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "Describe the google-research/albert repository that implements ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "What language is the frankxu2004/knnlm-why repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2308.08155.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "Summarize the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 1,005 stars, indicating high community interest."
  },
  {
    "input": "What does the PetarV-/GAT repository do?",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the ContextualAI/HALOs implementation of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What paper has ArXiv ID 2205.05198?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "What topics does the SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation cover?",
    "output": "The princeton-nlp/SimCSE repository covers: nlp, sentence-embeddings."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What does the lisa-groundhog/GroundHog repository do?",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the PaLM: Scaling Language Modeling with Pathways implementation?",
    "output": "The lucidrains/PaLM-pytorch repository has 827 stars and 82 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "Describe the openai/whisper repository that implements Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Synthetic Data Generation with Large Language Models repository have?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "What year was InPars: Data Augmentation for Information Retrieval released?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "What year was Masked Autoencoders Are Scalable Vision Learners released?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "Summarize the XLNet: Generalized Autoregressive Pretraining paper.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,181 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "What does the CompVis/latent-diffusion repository do?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "Which field does Codex: Evaluating Large Language Models Trained on Code belong to?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2001.04451.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Summarize the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "Describe the anthropics/hh-rlhf repository that implements arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10.",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What paper has ArXiv ID 2501.13824?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "When was the Llama 2: Open Foundation and Fine-Tuned Chat Models paper published?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 implementation cover?",
    "output": "The tomekkorbak/pretraining-with-human-feedback repository covers: ai-alignment, ai-safety, decision-transformers, gpt, language-models, pretraining, reinforcement-learning, rlhf."
  },
  {
    "input": "In what year did TinyBERT: Distilling BERT for Natural Language Understanding come out?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Wide & Deep Learning for Recommender Systems implemented in?",
    "output": "The Wide & Deep Learning for Recommender Systems implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Describe the ur-whitelab/chemcrow-public repository that implements ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "Chemcrow"
  },
  {
    "input": "Which field does Playing Atari with Deep Reinforcement Learning belong to?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Lookahead Optimizer: k steps forward, 1 step back repository have?",
    "output": "The alphadl/lookahead.pytorch repository has 338 stars and 64 forks on GitHub."
  },
  {
    "input": "When was the Code Llama: Open Foundation Models for Code paper published?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at sylinrl/TruthfulQA which truthfulqa: measuring how models imitate human falsehoods It is written in Jupyter Notebook. The repository has 854 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "What paper has ArXiv ID 1901.02860?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "What is S4: Efficiently Modeling Long Sequences about?",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,794 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is available at https://github.com/allenai/FineGrainedRLHF (allenai/FineGrainedRLHF)."
  },
  {
    "input": "Explain the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,978 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.01652.",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What programming language is AlpacaEval: An Automatic Evaluator for Instruction-following implemented in?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the ArXiv category of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is ALBERT: A Lite BERT about?",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,275 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Explain the GPT-4 Technical Report paper.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,406 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 implementation?",
    "output": "The zai-org/ImageReward repository has 1,594 stars and 83 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.11844.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What is the ArXiv category of HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Which field does Segment Anything in Medical Images belong to?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 implementation?",
    "output": "The salesforce/booksum repository has 195 stars and 35 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "What does the stanford-futuredata/ColBERT repository do?",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "What's the implementation repository for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "In what year did Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context come out?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 implementation?",
    "output": "The OpenRLHF/OpenRLHF repository has 8,562 stars and 826 forks on GitHub."
  },
  {
    "input": "What is DreamBooth: Fine Tuning Text-to-Image Diffusion Models about?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 1,005 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of MRKL Systems: A modular approach to AGI.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 121,514 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is available at https://github.com/RLHFlow/Directional-Preference-Alignment (RLHFlow/Directional-Preference-Alignment)."
  },
  {
    "input": "What does the ashawkey/stable-dreamfusion repository do?",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 repository have?",
    "output": "The RLHFlow/Online-RLHF repository has 537 stars and 49 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.04856?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.12824.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "What is ViT: An Image is Worth 16x16 Words about?",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "Explain the Linformer: Self-Attention with Linear Complexity paper.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,645 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "What is the GitHub repository for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Explain the lamini-ai/Lamini-Memory-Tuning implementation of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What is the ArXiv category of Distilling the Knowledge in a Neural Network?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What are the key topics in the bitsandbytes-foundation/bitsandbytes repository?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository covers: llm, machine-learning, pytorch, qlora, quantization."
  },
  {
    "input": "What is the ArXiv category of Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the InstructBLIP: Towards General-purpose Vision-Language Models paper published?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning repository have?",
    "output": "The QingruZhang/AdaLoRA repository has 362 stars and 37 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2109.04908?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at allenai/FineGrainedRLHF It is written in Python. The repository has 281 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the langchain-ai/langchain repository that implements MRKL Systems: A modular approach to AGI.",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "Give me an overview of Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,262 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.08518.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "How many stars does the CLIP: Learning Transferable Visual Models From Natural Language repository have?",
    "output": "The openai/CLIP repository has 31,905 stars and 3,859 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What category is the JAX: Composable transformations of Python+NumPy programs paper in?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What are the key topics in the haitongli/knowledge-distillation-pytorch repository?",
    "output": "The haitongli/knowledge-distillation-pytorch repository covers: pytorch, knowledge-distillation, deep-neural-networks, cifar10, model-compression, dark-knowledge, computer-vision."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 repository have?",
    "output": "The allenai/FineGrainedRLHF repository has 281 stars and 22 forks on GitHub."
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "How many stars does the T5: Exploring the Limits of Transfer Learning repository have?",
    "output": "The google-research/text-to-text-transfer-transformer repository has 6,458 stars and 788 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.12895?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10"
  },
  {
    "input": "What programming language is Code Llama: Open Foundation Models for Code implemented in?",
    "output": "The Code Llama: Open Foundation Models for Code implementation is written in Python."
  },
  {
    "input": "How many stars does the GPT-3: Language Models are Few-Shot Learners repository have?",
    "output": "The openai/gpt-3 repository has 15,774 stars and 2,290 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1409.0473?",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the microsoft/unilm implementation of Text Embeddings by Weakly-Supervised Contrastive Pre-training.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What does the NVlabs/prismer repository do?",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What topics does the EfficientDet: Scalable and Efficient Object Detection implementation cover?",
    "output": "The google/automl repository covers: automl, efficientdet, object-detection, efficientnet, efficientnetv2."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.05556?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of RetNet: Retentive Network: A Successor to Transformer?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation?",
    "output": "The Shark-NLP/OpenICL repository has 582 stars and 30 forks on GitHub."
  },
  {
    "input": "What are the key topics in the lucidrains/muse-maskgit-pytorch repository?",
    "output": "The lucidrains/muse-maskgit-pytorch repository covers: artificial-intelligence, deep-learning, text-to-image, attention-mechanisms, transformers."
  },
  {
    "input": "What programming language is Denoising Diffusion Probabilistic Models implemented in?",
    "output": "The Denoising Diffusion Probabilistic Models implementation is written in Python."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "What programming language is Analyzing the Structure of Attention in a Transformer Language Model implemented in?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.17437?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "Summarize the Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the ALBERT: A Lite BERT paper.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,275 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1409.1556.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "Explain the google/trax implementation of Reformer: The Efficient Transformer.",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2007.16122.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "Which field does Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks belong to?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2111.00396?",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "Explain the openai/CLIP implementation of CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "Explain the Vision-CAIR/MiniGPT-4 implementation of MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.10592?",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 210 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2109.01652?",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.10391?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation?",
    "output": "The allenai/codescientist repository has 303 stars and 40 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2203.08913?",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What category is the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper in?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the QingruZhang/AdaLoRA implementation of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). "
  },
  {
    "input": "Summarize the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What is Code Llama: Open Foundation Models for Code about?",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is Competition-Level Code Generation with AlphaCode about?",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "Describe the farizrahman4u/seq2seq repository that implements Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "What's the implementation repository for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Give me an overview of T5: Exploring the Limits of Transfer Learning.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,458 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,076 stars, indicating high community interest."
  },
  {
    "input": "Describe the tensorflow/tensorflow repository that implements TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "Explain the rll-research/BPref implementation of arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10.",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "What category is the Densely Connected Convolutional Networks paper in?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What's the implementation repository for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "Explain the lisa-groundhog/GroundHog implementation of Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "What's the implementation repository for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "In what year did The Flan Collection: Designing Data and Methods for Effective Instruction Tuning come out?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Explain the tgxs002/align_sd implementation of arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10.",
    "output": "Better Aligning Text-to-Image Models with Human Preference. ICCV 2023"
  },
  {
    "input": "What is BERT Rediscovers the Classical NLP Pipeline about?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What does the huggingface/sentence-transformers repository do?",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "What paper has ArXiv ID 2305.06500?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "What is the ArXiv category of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Proximal Policy Optimization Algorithms about?",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,588 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.23513?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Explain the OptimalScale/LMFlow implementation of arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10.",
    "output": "An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.14420?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.22678?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "How popular is the Neural Machine Translation by Jointly Learning to Align and Translate implementation?",
    "output": "The lisa-groundhog/GroundHog repository has 613 stars and 230 forks on GitHub."
  },
  {
    "input": "How many stars does the SQuAD: 100,000+ Questions for Machine Comprehension of Text repository have?",
    "output": "The rajpurkar/SQuAD-explorer repository has 569 stars and 121 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is LLaMA-Adapter: Efficient Fine-tuning of Language Models about?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,924 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.07958?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "Describe the lm-sys/FastChat repository that implements Vicuna: An Open-Source Chatbot.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "How many stars does the Efficient Estimation of Word Representations in Vector Space repository have?",
    "output": "The tmikolov/word2vec repository has 1,572 stars and 544 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is available at https://github.com/sentient-agi/OpenDeepSearch (sentient-agi/OpenDeepSearch)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 repository have?",
    "output": "The arcee-ai/mergekit repository has 6,564 stars and 644 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2004.03270?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of Adam: A Method for Stochastic Optimization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Universal Language Model Fine-tuning for Text Classification.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,653 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "What topics does the BEiT: BERT Pre-Training of Image Transformers implementation cover?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "When was the BLIP: Bootstrapping Language-Image Pre-training paper published?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "Summarize the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,415 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 implementation?",
    "output": "The llava-rlhf/LLaVA-RLHF repository has 387 stars and 26 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2502.03387?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What's the implementation repository for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What programming language is AudioPaLM: A Large Language Model for Speech Understanding implemented in?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding implementation is written in Python."
  },
  {
    "input": "Describe the google/trax repository that implements Reformer: The Efficient Transformer.",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "What's the implementation repository for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "What is the GitHub repository for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Explain the lllyasviel/ControlNet implementation of ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "What language is the ContextualAI/HALOs repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements Adam: A Method for Stochastic Optimization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What paper has ArXiv ID 2104.05556?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.10496.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "How many stars does the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale repository have?",
    "output": "The google-research/vision_transformer repository has 12,102 stars and 1,428 forks on GitHub."
  },
  {
    "input": "What category is the DistilBERT: a distilled version of BERT paper in?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the OpenRLHF/OpenRLHF implementation of arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10.",
    "output": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agentic RL)"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is available at https://github.com/lucidrains/muse-maskgit-pytorch (lucidrains/muse-maskgit-pytorch)."
  },
  {
    "input": "How popular is the Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What does the haitongli/knowledge-distillation-pytorch repository do?",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "What is the GitHub repository for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "What year was An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale released?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the CLIP: Learning Transferable Visual Models From Natural Language paper.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,905 stars, indicating high community interest."
  },
  {
    "input": "When was the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper published?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What does the state-spaces/s4 repository do?",
    "output": "Structured state space sequence models"
  },
  {
    "input": "Explain the namisan/mt-dnn implementation of Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is High-Resolution Image Synthesis with Latent Diffusion Models implemented in?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.12872.",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "Summarize the MiniGPT-4: Enhancing Vision-Language Understanding paper.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,762 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the namisan/mt-dnn repository?",
    "output": "The namisan/mt-dnn repository covers: multi-task-learning, natural-language-understanding, deep-learning, microsoft, ranking, named-entity-recognition, bert, machine-reading-comprehension, nlp, pytorch."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the tensorflow/tensorflow implementation of TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was ALBERT: A Lite BERT released?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at yafuly/TPO which test-time preferenece optimization (icml 2025). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of FLAN: Finetuned Language Models Are Zero-Shot Learners.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Feature Pyramid Networks for Object Detection repository have?",
    "output": "The facebookresearch/Detectron repository has 26,396 stars and 5,435 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2210.17323.",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10"
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the hendrycks/test repository?",
    "output": "The hendrycks/test repository covers: muti-task, transfer-learning, gpt-3, few-shot-learning."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Inductive Representation Learning on Large Graphs?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "How many stars does the BEiT: BERT Pre-Training of Image Transformers repository have?",
    "output": "The microsoft/unilm repository has 21,873 stars and 2,681 forks on GitHub."
  },
  {
    "input": "When was the Lookahead Optimizer: k steps forward, 1 step back paper published?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "Give me an overview of DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1912.01703.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "What's the implementation repository for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1801.01290?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How many stars does the Graph Attention Networks repository have?",
    "output": "The PetarV-/GAT repository has 3,469 stars and 672 forks on GitHub."
  },
  {
    "input": "Explain the Shark-NLP/OpenICL implementation of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "What is BLIP-2: Bootstrapping Language-Image Pre-training about?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What is the ArXiv category of JAX: Composable transformations of Python+NumPy programs?",
    "output": "JAX: Composable transformations of Python+NumPy programs is in the cs.MS (cs.MS) category."
  },
  {
    "input": "What paper has ArXiv ID 2307.08691?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Diffusion Models Beat GANs on Image Synthesis?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.07447.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "What paper has ArXiv ID 2005.14165?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Which field does LLaVA-1.5: Improved Baselines with Visual Instruction Tuning belong to?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Summarize the TensorFlow: A System for Large-Scale Machine Learning paper.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,725 stars, indicating high community interest."
  },
  {
    "input": "What category is the InstructBLIP: Towards General-purpose Vision-Language Models paper in?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What's the implementation repository for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "What is the GitHub repository for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the danijar/dreamer repository that implements Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "Describe the shizhediao/active-prompt repository that implements arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "Describe the GAIR-NLP/LIMO repository that implements arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.13636?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "In what year did BLIP-2: Bootstrapping Language-Image Pre-training come out?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "Give me an overview of Synthetic Data Generation with Large Language Models.",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the google/trax repository do?",
    "output": "Trax \u2014 Deep Learning with Clear Code and Speed"
  },
  {
    "input": "What is Text Embeddings by Weakly-Supervised Contrastive Pre-training about?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2502.17437?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "Explain the pytorch/pytorch implementation of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What category is the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper in?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 1912.01703?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is available at https://github.com/TsinghuaAI/CPM-1-Generate (TsinghuaAI/CPM-1-Generate)."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the hojonathanho/diffusion repository written in?",
    "output": "The Denoising Diffusion Probabilistic Models implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at PKU-Alignment/align-anything which align anything: training all-modality model with feedback It is written in Python. The repository has 4,601 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2211.10438.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "Describe the uclaml/SPIN repository that implements arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10.",
    "output": "The official implementation of Self-Play Fine-Tuning (SPIN)"
  },
  {
    "input": "What language is the alphadl/lookahead.pytorch repository written in?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back implementation is written in Python."
  },
  {
    "input": "What does the rail-berkeley/softlearning repository do?",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "Describe the OpenRLHF/OpenRLHF repository that implements arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10.",
    "output": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agentic RL)"
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What are the key topics in the danijar/dreamer repository?",
    "output": "The danijar/dreamer repository covers: reinforcement-learning, artificial-intelligence, deep-learning, robotics, world-models."
  },
  {
    "input": "Tell me the title of ArXiv paper 2010.11929.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "What paper has ArXiv ID 2301.00774?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "How many stars does the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities repository have?",
    "output": "The 0nutation/SpeechGPT repository has 1,396 stars and 95 forks on GitHub."
  },
  {
    "input": "What programming language is InstructBLIP: Towards General-purpose Vision-Language Models implemented in?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "Summarize the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,674 stars, indicating high community interest."
  },
  {
    "input": "What does the google-deepmind/dqn repository do?",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "What are the key topics in the langchain-ai/langchain repository?",
    "output": "The langchain-ai/langchain repository covers: ai, anthropic, gemini, langchain, llm, openai, python, agents, ai-agents, ai-agents-framework, chatgpt, enterprise, framework, generative-ai, multiagent, open-source, pydantic, rag, aiagentframework."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,735 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research/prompt-tuning repository written in?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2311.08045?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10"
  },
  {
    "input": "Explain the state-spaces/s4 implementation of S4: Efficiently Modeling Long Sequences.",
    "output": "Structured state space sequence models"
  },
  {
    "input": "Give me an overview of LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,804 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.03846?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "How popular is the BERT Rediscovers the Classical NLP Pipeline implementation?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "Explain the google-research-datasets/natural-questions implementation of Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Code Llama: Open Foundation Models for Code paper.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.09864.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "When was the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper published?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What language is the facebookresearch/CovidPrognosis repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the KimMeen/Time-LLM repository?",
    "output": "The KimMeen/Time-LLM repository covers: cross-modal-learning, cross-modality, deep-learning, language-model, large-language-models, machine-learning, multimodal-deep-learning, multimodal-time-series, prompt-tuning, time-series, time-series-analysis, time-series-forecast, time-series-forecasting."
  },
  {
    "input": "Explain the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the THUDM/P-tuning-v2 repository that implements P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is a 2024 paper in Computation and Language (Natural Language Processing). The implementation is available at deepseek-ai/DeepSeek-Coder-V2 which deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence The repository has 6,281 stars, indicating high community interest."
  },
  {
    "input": "What category is the BEiT: BERT Pre-Training of Image Transformers paper in?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What language is the microsoft/torchscale repository written in?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.10343?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What paper has ArXiv ID 2103.00020?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Attention Is All You Need paper.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,800 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.12246?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "Explain the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "When was the MiniGPT-4: Enhancing Vision-Language Understanding paper published?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 956 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Wide & Deep Learning for Recommender Systems paper.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,680 stars, indicating high community interest."
  },
  {
    "input": "What programming language is SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implemented in?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2208.01618.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "How popular is the DreamFusion: Text-to-3D using 2D Diffusion implementation?",
    "output": "The ashawkey/stable-dreamfusion repository has 8,785 stars and 771 forks on GitHub."
  },
  {
    "input": "What is Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena about?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 implementation?",
    "output": "The volcengine/verl repository has 17,344 stars and 2,782 forks on GitHub."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 repository have?",
    "output": "The turboderp-org/exllamav2 repository has 4,380 stars and 325 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.22101?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,056 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.14314?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Describe the linkedin/gdmix repository that implements arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "What is the ArXiv category of Decoupled Weight Decay Regularization?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the turboderp-org/exllamav2 implementation of arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10.",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Beir: A Heterogeneous Benchmark for IR paper published?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "What category is the ChemCrow: Augmenting large-language models with chemistry tools paper in?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1506.02640.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "What paper has ArXiv ID 2504.19413?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "What category is the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper in?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "When was the Segment Anything paper published?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What's the implementation repository for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is Deep Graph Library: A Graph-Centric, Highly-Performant Package about?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,165 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was SmoothQuant: Accurate and Efficient Post-Training Quantization released?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2306.05685?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 implementation?",
    "output": "The anthropics/ConstitutionalHarmlessnessPaper repository has 249 stars and 24 forks on GitHub."
  },
  {
    "input": "What language is the danijar/dreamer repository written in?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.10866?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "What year was Segment Anything released?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 repository have?",
    "output": "The ContextualAI/HALOs repository has 894 stars and 50 forks on GitHub."
  },
  {
    "input": "Explain the hojonathanho/diffusion implementation of Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "What year was T5: Exploring the Limits of Transfer Learning released?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "How many stars does the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning repository have?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "Where can I find the code for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is available at https://github.com/anthropics/ConstitutionalHarmlessnessPaper (anthropics/ConstitutionalHarmlessnessPaper)."
  },
  {
    "input": "What paper has ArXiv ID 1905.09418?",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "Summarize the Reformer: The Efficient Transformer paper.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,294 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2002.04745.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "In what year did SimCSE: Simple Contrastive Learning of Sentence Embeddings come out?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation is written in Python."
  },
  {
    "input": "When was the Distilling the Knowledge in a Neural Network paper published?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "What paper has ArXiv ID 2502.10391?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Mistral 7B repository have?",
    "output": "The mistralai/mistral-inference repository has 10,577 stars and 994 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/TransCoder repository has 1,724 stars and 263 forks on GitHub."
  },
  {
    "input": "Describe the yafuly/TPO repository that implements arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10.",
    "output": "Test-time preferenece optimization (ICML 2025)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is LIMA: Less Is More for Alignment about?",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.05302.",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10"
  },
  {
    "input": "Describe the facebookresearch/DPR repository that implements RAG: Retrieval-Augmented Generation.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 repository have?",
    "output": "The openai/frontier-evals repository has 958 stars and 111 forks on GitHub."
  },
  {
    "input": "How many stars does the Semi-Supervised Classification with Graph Convolutional Networks repository have?",
    "output": "The tkipf/gcn repository has 7,339 stars and 2,013 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/TransCoder repository do?",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What is the GitHub repository for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "What language is the sentient-agi/OpenDeepSearch repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is TensorFlow: A System for Large-Scale Machine Learning implemented in?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning implementation is written in C++."
  },
  {
    "input": "Summarize the Dream to Control: Learning Behaviors by Latent Imagination paper.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did LLaVA: Large Language and Vision Assistant come out?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Where can I find the code for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.02506?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "What language is the lisa-groundhog/GroundHog repository written in?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate implementation is written in Python."
  },
  {
    "input": "Describe the salesforce/GeDi repository that implements arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10.",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What is the ArXiv category of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Neural Message Passing for Quantum Chemistry repository have?",
    "output": "The priba/nmp_qc repository has 344 stars and 84 forks on GitHub."
  },
  {
    "input": "What topics does the Synthetic Data Generation with Large Language Models implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What topics does the Gorilla: Large Language Model Connected with Massive APIs implementation cover?",
    "output": "The ShishirPatil/gorilla repository covers: api, llm, api-documentation, chatgpt, gpt-4-api, claude-api, openai-api, openai-functions."
  },
  {
    "input": "Explain the Dense Passage Retrieval for Open-Domain Question Answering paper.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Give me an overview of Reformer: The Efficient Transformer.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,294 stars, indicating high community interest."
  },
  {
    "input": "When was the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper published?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "How popular is the Gorilla: Large Language Model Connected with Massive APIs implementation?",
    "output": "The ShishirPatil/gorilla repository has 12,603 stars and 1,294 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper published?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification paper was published in 2019."
  },
  {
    "input": "Describe the Vision-CAIR/MiniGPT-4 repository that implements MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "What programming language is Grounding DINO: Marrying DINO with Grounded Pre-Training implemented in?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "Give me an overview of High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,646 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.05543?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.08582?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2301.12597?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "Summarize the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,794 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1904.12848?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at llava-rlhf/LLaVA-RLHF which aligning lmms with factually augmented rlhf It is written in Python. The repository has 387 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Which field does DeBERTa: Decoding-enhanced BERT with Disentangled Attention belong to?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper in?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Summarize the Mistral 7B paper.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,577 stars, indicating high community interest."
  },
  {
    "input": "Explain the Contriever: Unsupervised Dense Information Retrieval paper.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 766 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Describe the siddk/voltron-robotics repository that implements arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "Voltron: Language-Driven Representation Learning for Robotics"
  },
  {
    "input": "Explain the google-deepmind/dqn implementation of Playing Atari with Deep Reinforcement Learning.",
    "output": "Lua/Torch implementation of DQN (Nature, 2015)"
  },
  {
    "input": "What year was Mamba: Linear-Time Sequence Modeling with Selective State Spaces released?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.05556.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "What paper has ArXiv ID 2501.12895?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 repository have?",
    "output": "The zai-org/ImageReward repository has 1,594 stars and 83 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Tell me the title of ArXiv paper 1608.06993.",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "What's the implementation repository for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Billion-scale similarity search with GPUs come out?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "How many stars does the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context repository have?",
    "output": "The kimiyoung/transformer-xl repository has 3,680 stars and 765 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is available at https://github.com/AlibabaResearch/DAMO-ConvAI (AlibabaResearch/DAMO-ConvAI)."
  },
  {
    "input": "How many stars does the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision repository have?",
    "output": "The openai/whisper repository has 91,793 stars and 11,506 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "In what year did XLNet: Generalized Autoregressive Pretraining come out?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "What are the key topics in the lucidrains/PaLM-pytorch repository?",
    "output": "The lucidrains/PaLM-pytorch repository covers: deep-learning, transformers, attention-mechanism, artificial-general-intelligence."
  },
  {
    "input": "What year was Beir: A Heterogeneous Benchmark for IR released?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Denoising Diffusion Implicit Models implementation?",
    "output": "The ermongroup/ddim repository has 1,752 stars and 229 forks on GitHub."
  },
  {
    "input": "Summarize the FlashAttention: Fast and Memory-Efficient Exact Attention paper.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2311.08045.",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10"
  },
  {
    "input": "What year was EfficientDet: Scalable and Efficient Object Detection released?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "How popular is the Masked Autoencoders Are Scalable Vision Learners implementation?",
    "output": "The facebookresearch/mae repository has 8,127 stars and 1,338 forks on GitHub."
  },
  {
    "input": "When was the Segment Anything in Medical Images paper published?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did DreamBooth: Fine Tuning Text-to-Image Diffusion Models come out?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "Which field does Depthwise Separable Convolutions for Neural Machine Translation belong to?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1711.05101?",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "When was the DreamFusion: Text-to-3D using 2D Diffusion paper published?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2501.13545?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Which field does RoBERTa: A Robustly Optimized BERT Pretraining Approach belong to?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the mistralai/mistral-inference repository?",
    "output": "The mistralai/mistral-inference repository covers: llm, llm-inference, mistralai."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.14838?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1804.09301?",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1607.01759?",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at salesforce/GeDi which gedi: generative discriminator guided sequence generation It is written in Python. The repository has 210 stars, indicating high community interest."
  },
  {
    "input": "What is Neural Machine Translation by Jointly Learning to Align and Translate about?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "What paper has ArXiv ID 1707.06347?",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 implementation?",
    "output": "The nyu-mll/quality repository has 144 stars and 10 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the Lookahead Optimizer: k steps forward, 1 step back paper.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 338 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What category is the Sequence to Sequence Learning with Neural Networks paper in?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at allenai/RL4LMs which a modular rl library to fine-tune language models to human preferences It is written in Python. The repository has 2,372 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of LIMA: Less Is More for Alignment?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper published?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "What category is the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper in?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 845 stars, indicating high community interest."
  },
  {
    "input": "How popular is the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation?",
    "output": "The nyu-mll/jiant repository has 1,674 stars and 297 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "How many stars does the MiniGPT-4: Enhancing Vision-Language Understanding repository have?",
    "output": "The Vision-CAIR/MiniGPT-4 repository has 25,762 stars and 2,929 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "When was the Dream to Control: Learning Behaviors by Latent Imagination paper published?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "What does the facebookresearch/TransCoder repository do?",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What language is the microsoft/LoRA repository written in?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models implementation is written in Python."
  },
  {
    "input": "What language is the facebookresearch/mae repository written in?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners implementation is written in Python."
  },
  {
    "input": "What programming language is FLAN: Finetuned Language Models Are Zero-Shot Learners implemented in?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "Explain the tmikolov/word2vec implementation of Efficient Estimation of Word Representations in Vector Space.",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "What are the key topics in the microsoft/unilm repository?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "What's the implementation repository for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Reflexion: Language Agents with Verbal Reinforcement Learning paper in?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "What programming language is ChemCrow: Augmenting large-language models with chemistry tools implemented in?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "When was the BEiT: BERT Pre-Training of Image Transformers paper published?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "Explain the google-research/text-to-text-transfer-transformer implementation of T5: Exploring the Limits of Transfer Learning.",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is available at https://github.com/allenai/FineGrainedRLHF (allenai/FineGrainedRLHF)."
  },
  {
    "input": "Which field does QLoRA: Efficient Finetuning of Quantized LLMs belong to?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2210.17323?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "Which field does SimCSE: Simple Contrastive Learning of Sentence Embeddings belong to?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did MiniGPT-4: Enhancing Vision-Language Understanding come out?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation?",
    "output": "The jacopotagliabue/you-dont-need-a-bigger-boat repository has 873 stars and 66 forks on GitHub."
  },
  {
    "input": "Give me an overview of XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,181 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at AlibabaResearch/DAMO-ConvAI which damo-convai: the official repository which contains the codebase for alibaba damo conversational ai. It is written in Python. The repository has 1,508 stars, indicating high community interest."
  },
  {
    "input": "Summarize the ReAct: Synergizing Reasoning and Acting in Language Models paper.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,264 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is available at https://github.com/Kwai-YuanQi/MM-RLHF (Kwai-YuanQi/MM-RLHF)."
  },
  {
    "input": "Give me an overview of Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,603 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 implementation?",
    "output": "The ZHZisZZ/modpo repository has 93 stars and 7 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation?",
    "output": "The simplescaling/s1 repository has 6,611 stars and 762 forks on GitHub."
  },
  {
    "input": "Describe the google-research/text-to-text-transfer-transformer repository that implements T5: Exploring the Limits of Transfer Learning.",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the Distilling the Knowledge in a Neural Network paper.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,974 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.03533.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "What are the key topics in the plurai-ai/intellagent repository?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.10425?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "Where can I find the code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "Explain the google/automl implementation of EfficientDet: Scalable and Efficient Object Detection.",
    "output": "Google Brain AutoML"
  },
  {
    "input": "How many stars does the ChemCrow: Augmenting large-language models with chemistry tools repository have?",
    "output": "The ur-whitelab/chemcrow-public repository has 845 stars and 134 forks on GitHub."
  },
  {
    "input": "What year was BERT: Pre-training of Deep Bidirectional Transformers released?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.18290?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at allenai/RL4LMs which a modular rl library to fine-tune language models to human preferences It is written in Python. The repository has 2,372 stars, indicating high community interest."
  },
  {
    "input": "What does the beir-cellar/beir repository do?",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "Where can I find the code for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1904.12848.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the tensorflow/tensor2tensor repository?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of ALBERT: A Lite BERT.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,275 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.12824?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2004.13637?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "What language is the haotian-liu/LLaVA repository written in?",
    "output": "The LLaVA: Large Language and Vision Assistant implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2501.03262?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10"
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "Explain the Longformer: The Long-Document Transformer paper.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,176 stars, indicating high community interest."
  },
  {
    "input": "Explain the TensorFlow: A System for Large-Scale Machine Learning paper.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,725 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.13688?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the arcee-ai/mergekit repository?",
    "output": "The arcee-ai/mergekit repository covers: llama, llm, model-merging."
  },
  {
    "input": "Tell me the title of ArXiv paper 1409.0473.",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 implementation?",
    "output": "The anthropics/hh-rlhf repository has 1,801 stars and 150 forks on GitHub."
  },
  {
    "input": "How many stars does the Flamingo: A Visual Language Model for Few-Shot Learning repository have?",
    "output": "The lucidrains/flamingo-pytorch repository has 1,273 stars and 66 forks on GitHub."
  },
  {
    "input": "What programming language is P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implemented in?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Multi-Task Deep Neural Networks for Natural Language Understanding about?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "In what year did Neural Machine Translation by Jointly Learning to Align and Translate come out?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate paper was published in 2015."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00808?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "What year was AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning released?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What category is the BERT Rediscovers the Classical NLP Pipeline paper in?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is Med-PaLM 2: Towards Expert-Level Medical Question Answering about?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.03762.",
    "output": "Attention Is All You Need"
  },
  {
    "input": "What is the GitHub repository for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.16084.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "Explain the uber/orbit implementation of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What is the ArXiv category of Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the facebookresearch/TransCoder repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "What's the implementation repository for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "What is Lookahead Optimizer: k steps forward, 1 step back about?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 338 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2005.00687?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.04426.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "Explain the S4: Efficiently Modeling Long Sequences paper.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,794 stars, indicating high community interest."
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.02643.",
    "output": "Segment Anything"
  },
  {
    "input": "How many stars does the Depthwise Separable Convolutions for Neural Machine Translation repository have?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "Which field does Faster R-CNN: Towards Real-Time Object Detection belong to?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.05198?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What programming language is CLIP: Learning Transferable Visual Models From Natural Language implemented in?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the ArXiv category of InPars: Data Augmentation for Information Retrieval?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What paper has ArXiv ID 1512.03385?",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "How popular is the StarCoder: May the source be with you! implementation?",
    "output": "The bigcode-project/starcoder repository has 7,478 stars and 531 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2012.06009.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What category is the Multi-Task Deep Neural Networks for Natural Language Understanding paper in?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.13637.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 582 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.08774?",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "Explain the tomekkorbak/pretraining-with-human-feedback implementation of arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10.",
    "output": "Code accompanying the paper Pretraining Language Models with Human Preferences"
  },
  {
    "input": "Explain the microsoft/LoRA implementation of LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "In what year did Contriever: Unsupervised Dense Information Retrieval come out?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "What paper has ArXiv ID 2406.06326?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "What language is the linkedin/gdmix repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What topics does the PyTorch Geometric: Library for Geometric Deep Learning implementation cover?",
    "output": "The pyg-team/pytorch_geometric repository covers: pytorch, geometric-deep-learning, graph-neural-networks, deep-learning, graph-convolutional-networks."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2209.00796.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "Summarize the The Power of Scale for Parameter-Efficient Prompt Tuning paper.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 699 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "How many stars does the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning repository have?",
    "output": "The THUDM/P-tuning-v2 repository has 2,069 stars and 205 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.14233?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "What language is the RLHFlow/Online-RLHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.01848.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.14815.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 implementation cover?",
    "output": "The allenai/RL4LMs repository covers: language-modeling, nlp, reinforcement-learning, dialogue-generation, machine-translation, natural-language-processing, summarization, table-to-text, text-generation."
  },
  {
    "input": "What paper has ArXiv ID 1301.3781?",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "In what year did ELECTRA: Pre-training Text Encoders as Discriminators come out?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "When was the Faster R-CNN: Towards Real-Time Object Detection paper published?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "What is RAG: Retrieval-Augmented Generation about?",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "Summarize the BLIP: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,603 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.02155.",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2210.03629?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Masked Autoencoders Are Scalable Vision Learners?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1808.07042.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "In what year did Very Deep Convolutional Networks for Large-Scale Image Recognition come out?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "How popular is the AudioPaLM: A Large Language Model for Speech Understanding implementation?",
    "output": "The lucidrains/audiolm-pytorch repository has 2,610 stars and 280 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "When was the Vicuna: An Open-Source Chatbot paper published?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2005.12872?",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.05543?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "What's the implementation repository for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "What's the implementation repository for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "Give me an overview of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What are the key topics in the RLHFlow/Online-RLHF repository?",
    "output": "The RLHFlow/Online-RLHF repository covers: llm, rlhf, llama3."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1612.03144?",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "What is Masked Autoencoders Are Scalable Vision Learners about?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,127 stars, indicating high community interest."
  },
  {
    "input": "What programming language is SmoothQuant: Accurate and Efficient Post-Training Quantization implemented in?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization implementation is written in Python."
  },
  {
    "input": "What topics does the XLNet: Generalized Autoregressive Pretraining implementation cover?",
    "output": "The zihangdai/xlnet repository covers: tensorflow, nlp, deep-learning."
  },
  {
    "input": "Explain the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,623 stars, indicating high community interest."
  },
  {
    "input": "Explain the huawei-noah/Pretrained-Language-Model implementation of TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does ViT: An Image is Worth 16x16 Words belong to?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at openai/following-instructions-human-feedback The repository has 1,251 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "In what year did Know What You Don't Know: Unanswerable Questions for SQuAD come out?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2203.07814?",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What programming language is Very Deep Convolutional Networks for Large-Scale Image Recognition implemented in?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition implementation is written in Python."
  },
  {
    "input": "Describe the alphadl/lookahead.pytorch repository that implements Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the williamleif/GraphSAGE repository that implements Inductive Representation Learning on Large Graphs.",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "In what year did Question and Answer Test-Train Overlap in Open-Domain QA Datasets come out?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Summarize the BEiT: BERT Pre-Training of Image Transformers paper.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Neural Collaborative Filtering implementation?",
    "output": "The hexiangnan/neural_collaborative_filtering repository has 1,865 stars and 666 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the RetNet: Retentive Network: A Successor to Transformer implementation?",
    "output": "The microsoft/torchscale repository has 3,124 stars and 222 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at OpenRLHF/OpenRLHF which an easy-to-use, scalable and high-performance rlhf framework based on ray (ppo & grpo & reinforce++ & vllm & ray & dynamic sampling & async agentic rl) It is written in Python. The repository has 8,562 stars, indicating high community interest."
  },
  {
    "input": "In what year did S4: Efficiently Modeling Long Sequences come out?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "Describe the facebookresearch/fastText repository that implements Enriching Word Vectors with Subword Information.",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "Explain the Analyzing the Structure of Attention in a Transformer Language Model paper.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "Describe the machrisaa/tensorflow-vgg repository that implements Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "How popular is the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation?",
    "output": "The jasonwei20/eda_nlp repository has 1,649 stars and 317 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.14525?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.02054.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "What year was Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena released?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did Mamba: Linear-Time Sequence Modeling with Selective State Spaces come out?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper was published in 2023."
  },
  {
    "input": "What topics does the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation cover?",
    "output": "The jasonwei20/eda_nlp repository covers: nlp, data-augmentation, text-classification, synonyms, embeddings, sentence, classification, rnn, cnn, swap, position."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.03264?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2205.05638?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "Give me an overview of Efficient Estimation of Word Representations in Vector Space.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,572 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation?",
    "output": "The ContextualAI/HALOs repository has 894 stars and 50 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 implementation cover?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "Explain the tensorflow/tensor2tensor implementation of Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "Where can I find the code for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "What language is the ShishirPatil/gorilla repository written in?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs implementation is written in Python."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.11511.",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.12652.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of Denoising Diffusion Probabilistic Models?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the Zephyr: Direct Distillation of LM Alignment paper published?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Proximal Policy Optimization Algorithms repository have?",
    "output": "The openai/baselines repository has 16,588 stars and 4,951 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What category is the LIMA: Less Is More for Alignment paper in?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2405.03520?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "What year was Linformer: Self-Attention with Linear Complexity released?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2310.03708?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "What paper has ArXiv ID 2302.00902?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "Explain the Llama 2: Open Foundation and Fine-Tuned Chat Models paper.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What is Horovod: fast and easy distributed deep learning in TensorFlow about?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,642 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "In what year did YOLOX: Exceeding YOLO Series in 2021 come out?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 repository have?",
    "output": "The salesforce/booksum repository has 195 stars and 35 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/TransCoder repository has 1,724 stars and 263 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at TsinghuaAI/CPM-1-Generate which chinese pre-trained language models (cpm-lm) version-i It is written in Python. The repository has 1,582 stars, indicating high community interest."
  },
  {
    "input": "When was the BLIP-2: Bootstrapping Language-Image Pre-training paper published?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "When was the LLaVA: Large Language and Vision Assistant paper published?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2210.03629.",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "How popular is the InstructBLIP: Towards General-purpose Vision-Language Models implementation?",
    "output": "The salesforce/LAVIS repository has 11,064 stars and 1,087 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Masked Autoencoders Are Scalable Vision Learners paper in?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What language is the rll-research/BPref repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "What programming language is LLaVA: Large Language and Vision Assistant implemented in?",
    "output": "The LLaVA: Large Language and Vision Assistant implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the bigcode-project/starcoder repository written in?",
    "output": "The StarCoder: May the source be with you! implementation is written in Python."
  },
  {
    "input": "What are the key topics in the salesforce/BLIP repository?",
    "output": "The salesforce/BLIP repository covers: vision-language, vision-and-language-pre-training, image-text-retrieval, image-captioning, visual-question-answering, visual-reasoning, vision-language-transformer."
  },
  {
    "input": "What programming language is Densely Connected Convolutional Networks implemented in?",
    "output": "The Densely Connected Convolutional Networks implementation is written in Lua."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 1909.11942?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "In what year did Proximal Policy Optimization Algorithms come out?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2006.03511?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of Diffusion Models Beat GANs on Image Synthesis.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,183 stars, indicating high community interest."
  },
  {
    "input": "What category is the AlpacaEval: An Automatic Evaluator for Instruction-following paper in?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the google-deepmind/code_contests repository written in?",
    "output": "The Competition-Level Code Generation with AlphaCode implementation is written in C++."
  },
  {
    "input": "What's the implementation repository for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "How popular is the Segment Anything in Medical Images implementation?",
    "output": "The bowang-lab/MedSAM repository has 3,974 stars and 546 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "Summarize the Universal Language Model Fine-tuning for Text Classification paper.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,653 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Decoupled Weight Decay Regularization implementation?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "Describe the rbgirshick/py-faster-rcnn repository that implements Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.05062.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the llava-rlhf/LLaVA-RLHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is available at https://github.com/tgxs002/align_sd (tgxs002/align_sd)."
  },
  {
    "input": "Which field does Vicuna: An Open-Source Chatbot belong to?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "What category is the Inductive Representation Learning on Large Graphs paper in?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "Describe the openai/frontier-evals repository that implements arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "Explain the SmoothQuant: Accurate and Efficient Post-Training Quantization paper.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,567 stars, indicating high community interest."
  },
  {
    "input": "What year was Segment Anything in Medical Images released?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2212.10375?",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is available at https://github.com/Kwai-YuanQi/MM-RLHF (Kwai-YuanQi/MM-RLHF)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "Which field does S4: Efficiently Modeling Long Sequences belong to?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.12766.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.05543.",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 implementation cover?",
    "output": "The RLHFlow/Online-RLHF repository covers: llm, rlhf, llama3."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "Explain the rll-research/BPref implementation of arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10.",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the jasonwei20/eda_nlp implementation of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "When was the High-Resolution Image Synthesis with Latent Diffusion Models paper published?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "What does the tmikolov/word2vec repository do?",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What topics does the BLIP: Bootstrapping Language-Image Pre-training implementation cover?",
    "output": "The salesforce/BLIP repository covers: vision-language, vision-and-language-pre-training, image-text-retrieval, image-captioning, visual-question-answering, visual-reasoning, vision-language-transformer."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does Swin Transformer: Hierarchical Vision Transformer using Shifted Windows belong to?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is available at https://github.com/sylinrl/TruthfulQA (sylinrl/TruthfulQA)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.13461?",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.11401?",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What language is the KimMeen/Time-LLM repository written in?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2303.04226?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the mistralai/mistral-inference repository do?",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "What year was LLaMA: Open and Efficient Foundation Language Models released?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "How many stars does the RAG: Retrieval-Augmented Generation repository have?",
    "output": "The facebookresearch/DPR repository has 1,851 stars and 316 forks on GitHub."
  },
  {
    "input": "What language is the openai/evals repository written in?",
    "output": "The GPT-4 Technical Report implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the facebookresearch/faiss repository do?",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "What paper has ArXiv ID 1910.01108?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "How popular is the JAX: Composable transformations of Python+NumPy programs implementation?",
    "output": "The jax-ml/jax repository has 34,268 stars and 3,294 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/CovidPrognosis repository do?",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "Which field does FlashAttention: Fast and Memory-Efficient Exact Attention belong to?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the FlashAttention-2: Faster Attention with Better Parallelism repository have?",
    "output": "The Dao-AILab/flash-attention repository has 20,993 stars and 2,199 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2410.12896?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does Multi-Task Deep Neural Networks for Natural Language Understanding belong to?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.09308?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.02054?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "What does the noahshinn/reflexion repository do?",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.11147.",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10"
  },
  {
    "input": "Explain the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.15334.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What paper has ArXiv ID 1807.11164?",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Explain the KimMeen/Time-LLM implementation of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at OptimalScale/LMFlow which an extensible toolkit for finetuning and inference of large foundation models. large models for all. It is written in Python. The repository has 8,489 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2103.14899?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "What paper has ArXiv ID 1706.03762?",
    "output": "Attention Is All You Need"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 implementation?",
    "output": "The allenai/FineGrainedRLHF repository has 281 stars and 22 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the jacopotagliabue/you-dont-need-a-bigger-boat repository do?",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "Describe the 0nutation/SpeechGPT repository that implements SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 repository have?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What language is the fastai/fastai repository written in?",
    "output": "The Universal Language Model Fine-tuning for Text Classification implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the google-research/bert implementation of BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Distilling the Knowledge in a Neural Network?",
    "output": "The code for Distilling the Knowledge in a Neural Network is available at https://github.com/haitongli/knowledge-distillation-pytorch (haitongli/knowledge-distillation-pytorch)."
  },
  {
    "input": "How popular is the EfficientDet: Scalable and Efficient Object Detection implementation?",
    "output": "The google/automl repository has 6,430 stars and 1,460 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1711.05101?",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "What are the key topics in the noahshinn/reflexion repository?",
    "output": "The noahshinn/reflexion repository covers: ai, artificial-intelligence, llm."
  },
  {
    "input": "Where can I find the code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is a 2024 paper in Computation and Language (Natural Language Processing). The implementation is available at deepseek-ai/DeepSeek-Coder-V2 which deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence The repository has 6,281 stars, indicating high community interest."
  },
  {
    "input": "Explain the google-research/google-research implementation of Depthwise Separable Convolutions for Neural Machine Translation.",
    "output": "Google Research"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.07814?",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "What paper has ArXiv ID 2502.12115?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "Describe the meta-llama/llama repository that implements Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What is the GitHub repository for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "What language is the ashawkey/stable-dreamfusion repository written in?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion implementation is written in Python."
  },
  {
    "input": "How many stars does the Analyzing the Structure of Attention in a Transformer Language Model repository have?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 implementation?",
    "output": "The sylinrl/TruthfulQA repository has 854 stars and 107 forks on GitHub."
  },
  {
    "input": "What does the salesforce/LAVIS repository do?",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "Give me an overview of YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,223 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the High-Resolution Image Synthesis with Latent Diffusion Models paper.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,646 stars, indicating high community interest."
  },
  {
    "input": "Describe the ermongroup/ddim repository that implements Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Diffusion Models Beat GANs on Image Synthesis repository have?",
    "output": "The openai/guided-diffusion repository has 7,183 stars and 887 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 repository have?",
    "output": "The anthropics/hh-rlhf repository has 1,801 stars and 150 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.20734.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.10592.",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "What paper has ArXiv ID 2503.22678?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "When was the PaLM: Scaling Language Modeling with Pathways paper published?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.22101.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "In what year did Prefix-Tuning: Optimizing Continuous Prompts for Generation come out?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 248 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Playing Atari with Deep Reinforcement Learning come out?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,341 stars, indicating high community interest."
  },
  {
    "input": "Explain the openai/summarize-from-feedback implementation of arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10.",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "How many stars does the AlpacaEval: An Automatic Evaluator for Instruction-following repository have?",
    "output": "The tatsu-lab/alpaca_eval repository has 1,924 stars and 294 forks on GitHub."
  },
  {
    "input": "What topics does the TinyBERT: Distilling BERT for Natural Language Understanding implementation cover?",
    "output": "The huawei-noah/Pretrained-Language-Model repository covers: knowledge-distillation, model-compression, quantization, pretrained-models, large-scale-distributed."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is available at https://github.com/arcee-ai/mergekit (arcee-ai/mergekit)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 repository have?",
    "output": "The openai/summarize-from-feedback repository has 1,056 stars and 151 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.01917?",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "Describe the ali-vilab/composer repository that implements arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "In what year did RAG: Retrieval-Augmented Generation come out?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What category is the Neural Collaborative Filtering paper in?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did LoRA: Low-Rank Adaptation of Large Language Models come out?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2304.02643?",
    "output": "Segment Anything"
  },
  {
    "input": "Give me an overview of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,794 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.13824.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 repository have?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/segment-anything repository written in?",
    "output": "The Segment Anything implementation is written in Jupyter Notebook."
  },
  {
    "input": "Which field does Diffusion Models Beat GANs on Image Synthesis belong to?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "What are the key topics in the FoundationAgents/MetaGPT repository?",
    "output": "The FoundationAgents/MetaGPT repository covers: agent, gpt, llm, metagpt, multi-agent."
  },
  {
    "input": "Describe the rll-research/BPref repository that implements arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10.",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is available at https://github.com/PKU-Alignment/align-anything (PKU-Alignment/align-anything)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Describe the Kwai-YuanQi/MM-RLHF repository that implements arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10.",
    "output": "The Next Step Forward in Multimodal LLM Alignment"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.09515.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 implementation?",
    "output": "The anthropics/evals repository has 315 stars and 33 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at AlibabaResearch/DAMO-ConvAI which damo-convai: the official repository which contains the codebase for alibaba damo conversational ai. It is written in Python. The repository has 1,508 stars, indicating high community interest."
  },
  {
    "input": "Summarize the DistilBERT: a distilled version of BERT paper.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 153,664 stars, indicating high community interest."
  },
  {
    "input": "What category is the TensorFlow: A System for Large-Scale Machine Learning paper in?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "Explain the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is Distilling the Knowledge in a Neural Network implemented in?",
    "output": "The Distilling the Knowledge in a Neural Network implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Explain the simplescaling/s1 implementation of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of EfficientDet: Scalable and Efficient Object Detection?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks come out?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at sylinrl/TruthfulQA which truthfulqa: measuring how models imitate human falsehoods It is written in Jupyter Notebook. The repository has 854 stars, indicating high community interest."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What paper has ArXiv ID 2212.04356?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "When was the XLNet: Generalized Autoregressive Pretraining paper published?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/TransCoder repository has 1,724 stars and 263 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 implementation?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 233 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,339 stars, indicating high community interest."
  },
  {
    "input": "What does the Dao-AILab/flash-attention repository do?",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 repository have?",
    "output": "The Linear95/APO repository has 56 stars and 3 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2012.00413?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the The Power of Scale for Parameter-Efficient Prompt Tuning paper published?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "Give me an overview of Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is a 2017 paper in Machine Learning. The implementation is available at horovod/horovod which distributed training framework for tensorflow, keras, pytorch, and apache mxnet. It is written in Python. The repository has 14,642 stars, indicating high community interest."
  },
  {
    "input": "Explain the Know What You Don't Know: Unanswerable Questions for SQuAD paper.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "What does the liuzhuang13/DenseNet repository do?",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "Summarize the FLAN: Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.04426?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 repository have?",
    "output": "The Crista23/goal_directedness_llms repository has 12 stars and 2 forks on GitHub."
  },
  {
    "input": "What category is the Denoising Diffusion Implicit Models paper in?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.02216?",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "When was the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper published?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What is Flamingo: A Visual Language Model for Few-Shot Learning about?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,273 stars, indicating high community interest."
  },
  {
    "input": "What programming language is PaLM: Scaling Language Modeling with Pathways implemented in?",
    "output": "The PaLM: Scaling Language Modeling with Pathways implementation is written in Python."
  },
  {
    "input": "Where can I find the code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "How popular is the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Segment Anything in Medical Images?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is ReAct: Synergizing Reasoning and Acting in Language Models about?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at ysymyth/ReAct which [iclr 2023] react: synergizing reasoning and acting in language models It is written in Jupyter Notebook. The repository has 3,264 stars, indicating high community interest."
  },
  {
    "input": "When was the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper published?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1706.03762?",
    "output": "Attention Is All You Need"
  },
  {
    "input": "What language is the google-research/albert repository written in?",
    "output": "The ALBERT: A Lite BERT implementation is written in Python."
  },
  {
    "input": "Describe the fastai/fastai repository that implements Universal Language Model Fine-tuning for Text Classification.",
    "output": "The fastai deep learning library"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.09403.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2304.05302?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of LLaVA: Large Language and Vision Assistant?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2208.02294?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10"
  },
  {
    "input": "What is Time-LLM: Time Series Forecasting by Reprogramming Large Language Models about?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,410 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the BLIP: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,603 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.12766?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "How popular is the DistilBERT: a distilled version of BERT implementation?",
    "output": "The huggingface/transformers repository has 153,664 stars and 31,358 forks on GitHub."
  },
  {
    "input": "How popular is the ChemCrow: Augmenting large-language models with chemistry tools implementation?",
    "output": "The ur-whitelab/chemcrow-public repository has 845 stars and 134 forks on GitHub."
  },
  {
    "input": "Explain the Reformer: The Efficient Transformer paper.",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,294 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was XLNet: Generalized Autoregressive Pretraining released?",
    "output": "The XLNet: Generalized Autoregressive Pretraining paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.03409.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "Explain the RoFormer: Enhanced Transformer with Rotary Position Embedding paper.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,059 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2406.10209?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "What programming language is Depthwise Separable Convolutions for Neural Machine Translation implemented in?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation implementation is written in Jupyter Notebook."
  },
  {
    "input": "What year was RAG: Retrieval-Augmented Generation released?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What is Vicuna: An Open-Source Chatbot about?",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2108.12409?",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "When was the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper published?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "What programming language is PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implemented in?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 repository have?",
    "output": "The sylinrl/TruthfulQA repository has 854 stars and 107 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.03548?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Synthetic Data Generation with Large Language Models?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.11366?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "In what year did Attention Is All You Need come out?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "What does the Vision-CAIR/MiniGPT-4 repository do?",
    "output": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)"
  },
  {
    "input": "Which field does ChemCrow: Augmenting large-language models with chemistry tools belong to?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Where can I find the code for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is available at https://github.com/GanjinZero/RRHF (GanjinZero/RRHF)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the google/automl repository do?",
    "output": "Google Brain AutoML"
  },
  {
    "input": "What is the ArXiv category of Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2503.20201?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "What's the implementation repository for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "What is the ArXiv category of Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "How popular is the ReAct: Synergizing Reasoning and Acting in Language Models implementation?",
    "output": "The ysymyth/ReAct repository has 3,264 stars and 331 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the haotian-liu/LLaVA repository?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "What paper has ArXiv ID 1603.04467?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning"
  },
  {
    "input": "What is Finetuned Language Models Are Zero-Shot Learners about?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What category is the LoRA: Low-Rank Adaptation of Large Language Models paper in?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Billion-scale similarity search with GPUs implemented in?",
    "output": "The Billion-scale similarity search with GPUs implementation is written in C++."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.04356?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "What paper has ArXiv ID 2205.13636?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10"
  },
  {
    "input": "In what year did ChemCrow: Augmenting large-language models with chemistry tools come out?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Faster R-CNN: Towards Real-Time Object Detection paper in?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.03109.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "What language is the IDEA-Research/GroundingDINO repository written in?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the shizhediao/active-prompt repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the AlpacaEval: An Automatic Evaluator for Instruction-following paper.",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,924 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 repository have?",
    "output": "The tgxs002/align_sd repository has 293 stars and 10 forks on GitHub."
  },
  {
    "input": "What topics does the RetNet: Retentive Network: A Successor to Transformer implementation cover?",
    "output": "The microsoft/torchscale repository covers: computer-vision, machine-learning, multimodal, natural-language-processing, pretrained-language-model, speech-processing, transformer, translation."
  },
  {
    "input": "Explain the Inductive Representation Learning on Large Graphs paper.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,636 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is available at https://github.com/OpenRLHF/OpenRLHF (OpenRLHF/OpenRLHF)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation is written in Python."
  },
  {
    "input": "What topics does the LoRA: Low-Rank Adaptation of Large Language Models implementation cover?",
    "output": "The microsoft/LoRA repository covers: gpt-2, adaptation, language-model, gpt-3, low-rank, pytorch, deep-learning, roberta, deberta, lora."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2102.01192?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the facebookresearch/DPR repository written in?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering implementation is written in Python."
  },
  {
    "input": "Explain the Dao-AILab/flash-attention implementation of FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is available at https://github.com/llava-rlhf/LLaVA-RLHF (llava-rlhf/LLaVA-RLHF)."
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Explain the Multi-Task Deep Neural Networks for Natural Language Understanding paper.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is available at https://github.com/arcee-ai/mergekit (arcee-ai/mergekit)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 repository have?",
    "output": "The allenai/RL4LMs repository has 2,372 stars and 203 forks on GitHub."
  },
  {
    "input": "How many stars does the ELECTRA: Pre-training Text Encoders as Discriminators repository have?",
    "output": "The google-research/electra repository has 2,365 stars and 349 forks on GitHub."
  },
  {
    "input": "Which field does HuggingGPT: Solving AI Tasks with ChatGPT belong to?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2307.09288?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What category is the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper in?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is available at https://github.com/GXimingLu/Quark (GXimingLu/Quark)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.16779.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.03548.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "Describe the openai/baselines repository that implements Proximal Policy Optimization Algorithms.",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "What paper has ArXiv ID 2103.14030?",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "Explain the haoliuhl/language-quantized-autoencoders implementation of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Describe the facebookresearch/DPR repository that implements Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "What does the RLHFlow/Online-RLHF repository do?",
    "output": "A recipe for online RLHF and online iterative DPO."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "In what year did Codex: Evaluating Large Language Models Trained on Code come out?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "Explain the meta-llama/codellama implementation of Code Llama: Open Foundation Models for Code.",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "What does the allenai/RL4LMs repository do?",
    "output": "A modular RL library to fine-tune language models to human preferences"
  },
  {
    "input": "What year was Neural Message Passing for Quantum Chemistry released?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "Summarize the StarCoder: May the source be with you! paper.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,478 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What paper has ArXiv ID 2106.05091?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10"
  },
  {
    "input": "When was the Densely Connected Convolutional Networks paper published?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What does the allenai/codescientist repository do?",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "How popular is the LLaMA: Open and Efficient Foundation Language Models implementation?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "Explain the facebookresearch/TransCoder implementation of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "When was the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper published?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04671?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What is the GitHub repository for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "What topics does the Adam: A Method for Stochastic Optimization implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.08913.",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the noahshinn/reflexion implementation of Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "Describe the simplescaling/s1 repository that implements arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 implementation?",
    "output": "The shizhediao/active-prompt repository has 248 stars and 30 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "In what year did Adam: A Method for Stochastic Optimization come out?",
    "output": "The Adam: A Method for Stochastic Optimization paper was published in 2014."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "What are the key topics in the OptimalScale/LMFlow repository?",
    "output": "The OptimalScale/LMFlow repository covers: chatgpt, deep-learning, instruction-following, language-model, pretrained-models, pytorch, transformer."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.21801.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "Which field does Competition-Level Code Generation with AlphaCode belong to?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the FlashAttention-2: Faster Attention with Better Parallelism paper in?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1902.05188.",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "When was the JAX: Composable transformations of Python+NumPy programs paper published?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "In what year did Graph Attention Networks come out?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "How popular is the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.12246.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was AlpacaEval: An Automatic Evaluator for Instruction-following released?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "When was the Playing Atari with Deep Reinforcement Learning paper published?",
    "output": "The Playing Atari with Deep Reinforcement Learning paper was published in 2013."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.08518?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2112.10752?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "When was the Sequence to Sequence Learning with Neural Networks paper published?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "Explain the Neural Collaborative Filtering paper.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,865 stars, indicating high community interest."
  },
  {
    "input": "What language is the jacopotagliabue/you-dont-need-a-bigger-boat repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the haotian-liu/LLaVA implementation of LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What is the GitHub repository for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What programming language is DistilBERT: a distilled version of BERT implemented in?",
    "output": "The DistilBERT: a distilled version of BERT implementation is written in Python."
  },
  {
    "input": "What category is the WizardCoder: Empowering Code LLMs with Evol-Instruct paper in?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "Explain the Neural Machine Translation by Jointly Learning to Align and Translate paper.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "Describe the google-research-datasets/natural-questions repository that implements Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems."
  },
  {
    "input": "What is PyTorch Geometric: Library for Geometric Deep Learning about?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,255 stars, indicating high community interest."
  },
  {
    "input": "Describe the microsoft/DeBERTa repository that implements DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "What language is the Kwai-YuanQi/MM-RLHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 repository have?",
    "output": "The TsinghuaAI/CPM-1-Generate repository has 1,582 stars and 211 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.09418?",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Grounding DINO: Marrying DINO with Grounded Pre-Training paper in?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did Faster R-CNN: Towards Real-Time Object Detection come out?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection paper was published in 2015."
  },
  {
    "input": "What language is the google/trax repository written in?",
    "output": "The Reformer: The Efficient Transformer implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is available at https://github.com/allenai/FineGrainedRLHF (allenai/FineGrainedRLHF)."
  },
  {
    "input": "What paper has ArXiv ID 2410.12896?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What does the lm-sys/FastChat repository do?",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Describe the microsoft/JARVIS repository that implements HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What category is the Llama 2: Open Foundation and Fine-Tuned Chat Models paper in?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Densely Connected Convolutional Networks about?",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,848 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What year was Know What You Don't Know: Unanswerable Questions for SQuAD released?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "How many stars does the Finetuned Language Models Are Zero-Shot Learners repository have?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.01952.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.07394.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the Know What You Don't Know: Unanswerable Questions for SQuAD implementation?",
    "output": "The rajpurkar/SQuAD-explorer repository has 569 stars and 121 forks on GitHub."
  },
  {
    "input": "In what year did Multi-Task Deep Neural Networks for Natural Language Understanding come out?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "What are the key topics in the salesforce/LAVIS repository?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "Where can I find the code for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "Which field does Mistral 7B belong to?",
    "output": "Mistral 7B is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the salesforce/LAVIS repository written in?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at uclaml/SPIN which the official implementation of self-play fine-tuning (spin) It is written in Python. The repository has 1,226 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the ArXiv category of SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What year was S4: Efficiently Modeling Long Sequences released?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Grounding DINO: Marrying DINO with Grounded Pre-Training belong to?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.07842?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "Explain the Competition-Level Code Generation with AlphaCode paper.",
    "output": "Competition-Level Code Generation with AlphaCode is a 2022 paper in Machine Learning. The implementation is available at google-deepmind/code_contests It is written in C++. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "Explain the williamleif/GraphSAGE implementation of Inductive Representation Learning on Large Graphs.",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.08100.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "What is the ArXiv category of Neural Message Passing for Quantum Chemistry?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the lucidrains/flamingo-pytorch repository?",
    "output": "The lucidrains/flamingo-pytorch repository covers: artificial-intelligence, attention-mechanism, deep-learning, transformers, visual-question-answering."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.13188?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "What is Beir: A Heterogeneous Benchmark for IR about?",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,014 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "What programming language is Llama 2: Open Foundation and Fine-Tuned Chat Models implemented in?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "How many stars does the Distilling the Knowledge in a Neural Network repository have?",
    "output": "The haitongli/knowledge-distillation-pytorch repository has 1,974 stars and 351 forks on GitHub."
  },
  {
    "input": "What language is the openai/whisper repository written in?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision implementation is written in Python."
  },
  {
    "input": "Describe the MoonshotAI/Kimi-Audio repository that implements arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the ZhuiyiTechnology/roformer repository written in?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding implementation is written in Python."
  },
  {
    "input": "Explain the facebookresearch/CovidPrognosis implementation of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "What programming language is The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implemented in?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation is written in Python."
  },
  {
    "input": "What language is the princeton-nlp/SimCSE repository written in?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the ELECTRA: Pre-training Text Encoders as Discriminators paper.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation?",
    "output": "The tianjunz/HIR repository has 159 stars and 11 forks on GitHub."
  },
  {
    "input": "What are the key topics in the state-spaces/s4 repository?",
    "output": "The state-spaces/s4 repository covers: state-space-models, sequence-models, pytorch."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2212.10496?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "Give me an overview of SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,623 stars, indicating high community interest."
  },
  {
    "input": "What category is the GPT-3: Language Models are Few-Shot Learners paper in?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Self-RAG: Learning to Retrieve, Generate, and Critique released?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2209.00796?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at OptimalScale/LMFlow which an extensible toolkit for finetuning and inference of large foundation models. large models for all. It is written in Python. The repository has 8,489 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the FLAN: Finetuned Language Models Are Zero-Shot Learners repository have?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.19328.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "What programming language is Zephyr: Direct Distillation of LM Alignment implemented in?",
    "output": "The Zephyr: Direct Distillation of LM Alignment implementation is written in Python."
  },
  {
    "input": "Explain the Segment Anything paper.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,837 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1903.07293?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "What is the ArXiv category of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the deepseek-ai/DeepSeek-Coder-V2 repository that implements arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10.",
    "output": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Very Deep Convolutional Networks for Large-Scale Image Recognition repository have?",
    "output": "The machrisaa/tensorflow-vgg repository has 2,240 stars and 1,072 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1909.10351.",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 repository have?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.09431?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "Summarize the Neural Collaborative Filtering paper.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,865 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "How many stars does the Med-PaLM 2: Towards Expert-Level Medical Question Answering repository have?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "What topics does the Decoupled Weight Decay Regularization implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "What is the GitHub repository for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "How many stars does the Attention Is All You Need repository have?",
    "output": "The tensorflow/tensor2tensor repository has 16,800 stars and 3,698 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "In what year did FLAN: Finetuned Language Models Are Zero-Shot Learners come out?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.08575?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "How many stars does the The Power of Scale for Parameter-Efficient Prompt Tuning repository have?",
    "output": "The google-research/prompt-tuning repository has 699 stars and 61 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2009.01325?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2107.03374?",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "How many stars does the LLaMA: Open and Efficient Foundation Language Models repository have?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.12755.",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2303.17580?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 repository have?",
    "output": "The google-research/pegasus repository has 1,645 stars and 317 forks on GitHub."
  },
  {
    "input": "Explain the Adam: A Method for Stochastic Optimization paper.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What language is the frankxu2004/knnlm-why repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is BERT Rediscovers the Classical NLP Pipeline implemented in?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "In what year did AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning come out?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "When was the Longformer: The Long-Document Transformer paper published?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 1710.10903?",
    "output": "Graph Attention Networks"
  },
  {
    "input": "Describe the lucidrains/flamingo-pytorch repository that implements Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "What is the GitHub repository for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What language is the OpenBMB/Eurus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the JohnNay/llm-lobbyist implementation of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "How popular is the Sequence to Sequence Learning with Neural Networks implementation?",
    "output": "The farizrahman4u/seq2seq repository has 3,175 stars and 837 forks on GitHub."
  },
  {
    "input": "What category is the Longformer: The Long-Document Transformer paper in?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Deep Residual Learning for Image Recognition released?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "What topics does the CLIP: Learning Transferable Visual Models From Natural Language implementation cover?",
    "output": "The openai/CLIP repository covers: deep-learning, machine-learning."
  },
  {
    "input": "Explain the haitongli/knowledge-distillation-pytorch implementation of Distilling the Knowledge in a Neural Network.",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "What is the GitHub repository for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.14420.",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10"
  },
  {
    "input": "What does the lucidrains/PaLM-pytorch repository do?",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "Tell me the title of ArXiv paper 2009.01325.",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Attention Is All You Need about?",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,800 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "What is the ArXiv category of Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.10391.",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10"
  },
  {
    "input": "Describe the turboderp-org/exllamav2 repository that implements arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10.",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "Explain the dmlc/dgl implementation of Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "What year was Llama 2: Open Foundation and Fine-Tuned Chat Models released?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2505.22954?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "Explain the CompVis/latent-diffusion implementation of High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2009.06367.",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 repository have?",
    "output": "The linkedin/gdmix repository has 133 stars and 18 forks on GitHub."
  },
  {
    "input": "How popular is the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Denoising Diffusion Probabilistic Models belong to?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the Megvii-BaseDetection/YOLOX repository?",
    "output": "The Megvii-BaseDetection/YOLOX repository covers: yolox, yolov3, onnx, tensorrt, ncnn, openvino, pytorch, megengine, object-detection, yolo, deep-learning."
  },
  {
    "input": "What programming language is XLNet: Generalized Autoregressive Pretraining implemented in?",
    "output": "The XLNet: Generalized Autoregressive Pretraining implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2404.03592.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "Describe the nyu-mll/jiant repository that implements SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "What language is the lucidrains/audiolm-pytorch repository written in?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation?",
    "output": "The dmlc/dgl repository has 14,165 stars and 3,056 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is available at https://github.com/ZHZisZZ/modpo (ZHZisZZ/modpo)."
  },
  {
    "input": "What is JAX: Composable transformations of Python+NumPy programs about?",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,268 stars, indicating high community interest."
  },
  {
    "input": "Which field does PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel belong to?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What does the linkedin/gdmix repository do?",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Dream to Control: Learning Behaviors by Latent Imagination implementation?",
    "output": "The danijar/dreamer repository has 570 stars and 113 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Adam: A Method for Stochastic Optimization?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Longformer: The Long-Document Transformer implemented in?",
    "output": "The Longformer: The Long-Document Transformer implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did PyTorch Geometric: Library for Geometric Deep Learning come out?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "What programming language is ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implemented in?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.04908.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "Give me an overview of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale.",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper.",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 implementation?",
    "output": "The RLHFlow/Online-RLHF repository has 537 stars and 49 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the uber/orbit repository?",
    "output": "The uber/orbit repository covers: python, forecasting, bayesian, exponential-smoothing, pyro, stan, pystan, probabilistic-programming, probabilistic, forecast, orbit, time-series, regression, arima, changepoint, bayesian-methods, bayesian-statistics, machine-learning, regression-models, pytorch."
  },
  {
    "input": "What category is the MMLU: Measuring Massive Multitask Language Understanding paper in?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.11446.",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2402.18571?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10"
  },
  {
    "input": "What topics does the DreamFusion: Text-to-3D using 2D Diffusion implementation cover?",
    "output": "The ashawkey/stable-dreamfusion repository covers: text-to-3d, gui, nerf, stable-diffusion, dreamfusion, image-to-3d."
  },
  {
    "input": "What language is the salesforce/LAVIS repository written in?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 2301.09515?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "Which field does SQuAD: 100,000+ Questions for Machine Comprehension of Text belong to?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What category is the ALBERT: A Lite BERT paper in?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What year was ViT: An Image is Worth 16x16 Words released?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "Describe the google/automl repository that implements EfficientDet: Scalable and Efficient Object Detection.",
    "output": "Google Brain AutoML"
  },
  {
    "input": "What year was Densely Connected Convolutional Networks released?",
    "output": "The Densely Connected Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is The Power of Scale for Parameter-Efficient Prompt Tuning about?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 699 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.09403?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "Explain the LLaMA: Open and Efficient Foundation Language Models paper.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper published?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale paper was published in 2020."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 implementation cover?",
    "output": "The arcee-ai/mergekit repository covers: llama, llm, model-merging."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,611 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2101.00027?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "What paper has ArXiv ID 2104.09864?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
  },
  {
    "input": "How popular is the Depthwise Separable Convolutions for Neural Machine Translation implementation?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at allenai/RL4LMs which a modular rl library to fine-tune language models to human preferences It is written in Python. The repository has 2,372 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.03708.",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "What is the ArXiv category of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel repository have?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models implementation is written in Python."
  },
  {
    "input": "What is Grounding DINO: Marrying DINO with Grounded Pre-Training about?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,401 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,240 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation cover?",
    "output": "The KimMeen/Time-LLM repository covers: cross-modal-learning, cross-modality, deep-learning, language-model, large-language-models, machine-learning, multimodal-deep-learning, multimodal-time-series, prompt-tuning, time-series, time-series-analysis, time-series-forecast, time-series-forecasting."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "What is the ArXiv category of T5: Exploring the Limits of Transfer Learning?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 248 stars, indicating high community interest."
  },
  {
    "input": "Explain the Segment Anything in Medical Images paper.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,974 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "When was the Billion-scale similarity search with GPUs paper published?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 1301.3781.",
    "output": "Efficient Estimation of Word Representations in Vector Space"
  },
  {
    "input": "Summarize the AudioPaLM: A Large Language Model for Speech Understanding paper.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,610 stars, indicating high community interest."
  },
  {
    "input": "What topics does the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation cover?",
    "output": "The THUDM/P-tuning-v2 repository covers: natural-language-processing, prompt-tuning, pretrained-language-model, p-tuning, parameter-efficient-learning."
  },
  {
    "input": "What year was Feature Pyramid Networks for Object Detection released?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1906.08237?",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "What does the frankxu2004/knnlm-why repository do?",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "In what year did Deep Graph Library: A Graph-Centric, Highly-Performant Package come out?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 1503.02531?",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,645 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "Which field does Lookahead Optimizer: k steps forward, 1 step back belong to?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Online-RLHF which a recipe for online rlhf and online iterative dpo. It is written in Python. The repository has 537 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2203.11147?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2106.07447?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "In what year did QLoRA: Efficient Finetuning of Quantized LLMs come out?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "In what year did WizardCoder: Empowering Code LLMs with Evol-Instruct come out?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "Which field does MMLU: Measuring Massive Multitask Language Understanding belong to?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "What is the GitHub repository for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2009.01325?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.10305?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "What topics does the LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation cover?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository covers: llm, machine-learning, pytorch, qlora, quantization."
  },
  {
    "input": "How many stars does the BLIP-2: Bootstrapping Language-Image Pre-training repository have?",
    "output": "The salesforce/LAVIS repository has 11,064 stars and 1,087 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at shizhediao/active-prompt which source code for the paper \"active prompting with chain-of-thought for large language models\" It is written in Python. The repository has 248 stars, indicating high community interest."
  },
  {
    "input": "What category is the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper in?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "Summarize the Adam: A Method for Stochastic Optimization paper.",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What does the Dao-AILab/flash-attention repository do?",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "How many stars does the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2405.03548?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10"
  },
  {
    "input": "What topics does the Attention Is All You Need implementation cover?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "Explain the YOLOX: Exceeding YOLO Series in 2021 paper.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,223 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Enriching Word Vectors with Subword Information?",
    "output": "The code for Enriching Word Vectors with Subword Information is available at https://github.com/facebookresearch/fastText (facebookresearch/fastText)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What paper has ArXiv ID 2209.14375?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research/electra repository written in?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the allenai/codescientist repository that implements arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification.",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "What year was MRKL Systems: A modular approach to AGI released?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What category is the Adam: A Method for Stochastic Optimization paper in?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Semi-Supervised Classification with Graph Convolutional Networks belong to?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Mistral 7B released?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 repository have?",
    "output": "The jacopotagliabue/you-dont-need-a-bigger-boat repository has 873 stars and 66 forks on GitHub."
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the priba/nmp_qc repository do?",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "What does the microsoft/LoRA repository do?",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "Tell me the title of ArXiv paper 1807.11164.",
    "output": "YOLOX: Exceeding YOLO Series in 2021"
  },
  {
    "input": "What year was Dream to Control: Learning Behaviors by Latent Imagination released?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Explain the allenai/codescientist implementation of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "In what year did ReAct: Synergizing Reasoning and Acting in Language Models come out?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "What does the williamleif/GraphSAGE repository do?",
    "output": "Representation learning on large graphs using stochastic graph convolutions."
  },
  {
    "input": "What paper has ArXiv ID 2301.10343?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "How popular is the AlpacaEval: An Automatic Evaluator for Instruction-following implementation?",
    "output": "The tatsu-lab/alpaca_eval repository has 1,924 stars and 294 forks on GitHub."
  },
  {
    "input": "What programming language is EfficientDet: Scalable and Efficient Object Detection implemented in?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "The code for Grounding DINO: Marrying DINO with Grounded Pre-Training is available at https://github.com/IDEA-Research/GroundingDINO (IDEA-Research/GroundingDINO)."
  },
  {
    "input": "Describe the openai/CLIP repository that implements CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "How popular is the Codex: Evaluating Large Language Models Trained on Code implementation?",
    "output": "The openai/human-eval repository has 3,048 stars and 423 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the hojonathanho/diffusion repository do?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "What is the GitHub repository for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2409.19256.",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Know What You Don't Know: Unanswerable Questions for SQuAD paper published?",
    "output": "The Know What You Don't Know: Unanswerable Questions for SQuAD paper was published in 2018."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "How popular is the TensorFlow: A System for Large-Scale Machine Learning implementation?",
    "output": "The tensorflow/tensorflow repository has 192,725 stars and 75,036 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.02828?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2302.04870?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.00698.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "Explain the EfficientDet: Scalable and Efficient Object Detection paper.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,430 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "When was the DistilBERT: a distilled version of BERT paper published?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,978 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,662 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.13636.",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10"
  },
  {
    "input": "What is QLoRA: Efficient Finetuning of Quantized LLMs about?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,782 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.15004?",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "In what year did Flamingo: A Visual Language Model for Few-Shot Learning come out?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "What language is the ur-whitelab/chemcrow-public repository written in?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the QLoRA: Efficient Finetuning of Quantized LLMs paper.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,782 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the facebookresearch/mae repository do?",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "Which field does Segment Anything belong to?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,021 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2107.07346?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.04434.",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Enriching Word Vectors with Subword Information repository have?",
    "output": "The facebookresearch/fastText repository has 26,445 stars and 4,815 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Masked Autoencoders Are Scalable Vision Learners belong to?",
    "output": "Masked Autoencoders Are Scalable Vision Learners is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation cover?",
    "output": "The uber/orbit repository covers: python, forecasting, bayesian, exponential-smoothing, pyro, stan, pystan, probabilistic-programming, probabilistic, forecast, orbit, time-series, regression, arima, changepoint, bayesian-methods, bayesian-statistics, machine-learning, regression-models, pytorch."
  },
  {
    "input": "What paper has ArXiv ID 2309.03409?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.03109?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1606.07792?",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "What is the ArXiv category of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.08691?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tianjunz/HIR It is written in Python. The repository has 159 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.04129?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "Explain the Gorilla: Large Language Model Connected with Massive APIs paper.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,603 stars, indicating high community interest."
  },
  {
    "input": "Describe the haitongli/knowledge-distillation-pytorch repository that implements Distilling the Knowledge in a Neural Network.",
    "output": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is available at https://github.com/allenai/RL4LMs (allenai/RL4LMs)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.02783.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "What does the ali-vilab/composer repository do?",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "Where can I find the code for Decoupled Weight Decay Regularization?",
    "output": "The code for Decoupled Weight Decay Regularization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "Which field does InPars: Data Augmentation for Information Retrieval belong to?",
    "output": "InPars: Data Augmentation for Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Describe the openai/summarize-from-feedback repository that implements arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10.",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "When was the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper published?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "How popular is the Neural Message Passing for Quantum Chemistry implementation?",
    "output": "The priba/nmp_qc repository has 344 stars and 84 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.08485.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tomekkorbak/pretraining-with-human-feedback which code accompanying the paper pretraining language models with human preferences It is written in Python. The repository has 180 stars, indicating high community interest."
  },
  {
    "input": "Which field does Llama 2: Open Foundation and Fine-Tuned Chat Models belong to?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Sequence to Sequence Learning with Neural Networks come out?",
    "output": "The Sequence to Sequence Learning with Neural Networks paper was published in 2014."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2007.16122?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does T5: Exploring the Limits of Transfer Learning belong to?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What category is the PyTorch Geometric: Library for Geometric Deep Learning paper in?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.11305?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "Describe the lllyasviel/ControlNet repository that implements ControlNet: Adding Conditional Control to Text-to-Image Diffusion.",
    "output": "Let us control diffusion models!"
  },
  {
    "input": "What are the key topics in the NVlabs/prismer repository?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "What does the ZhuiyiTechnology/roformer repository do?",
    "output": "Rotary Transformer"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Which field does FLAN: Finetuned Language Models Are Zero-Shot Learners belong to?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "What year was Competition-Level Code Generation with AlphaCode released?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "In what year did BLIP: Bootstrapping Language-Image Pre-training come out?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.06865.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "In what year did StarCoder: May the source be with you! come out?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "What is Zephyr: Direct Distillation of LM Alignment about?",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,444 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2110.07602.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "In what year did Horovod: fast and easy distributed deep learning in TensorFlow come out?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "When was the QLoRA: Efficient Finetuning of Quantized LLMs paper published?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Proximal Policy Optimization Algorithms implemented in?",
    "output": "The Proximal Policy Optimization Algorithms implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Describe the huggingface/alignment-handbook repository that implements Zephyr: Direct Distillation of LM Alignment.",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.04023?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What topics does the Dream to Control: Learning Behaviors by Latent Imagination implementation cover?",
    "output": "The danijar/dreamer repository covers: reinforcement-learning, artificial-intelligence, deep-learning, robotics, world-models."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2409.19256?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What does the GAIR-NLP/LIMO repository do?",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "Explain the MoonshotAI/Kimi-Audio implementation of arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10.",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What does the google-research/text-to-text-transfer-transformer repository do?",
    "output": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\""
  },
  {
    "input": "Explain the zetaalphavector/InPars implementation of InPars: Data Augmentation for Information Retrieval.",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Describe the AkariAsai/self-rag repository that implements Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research-datasets/natural-questions repository written in?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 implementation?",
    "output": "The tomekkorbak/pretraining-with-human-feedback repository has 180 stars and 13 forks on GitHub."
  },
  {
    "input": "What programming language is Segment Anything implemented in?",
    "output": "The Segment Anything implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the ArXiv category of Playing Atari with Deep Reinforcement Learning?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Beir: A Heterogeneous Benchmark for IR implementation cover?",
    "output": "The beir-cellar/beir repository covers: nlp, information-retrieval, bert, benchmark, sentence-transformers, question-generation, retrieval, passage-retrieval, elasticsearch, dpr, sbert, retrieval-models, dataset, colbert, zero-shot-retrieval, deep-learning, pytorch, llm, rag."
  },
  {
    "input": "In what year did Swin Transformer: Hierarchical Vision Transformer using Shifted Windows come out?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "What language is the arcee-ai/mergekit repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the FlashAttention-2: Faster Attention with Better Parallelism implementation?",
    "output": "The Dao-AILab/flash-attention repository has 20,993 stars and 2,199 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.13637?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "Explain the ashawkey/stable-dreamfusion implementation of DreamFusion: Text-to-3D using 2D Diffusion.",
    "output": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Explain the Reflexion: Language Agents with Verbal Reinforcement Learning paper.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,981 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.00020?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.03059.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is Efficient Estimation of Word Representations in Vector Space about?",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,572 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.09251.",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2102.01192?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.00109.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "What is GPT-4 Technical Report about?",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,406 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2503.00735?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.13461.",
    "output": "ALBERT: A Lite BERT"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1508.07909?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Zephyr: Direct Distillation of LM Alignment?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for BERT Rediscovers the Classical NLP Pipeline?",
    "output": "The code for BERT Rediscovers the Classical NLP Pipeline is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "What programming language is TinyBERT: Distilling BERT for Natural Language Understanding implemented in?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "Explain the InstructBLIP: Towards General-purpose Vision-Language Models paper.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2406.12824?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of Know What You Don't Know: Unanswerable Questions for SQuAD.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Explain the alphadl/lookahead.pytorch implementation of Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "Explain the sentient-agi/OpenDeepSearch implementation of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "What's the implementation repository for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What is the GitHub repository for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "How many stars does the BLIP: Bootstrapping Language-Image Pre-training repository have?",
    "output": "The salesforce/BLIP repository has 5,603 stars and 749 forks on GitHub."
  },
  {
    "input": "What language is the tensorflow/tensorflow repository written in?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning implementation is written in C++."
  },
  {
    "input": "What does the zetaalphavector/InPars repository do?",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.07459.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2010.11929?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Online-RLHF which a recipe for online rlhf and online iterative dpo. It is written in Python. The repository has 537 stars, indicating high community interest."
  },
  {
    "input": "What year was PyTorch Geometric: Library for Geometric Deep Learning released?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.11316?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "What year was Finetuned Language Models Are Zero-Shot Learners released?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1606.05250.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "What category is the Reformer: The Efficient Transformer paper in?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Depthwise Separable Convolutions for Neural Machine Translation paper in?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of Sequence to Sequence Learning with Neural Networks?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the FlashAttention: Fast and Memory-Efficient Exact Attention paper.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What year was ELECTRA: Pre-training Text Encoders as Discriminators released?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What language is the priba/nmp_qc repository written in?",
    "output": "The Neural Message Passing for Quantum Chemistry implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 1606.07792?",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2412.15838?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What does the bitsandbytes-foundation/bitsandbytes repository do?",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "What category is the Efficient Estimation of Word Representations in Vector Space paper in?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 1906.04341?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "How popular is the PyTorch Geometric: Library for Geometric Deep Learning implementation?",
    "output": "The pyg-team/pytorch_geometric repository has 23,255 stars and 3,928 forks on GitHub."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 repository have?",
    "output": "The google/BIG-bench repository has 3,167 stars and 616 forks on GitHub."
  },
  {
    "input": "When was the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper published?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Which field does SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities belong to?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is RoFormer: Enhanced Transformer with Rotary Position Embedding implemented in?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2308.08155?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Synthetic Data Generation with Large Language Models paper published?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "What does the ZHZisZZ/modpo repository do?",
    "output": "[ACL'24] Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.12755?",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the JAX: Composable transformations of Python+NumPy programs repository have?",
    "output": "The jax-ml/jax repository has 34,268 stars and 3,294 forks on GitHub."
  },
  {
    "input": "When was the T5: Exploring the Limits of Transfer Learning paper published?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "Explain the anthropics/hh-rlhf implementation of arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10.",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is available at https://github.com/salesforce/booksum (salesforce/booksum)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 repository have?",
    "output": "The uber/orbit repository has 2,021 stars and 141 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "Describe the dmlc/dgl repository that implements Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Python package built to ease deep learning on graph, on top of existing DL frameworks."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at arcee-ai/mergekit which tools for merging pretrained large language models. It is written in Python. The repository has 6,564 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Competition-Level Code Generation with AlphaCode?",
    "output": "The code for Competition-Level Code Generation with AlphaCode is available at https://github.com/google-deepmind/code_contests (google-deepmind/code_contests)."
  },
  {
    "input": "How popular is the FlashAttention: Fast and Memory-Efficient Exact Attention implementation?",
    "output": "The Dao-AILab/flash-attention repository has 20,993 stars and 2,199 forks on GitHub."
  },
  {
    "input": "What is BLIP: Bootstrapping Language-Image Pre-training about?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,603 stars, indicating high community interest."
  },
  {
    "input": "In what year did RoBERTa: A Robustly Optimized BERT Pretraining Approach come out?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "What category is the Enriching Word Vectors with Subword Information paper in?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2309.14525?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Kwai-YuanQi/MM-RLHF which the next step forward in multimodal llm alignment It is written in Python. The repository has 189 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Faster R-CNN: Towards Real-Time Object Detection repository have?",
    "output": "The rbgirshick/py-faster-rcnn repository has 8,278 stars and 4,104 forks on GitHub."
  },
  {
    "input": "Describe the microsoft/unilm repository that implements BEiT: BERT Pre-Training of Image Transformers.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the WizardCoder: Empowering Code LLMs with Evol-Instruct implementation?",
    "output": "The nlpxucan/WizardLM repository has 9,471 stars and 751 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2205.00445?",
    "output": "MRKL Systems: A modular approach to AGI"
  },
  {
    "input": "What year was Horovod: fast and easy distributed deep learning in TensorFlow released?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "What is ELECTRA: Pre-training Text Encoders as Discriminators about?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.02643?",
    "output": "Segment Anything"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Neural Collaborative Filtering?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 repository have?",
    "output": "The nyu-mll/quality repository has 144 stars and 10 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of RetNet: Retentive Network: A Successor to Transformer.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,124 stars, indicating high community interest."
  },
  {
    "input": "What does the Shark-NLP/OpenICL repository do?",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What language is the dmlc/dgl repository written in?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "What is AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning about?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 362 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.08242.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "Describe the lisa-groundhog/GroundHog repository that implements Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Library for implementing RNNs with Theano"
  },
  {
    "input": "Summarize the JAX: Composable transformations of Python+NumPy programs paper.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,268 stars, indicating high community interest."
  },
  {
    "input": "What does the salesforce/BLIP repository do?",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is available at https://github.com/anthropics/ConstitutionalHarmlessnessPaper (anthropics/ConstitutionalHarmlessnessPaper)."
  },
  {
    "input": "What is Linformer: Self-Attention with Linear Complexity about?",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "What is Playing Atari with Deep Reinforcement Learning about?",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "In what year did ALBERT: A Lite BERT come out?",
    "output": "The ALBERT: A Lite BERT paper was published in 2019."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.01181?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/TransCoder implementation of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,056 stars, indicating high community interest."
  },
  {
    "input": "Explain the microsoft/unilm implementation of BEiT: BERT Pre-Training of Image Transformers.",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What programming language is Diffusion Models Beat GANs on Image Synthesis implemented in?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What does the openai/human-eval repository do?",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is available at https://github.com/GXimingLu/Quark (GXimingLu/Quark)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Question and Answer Test-Train Overlap in Open-Domain QA Datasets repository have?",
    "output": "The google-research-datasets/natural-questions repository has 1,076 stars and 157 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at GXimingLu/Quark It is written in Python. The repository has 75 stars, indicating high community interest."
  },
  {
    "input": "Explain the liuzhuang13/DenseNet implementation of Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "Explain the microsoft/JARVIS implementation of HuggingGPT: Solving AI Tasks with ChatGPT.",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2211.10438?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was FlashAttention-2: Faster Attention with Better Parallelism released?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1607.01759.",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "What topics does the InstructBLIP: Towards General-purpose Vision-Language Models implementation cover?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "Where can I find the code for FlashAttention-2: Faster Attention with Better Parallelism?",
    "output": "The code for FlashAttention-2: Faster Attention with Better Parallelism is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the tensorflow/tensor2tensor repository written in?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,611 stars, indicating high community interest."
  },
  {
    "input": "Explain the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,377 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Segment Anything.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,837 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at GanjinZero/RRHF which [nips2023] rrhf & wombat It is written in Python. The repository has 811 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/evals The repository has 315 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1508.07909.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 implementation?",
    "output": "The rll-research/BPref repository has 133 stars and 31 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Whisper: Robust Speech Recognition via Large-Scale Weak Supervision.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,793 stars, indicating high community interest."
  },
  {
    "input": "How popular is the CLIP: Learning Transferable Visual Models From Natural Language implementation?",
    "output": "The openai/CLIP repository has 31,905 stars and 3,859 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1910.02054?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What is ChemCrow: Augmenting large-language models with chemistry tools about?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 845 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.10512.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the artidoro/qlora repository do?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "Which field does Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena belong to?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,175 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "When was the GPT-3: Language Models are Few-Shot Learners paper published?",
    "output": "The GPT-3: Language Models are Few-Shot Learners paper was published in 2020."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the bitsandbytes-foundation/bitsandbytes repository that implements LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "Where can I find the code for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "When was the GPT-4 Technical Report paper published?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "What does the salesforce/LAVIS repository do?",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.06326.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2203.02155?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10"
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What language is the NVlabs/prismer repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.12766?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10"
  },
  {
    "input": "Which field does Synthetic Data Generation with Large Language Models belong to?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the mit-han-lab/smoothquant repository that implements SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "Where can I find the code for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.02120?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.08492?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What language is the openai/baselines repository written in?",
    "output": "The Proximal Policy Optimization Algorithms implementation is written in Python."
  },
  {
    "input": "What programming language is An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implemented in?",
    "output": "The An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is MRKL Systems: A modular approach to AGI about?",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 121,514 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Kwai-YuanQi/MM-RLHF which the next step forward in multimodal llm alignment It is written in Python. The repository has 189 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What's the implementation repository for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "What language is the lllyasviel/ControlNet repository written in?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 repository have?",
    "output": "The ali-vilab/composer repository has 1,560 stars and 49 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "Summarize the Enriching Word Vectors with Subword Information paper.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,445 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.00923?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04226?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was GPT-4 Technical Report released?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "What language is the facebookresearch/ConvNeXt-V2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the Masked Autoencoders Are Scalable Vision Learners paper published?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01751?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "What is PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel about?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 implementation?",
    "output": "The google/BIG-bench repository has 3,167 stars and 616 forks on GitHub."
  },
  {
    "input": "Describe the haotian-liu/LLaVA repository that implements LLaVA: Large Language and Vision Assistant.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is available at https://github.com/PKU-Alignment/align-anything (PKU-Alignment/align-anything)."
  },
  {
    "input": "What language is the beir-cellar/beir repository written in?",
    "output": "The Beir: A Heterogeneous Benchmark for IR implementation is written in Python."
  },
  {
    "input": "How popular is the SmoothQuant: Accurate and Efficient Post-Training Quantization implementation?",
    "output": "The mit-han-lab/smoothquant repository has 1,567 stars and 190 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2402.18571?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for AlpacaEval: An Automatic Evaluator for Instruction-following?",
    "output": "The code for AlpacaEval: An Automatic Evaluator for Instruction-following is available at https://github.com/tatsu-lab/alpaca_eval (tatsu-lab/alpaca_eval)."
  },
  {
    "input": "What is the ArXiv category of Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "What category is the Neural Machine Translation by Jointly Learning to Align and Translate paper in?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Code Llama: Open Foundation Models for Code paper.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "Which field does Med-PaLM 2: Towards Expert-Level Medical Question Answering belong to?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is InPars: Data Augmentation for Information Retrieval about?",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at zai-org/ImageReward which [neurips 2023] imagereward: learning and evaluating human preferences for text-to-image generation It is written in Python. The repository has 1,594 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Grounding DINO: Marrying DINO with Grounded Pre-Training paper.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,401 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the ermongroup/ddim repository written in?",
    "output": "The Denoising Diffusion Implicit Models implementation is written in Python."
  },
  {
    "input": "Explain the T5: Exploring the Limits of Transfer Learning paper.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,458 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2201.08239?",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "Summarize the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "What year was SimCSE: Simple Contrastive Learning of Sentence Embeddings released?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings paper was published in 2021."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "Explain the Codex: Evaluating Large Language Models Trained on Code paper.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,048 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.11331?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "What programming language is HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implemented in?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "How popular is the Competition-Level Code Generation with AlphaCode implementation?",
    "output": "The google-deepmind/code_contests repository has 2,177 stars and 223 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Explain the microsoft/DeBERTa implementation of DeBERTa: Decoding-enhanced BERT with Disentangled Attention.",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "Which field does Gorilla: Large Language Model Connected with Massive APIs belong to?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2406.09403?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2109.10862?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10"
  },
  {
    "input": "In what year did HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction come out?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "Explain the HumanAIGC/AnimateAnyone implementation of Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "What year was Lookahead Optimizer: k steps forward, 1 step back released?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "Summarize the RetNet: Retentive Network: A Successor to Transformer paper.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,124 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Densely Connected Convolutional Networks?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What category is the Generating Wikipedia by Summarizing Long Sequences paper in?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "Explain the openai/human-eval implementation of Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "What year was Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context released?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the rll-research/BPref repository do?",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the CLIP: Learning Transferable Visual Models From Natural Language paper published?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.11305?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "Which field does Billion-scale similarity search with GPUs belong to?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What does the facebookresearch/CovidPrognosis repository do?",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "In what year did PyTorch: An Imperative Style, High-Performance Deep Learning Library come out?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the deepspeedai/DeepSpeed implementation of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the ZHZisZZ/modpo implementation of arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10.",
    "output": "[ACL'24] Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What does the tomekkorbak/pretraining-with-human-feedback repository do?",
    "output": "Code accompanying the paper Pretraining Language Models with Human Preferences"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 repository have?",
    "output": "The openai/following-instructions-human-feedback repository has 1,251 stars and 146 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2109.07958?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2301.04856?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "What language is the NVlabs/prismer repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "Where can I find the code for Reformer: The Efficient Transformer?",
    "output": "The code for Reformer: The Efficient Transformer is available at https://github.com/google/trax (google/trax)."
  },
  {
    "input": "What does the tensorflow/models repository do?",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.14030?",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "Summarize the DreamFusion: Text-to-3D using 2D Diffusion paper.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,785 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Billion-scale similarity search with GPUs?",
    "output": "Billion-scale similarity search with GPUs is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech about?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the google-research/google-research repository?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What is the ArXiv category of Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 repository have?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "Summarize the BLIP-2: Bootstrapping Language-Image Pre-training paper.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 repository have?",
    "output": "The sentient-agi/OpenDeepSearch repository has 3,735 stars and 343 forks on GitHub."
  },
  {
    "input": "Summarize the Sequence to Sequence Learning with Neural Networks paper.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,175 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Graph Attention Networks paper.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,469 stars, indicating high community interest."
  },
  {
    "input": "Summarize the PaLM: Scaling Language Modeling with Pathways paper.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 827 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 implementation cover?",
    "output": "The OpenRLHF/OpenRLHF repository covers: transformers, vllm, large-language-models, raylib, reinforcement-learning-from-human-feedback, reinforcement-learning, openai-o1, proximal-policy-optimization."
  },
  {
    "input": "When was the Gorilla: Large Language Model Connected with Massive APIs paper published?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "What is Wide & Deep Learning for Recommender Systems about?",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,680 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "Explain the Vicuna: An Open-Source Chatbot paper.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the google-research/bert repository?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "In what year did JAX: Composable transformations of Python+NumPy programs come out?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "What are the key topics in the oughtinc/ice repository?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What programming language is T5: Exploring the Limits of Transfer Learning implemented in?",
    "output": "The T5: Exploring the Limits of Transfer Learning implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "What category is the Competition-Level Code Generation with AlphaCode paper in?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the TinyBERT: Distilling BERT for Natural Language Understanding paper published?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "Explain the ELECTRA: Pre-training Text Encoders as Discriminators paper.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "What language is the namisan/mt-dnn repository written in?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "What does the yafuly/TPO repository do?",
    "output": "Test-time preferenece optimization (ICML 2025)."
  },
  {
    "input": "What category is the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper in?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the facebookresearch/contriever repository do?",
    "output": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2312.00752?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2410.12896.",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation?",
    "output": "The google-deepmind/tracr repository has 548 stars and 47 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 266 stars and 28 forks on GitHub."
  },
  {
    "input": "What are the key topics in the microsoft/JARVIS repository?",
    "output": "The microsoft/JARVIS repository covers: deep-learning, platform, pytorch."
  },
  {
    "input": "How many stars does the BERT: Pre-training of Deep Bidirectional Transformers repository have?",
    "output": "The google-research/bert repository has 39,724 stars and 9,710 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2002.05202.",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "Explain the google/BIG-bench implementation of arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10.",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 60,328 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2410.18050?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "How popular is the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation?",
    "output": "The QingruZhang/AdaLoRA repository has 362 stars and 37 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 repository have?",
    "output": "The shizhediao/active-prompt repository has 248 stars and 30 forks on GitHub."
  },
  {
    "input": "In what year did Universal Language Model Fine-tuning for Text Classification come out?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "What is LLM.int8(): 8-bit Matrix Multiplication for Transformers about?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,804 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.10375.",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "What programming language is BLIP-2: Bootstrapping Language-Image Pre-training implemented in?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "In what year did Whisper: Robust Speech Recognition via Large-Scale Weak Supervision come out?",
    "output": "The Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper was published in 2022."
  },
  {
    "input": "When was the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper published?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.09685?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "What is the GitHub repository for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for FlashAttention: Fast and Memory-Efficient Exact Attention?",
    "output": "The code for FlashAttention: Fast and Memory-Efficient Exact Attention is available at https://github.com/Dao-AILab/flash-attention (Dao-AILab/flash-attention)."
  },
  {
    "input": "What does the sylinrl/TruthfulQA repository do?",
    "output": "TruthfulQA: Measuring How Models Imitate Human Falsehoods"
  },
  {
    "input": "In what year did InPars: Data Augmentation for Information Retrieval come out?",
    "output": "The InPars: Data Augmentation for Information Retrieval paper was published in 2022."
  },
  {
    "input": "Where can I find the code for High-Resolution Image Synthesis with Latent Diffusion Models?",
    "output": "The code for High-Resolution Image Synthesis with Latent Diffusion Models is available at https://github.com/CompVis/latent-diffusion (CompVis/latent-diffusion)."
  },
  {
    "input": "What is the GitHub repository for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What language is the google/automl repository written in?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.04870?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "What category is the FLAN: Finetuned Language Models Are Zero-Shot Learners paper in?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the beir-cellar/beir repository?",
    "output": "The beir-cellar/beir repository covers: nlp, information-retrieval, bert, benchmark, sentence-transformers, question-generation, retrieval, passage-retrieval, elasticsearch, dpr, sbert, retrieval-models, dataset, colbert, zero-shot-retrieval, deep-learning, pytorch, llm, rag."
  },
  {
    "input": "When was the Contriever: Unsupervised Dense Information Retrieval paper published?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What topics does the Reflexion: Language Agents with Verbal Reinforcement Learning implementation cover?",
    "output": "The noahshinn/reflexion repository covers: ai, artificial-intelligence, llm."
  },
  {
    "input": "Explain the Flamingo: A Visual Language Model for Few-Shot Learning paper.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,273 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 implementation cover?",
    "output": "The OptimalScale/LMFlow repository covers: chatgpt, deep-learning, instruction-following, language-model, pretrained-models, pytorch, transformer."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1312.5602.",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "What programming language is SimCSE: Simple Contrastive Learning of Sentence Embeddings implemented in?",
    "output": "The SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Explain the openai/gpt-3 implementation of GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2111.03026?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the tianjunz/HIR repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the fastai/fastai implementation of Universal Language Model Fine-tuning for Text Classification.",
    "output": "The fastai deep learning library"
  },
  {
    "input": "What is the GitHub repository for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "What topics does the MRKL Systems: A modular approach to AGI implementation cover?",
    "output": "The langchain-ai/langchain repository covers: ai, anthropic, gemini, langchain, llm, openai, python, agents, ai-agents, ai-agents-framework, chatgpt, enterprise, framework, generative-ai, multiagent, open-source, pydantic, rag, aiagentframework."
  },
  {
    "input": "Explain the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Describe the ContextualAI/HALOs repository that implements arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What category is the ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper in?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper published?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Text Embeddings by Weakly-Supervised Contrastive Pre-training implemented in?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Enriching Word Vectors with Subword Information?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the RLHFlow/Directional-Preference-Alignment repository?",
    "output": "The RLHFlow/Directional-Preference-Alignment repository covers: rlhf, ai-alignment, large-language-models."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 repository have?",
    "output": "The Kwai-YuanQi/MM-RLHF repository has 189 stars and 8 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "Where can I find the code for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Attention Is All You Need?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was LIMA: Less Is More for Alignment released?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.00978.",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did LLaMA: Open and Efficient Foundation Language Models come out?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2502.01061?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1710.10903?",
    "output": "Graph Attention Networks"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What language is the google/BIG-bench repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.13971?",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the dmlc/dgl repository?",
    "output": "The dmlc/dgl repository covers: deep-learning, graph-neural-networks."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of Neural Machine Translation by Jointly Learning to Align and Translate.",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is a 2015 paper in Computation and Language (Natural Language Processing). The implementation is available at lisa-groundhog/GroundHog which library for implementing rnns with theano It is written in Python. The repository has 613 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,410 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the THUDM/P-tuning-v2 repository?",
    "output": "The THUDM/P-tuning-v2 repository covers: natural-language-processing, prompt-tuning, pretrained-language-model, p-tuning, parameter-efficient-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.12246?",
    "output": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Masked Autoencoders Are Scalable Vision Learners paper.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,127 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.11331.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "Explain the Denoising Diffusion Probabilistic Models paper.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,906 stars, indicating high community interest."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What topics does the Neural Collaborative Filtering implementation cover?",
    "output": "The hexiangnan/neural_collaborative_filtering repository covers: deep-learning, recommender-system, collaborative-filtering."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Text Embeddings by Weakly-Supervised Contrastive Pre-training belong to?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 implementation?",
    "output": "The rll-research/BPref repository has 133 stars and 31 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What category is the QLoRA: Efficient Finetuning of Quantized LLMs paper in?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.06500?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What paper has ArXiv ID 2401.01335?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "Which field does Reflexion: Language Agents with Verbal Reinforcement Learning belong to?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What does the JohnNay/llm-lobbyist repository do?",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "What language is the QingruZhang/AdaLoRA repository written in?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What topics does the Distilling the Knowledge in a Neural Network implementation cover?",
    "output": "The haitongli/knowledge-distillation-pytorch repository covers: pytorch, knowledge-distillation, deep-neural-networks, cifar10, model-compression, dark-knowledge, computer-vision."
  },
  {
    "input": "What is the GitHub repository for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What does the nlpxucan/WizardLM repository do?",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "What language is the oughtinc/ice repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the XiangLi1999/PrefixTuning repository that implements Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "What's the implementation repository for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What are the key topics in the tensorflow/tensor2tensor repository?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "Explain the Neural Message Passing for Quantum Chemistry paper.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 344 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What are the key topics in the fastai/fastai repository?",
    "output": "The fastai/fastai repository covers: deep-learning, machine-learning, pytorch, python, gpu, fastai, notebooks, colab."
  },
  {
    "input": "What does the rll-research/BPref repository do?",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "Give me an overview of Deep Graph Library: A Graph-Centric, Highly-Performant Package.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,165 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of Denoising Diffusion Probabilistic Models.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,906 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of BEiT: BERT Pre-Training of Image Transformers.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the ShishirPatil/gorilla repository?",
    "output": "The ShishirPatil/gorilla repository covers: api, llm, api-documentation, chatgpt, gpt-4-api, claude-api, openai-api, openai-functions."
  },
  {
    "input": "Give me an overview of Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,974 stars, indicating high community interest."
  },
  {
    "input": "What language is the rll-research/BPref repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2102.02779.",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "Give me an overview of MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,528 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.00902.",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "Which field does CLIP: Learning Transferable Visual Models From Natural Language belong to?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the meta-llama/llama repository written in?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models implementation is written in Python."
  },
  {
    "input": "What language is the facebookresearch/Detectron repository written in?",
    "output": "The Feature Pyramid Networks for Object Detection implementation is written in Python."
  },
  {
    "input": "What topics does the Grounding DINO: Marrying DINO with Grounded Pre-Training implementation cover?",
    "output": "The IDEA-Research/GroundingDINO repository covers: object-detection, open-world, open-world-detection, vision-language, vision-language-transformer."
  },
  {
    "input": "Explain the AkariAsai/self-rag implementation of Self-RAG: Learning to Retrieve, Generate, and Critique.",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the google-research/bert repository do?",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper in?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What language is the ContextualAI/HALOs repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,165 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What topics does the YOLOX: Exceeding YOLO Series in 2021 implementation cover?",
    "output": "The Megvii-BaseDetection/YOLOX repository covers: yolox, yolov3, onnx, tensorrt, ncnn, openvino, pytorch, megengine, object-detection, yolo, deep-learning."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2503.23513?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "In what year did Text Embeddings by Weakly-Supervised Contrastive Pre-training come out?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "What topics does the JAX: Composable transformations of Python+NumPy programs implementation cover?",
    "output": "The jax-ml/jax repository covers: jax."
  },
  {
    "input": "What paper has ArXiv ID 1910.10683?",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "What topics does the MMLU: Measuring Massive Multitask Language Understanding implementation cover?",
    "output": "The hendrycks/test repository covers: muti-task, transfer-learning, gpt-3, few-shot-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.21801?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "In what year did Llama 2: Open Foundation and Fine-Tuned Chat Models come out?",
    "output": "The Llama 2: Open Foundation and Fine-Tuned Chat Models paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 repository have?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "Which field does BERT Rediscovers the Classical NLP Pipeline belong to?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper published?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for LLaMA-Adapter: Efficient Fine-tuning of Language Models?",
    "output": "The code for LLaMA-Adapter: Efficient Fine-tuning of Language Models is available at https://github.com/OpenGVLab/LLaMA-Adapter (OpenGVLab/LLaMA-Adapter)."
  },
  {
    "input": "Explain the BERT: Pre-training of Deep Bidirectional Transformers paper.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the oughtinc/ice repository?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "Explain the Megvii-BaseDetection/YOLOX implementation of YOLOX: Exceeding YOLO Series in 2021.",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "Describe the microsoft/Swin-Transformer repository that implements Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "What does the hexiangnan/neural_collaborative_filtering repository do?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Explain the Playing Atari with Deep Reinforcement Learning paper.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "What category is the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper in?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.04906?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "How many stars does the LLM.int8(): 8-bit Matrix Multiplication for Transformers repository have?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository has 7,804 stars and 798 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Generating Wikipedia by Summarizing Long Sequences implementation?",
    "output": "The tensorflow/tensor2tensor repository has 16,801 stars and 3,698 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "Summarize the Decoupled Weight Decay Regularization paper.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the DreamBooth: Fine Tuning Text-to-Image Diffusion Models implementation?",
    "output": "The google/dreambooth repository has 1,005 stars and 93 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.00735?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10"
  },
  {
    "input": "Describe the salesforce/BLIP repository that implements BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "What is the GitHub repository for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "The code for Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is available at https://github.com/huggingface/sentence-transformers (huggingface/sentence-transformers)."
  },
  {
    "input": "What language is the stanford-futuredata/ColBERT repository written in?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 1508.07909?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "How popular is the Reflexion: Language Agents with Verbal Reinforcement Learning implementation?",
    "output": "The noahshinn/reflexion repository has 2,981 stars and 288 forks on GitHub."
  },
  {
    "input": "Give me an overview of Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,662 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Codex: Evaluating Large Language Models Trained on Code paper.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,048 stars, indicating high community interest."
  },
  {
    "input": "Summarize the High-Resolution Image Synthesis with Latent Diffusion Models paper.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at CompVis/latent-diffusion which high-resolution image synthesis with latent diffusion models It is written in Jupyter Notebook. The repository has 13,646 stars, indicating high community interest."
  },
  {
    "input": "Which field does Decoupled Weight Decay Regularization belong to?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What is the ArXiv category of DistilBERT: a distilled version of BERT?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2208.02294.",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10"
  },
  {
    "input": "In what year did High-Resolution Image Synthesis with Latent Diffusion Models come out?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models paper was published in 2021."
  },
  {
    "input": "What is Animate Anyone: Consistent and Controllable Image-to-Video Synthesis about?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at HumanAIGC/AnimateAnyone which animate anyone: consistent and controllable image-to-video synthesis for character animation The repository has 14,794 stars, indicating high community interest."
  },
  {
    "input": "What year was TinyBERT: Distilling BERT for Natural Language Understanding released?",
    "output": "The TinyBERT: Distilling BERT for Natural Language Understanding paper was published in 2019."
  },
  {
    "input": "Explain the facebookresearch/DPR implementation of Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "What's the implementation repository for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "How popular is the GPT-3: Language Models are Few-Shot Learners implementation?",
    "output": "The openai/gpt-3 repository has 15,774 stars and 2,290 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Summarize the SimCSE: Simple Contrastive Learning of Sentence Embeddings paper.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,623 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What is Adam: A Method for Stochastic Optimization about?",
    "output": "Adam: A Method for Stochastic Optimization is a 2014 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What topics does the The Power of Scale for Parameter-Efficient Prompt Tuning implementation cover?",
    "output": "The google-research/prompt-tuning repository covers: prompt-tuning, nlp, machine-learning, jax, flax, language-model."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.05685.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "What category is the Very Deep Convolutional Networks for Large-Scale Image Recognition paper in?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the frankxu2004/knnlm-why repository that implements arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 implementation?",
    "output": "The RLHFlow/Directional-Preference-Alignment repository has 58 stars and 3 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.02803?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2504.02732?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.03378.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper in?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What category is the Diffusion Models Beat GANs on Image Synthesis paper in?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the XLNet: Generalized Autoregressive Pretraining paper.",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,181 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Explain the microsoft/torchscale implementation of RetNet: Retentive Network: A Successor to Transformer.",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Explain the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Graph Attention Networks implementation?",
    "output": "The PetarV-/GAT repository has 3,469 stars and 672 forks on GitHub."
  },
  {
    "input": "What is Semi-Supervised Classification with Graph Convolutional Networks about?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,339 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Where can I find the code for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.19393?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "Summarize the Efficient Estimation of Word Representations in Vector Space paper.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,572 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "What programming language is Beir: A Heterogeneous Benchmark for IR implemented in?",
    "output": "The Beir: A Heterogeneous Benchmark for IR implementation is written in Python."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What are the key topics in the nyu-mll/jiant repository?",
    "output": "The nyu-mll/jiant repository covers: nlp, sentence-representation, bert, multitask-learning, transformers, transfer-learning."
  },
  {
    "input": "How many stars does the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning repository have?",
    "output": "The haotian-liu/LLaVA repository has 24,140 stars and 2,670 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "Summarize the ChemCrow: Augmenting large-language models with chemistry tools paper.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 845 stars, indicating high community interest."
  },
  {
    "input": "What year was Graph Attention Networks released?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "What topics does the HuggingGPT: Solving AI Tasks with ChatGPT implementation cover?",
    "output": "The microsoft/JARVIS repository covers: deep-learning, platform, pytorch."
  },
  {
    "input": "Describe the google-research/google-research repository that implements Synthetic Data Generation with Large Language Models.",
    "output": "Google Research"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 repository have?",
    "output": "The anthropics/ConstitutionalHarmlessnessPaper repository has 249 stars and 24 forks on GitHub."
  },
  {
    "input": "What programming language is Lookahead Optimizer: k steps forward, 1 step back implemented in?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back implementation is written in Python."
  },
  {
    "input": "Explain the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,924 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Contriever: Unsupervised Dense Information Retrieval.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 766 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Linformer: Self-Attention with Linear Complexity.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis?",
    "output": "The code for Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is available at https://github.com/HumanAIGC/AnimateAnyone (HumanAIGC/AnimateAnyone)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at TsinghuaAI/CPM-1-Generate which chinese pre-trained language models (cpm-lm) version-i It is written in Python. The repository has 1,582 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Universal Language Model Fine-tuning for Text Classification?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1711.00489.",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "What year was Billion-scale similarity search with GPUs released?",
    "output": "The Billion-scale similarity search with GPUs paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 1710.10903.",
    "output": "Graph Attention Networks"
  },
  {
    "input": "How popular is the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation?",
    "output": "The rail-berkeley/softlearning repository has 1,377 stars and 250 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Vicuna: An Open-Source Chatbot?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for RetNet: Retentive Network: A Successor to Transformer?",
    "output": "The code for RetNet: Retentive Network: A Successor to Transformer is available at https://github.com/microsoft/torchscale (microsoft/torchscale)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the LLaMA: Open and Efficient Foundation Language Models paper in?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Self-RAG: Learning to Retrieve, Generate, and Critique paper.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,262 stars, indicating high community interest."
  },
  {
    "input": "Which field does XLNet: Generalized Autoregressive Pretraining belong to?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does RoFormer: Enhanced Transformer with Rotary Position Embedding belong to?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did ViT: An Image is Worth 16x16 Words come out?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "In what year did Segment Anything in Medical Images come out?",
    "output": "The Segment Anything in Medical Images paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.01068.",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10"
  },
  {
    "input": "In what year did GPT-4 Technical Report come out?",
    "output": "The GPT-4 Technical Report paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.03520?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Denoising Diffusion Probabilistic Models?",
    "output": "The code for Denoising Diffusion Probabilistic Models is available at https://github.com/hojonathanho/diffusion (hojonathanho/diffusion)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation?",
    "output": "The ContextualAI/HALOs repository has 894 stars and 50 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the LoRA: Low-Rank Adaptation of Large Language Models repository have?",
    "output": "The microsoft/LoRA repository has 13,038 stars and 866 forks on GitHub."
  },
  {
    "input": "Give me an overview of Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What category is the Distilling the Knowledge in a Neural Network paper in?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Segment Anything paper.",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,837 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.05198.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
  },
  {
    "input": "What paper has ArXiv ID 2301.05062?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "What programming language is MRKL Systems: A modular approach to AGI implemented in?",
    "output": "The MRKL Systems: A modular approach to AGI implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "What category is the Denoising Diffusion Probabilistic Models paper in?",
    "output": "Denoising Diffusion Probabilistic Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the amazon-science/mm-cot repository that implements arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What's the implementation repository for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "The code for Reflexion: Language Agents with Verbal Reinforcement Learning is available at https://github.com/noahshinn/reflexion (noahshinn/reflexion)."
  },
  {
    "input": "What does the allenai/longformer repository do?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Which field does ELECTRA: Pre-training Text Encoders as Discriminators belong to?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the tensorflow/models repository written in?",
    "output": "The Wide & Deep Learning for Recommender Systems implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.11316?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2009.06367?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "How many stars does the Zephyr: Direct Distillation of LM Alignment repository have?",
    "output": "The huggingface/alignment-handbook repository has 5,444 stars and 464 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is available at https://github.com/salesforce/booksum (salesforce/booksum)."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2103.14899?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.08492.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the ContextualAI/HALOs repository do?",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "Which field does RetNet: Retentive Network: A Successor to Transformer belong to?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What is the GitHub repository for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "What's the implementation repository for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is available at https://github.com/tomekkorbak/pretraining-with-human-feedback (tomekkorbak/pretraining-with-human-feedback)."
  },
  {
    "input": "What does the huawei-noah/Pretrained-Language-Model repository do?",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of Denoising Diffusion Implicit Models.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,752 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation cover?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Code Llama: Open Foundation Models for Code repository have?",
    "output": "The meta-llama/codellama repository has 16,365 stars and 1,946 forks on GitHub."
  },
  {
    "input": "What topics does the Multi-Task Deep Neural Networks for Natural Language Understanding implementation cover?",
    "output": "The namisan/mt-dnn repository covers: multi-task-learning, natural-language-understanding, deep-learning, microsoft, ranking, named-entity-recognition, bert, machine-reading-comprehension, nlp, pytorch."
  },
  {
    "input": "What category is the RetNet: Retentive Network: A Successor to Transformer paper in?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "What language is the lucidrains/flamingo-pytorch repository written in?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What year was Enriching Word Vectors with Subword Information released?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Analyzing the Structure of Attention in a Transformer Language Model paper in?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the OpenRLHF/OpenRLHF repository?",
    "output": "The OpenRLHF/OpenRLHF repository covers: transformers, vllm, large-language-models, raylib, reinforcement-learning-from-human-feedback, reinforcement-learning, openai-o1, proximal-policy-optimization."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 repository have?",
    "output": "The Shark-NLP/OpenICL repository has 582 stars and 30 forks on GitHub."
  },
  {
    "input": "Summarize the Llama 2: Open Foundation and Fine-Tuned Chat Models paper.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.05091?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 repository have?",
    "output": "The lucidrains/muse-maskgit-pytorch repository has 919 stars and 89 forks on GitHub."
  },
  {
    "input": "Explain the Denoising Diffusion Implicit Models paper.",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,752 stars, indicating high community interest."
  },
  {
    "input": "What language is the huggingface/sentence-transformers repository written in?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation is written in Python."
  },
  {
    "input": "What language is the salesforce/booksum repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.04190.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the amazon-science/mm-cot repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at zai-org/ImageReward which [neurips 2023] imagereward: learning and evaluating human preferences for text-to-image generation It is written in Python. The repository has 1,594 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 2404.03592?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10"
  },
  {
    "input": "What programming language is MetaGPT: Meta Programming for Multi-Agent Systems implemented in?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems implementation is written in Python."
  },
  {
    "input": "In what year did Longformer: The Long-Document Transformer come out?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 2307.00109?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "Explain the linkedin/gdmix implementation of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10.",
    "output": "A deep ranking personalization framework"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2002.04745?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "What paper has ArXiv ID 2302.03917?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "What language is the turboderp-org/exllamav2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.08073.",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10"
  },
  {
    "input": "In what year did RetNet: Retentive Network: A Successor to Transformer come out?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "In what year did Dream to Control: Learning Behaviors by Latent Imagination come out?",
    "output": "The Dream to Control: Learning Behaviors by Latent Imagination paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is WizardCoder: Empowering Code LLMs with Evol-Instruct implemented in?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is available at https://github.com/uclaml/SPIN (uclaml/SPIN)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the MRKL Systems: A modular approach to AGI repository have?",
    "output": "The langchain-ai/langchain repository has 121,514 stars and 20,041 forks on GitHub."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "How popular is the Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation?",
    "output": "The google-research-datasets/natural-questions repository has 1,076 stars and 157 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Where can I find the code for ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "The code for ColBERT: Efficient Passage Search via Contextualized Late Interaction is available at https://github.com/stanford-futuredata/ColBERT (stanford-futuredata/ColBERT)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the Zephyr: Direct Distillation of LM Alignment paper in?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 958 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the SmoothQuant: Accurate and Efficient Post-Training Quantization repository have?",
    "output": "The mit-han-lab/smoothquant repository has 1,567 stars and 190 forks on GitHub."
  },
  {
    "input": "Summarize the MetaGPT: Meta Programming for Multi-Agent Systems paper.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 60,328 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.02120.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "How popular is the Billion-scale similarity search with GPUs implementation?",
    "output": "The facebookresearch/faiss repository has 38,366 stars and 4,147 forks on GitHub."
  },
  {
    "input": "Explain the BEiT: BERT Pre-Training of Image Transformers paper.",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "What category is the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper in?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.01917.",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "What is the GitHub repository for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "What does the openai/baselines repository do?",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did Lookahead Optimizer: k steps forward, 1 step back come out?",
    "output": "The Lookahead Optimizer: k steps forward, 1 step back paper was published in 2020."
  },
  {
    "input": "Which field does Prefix-Tuning: Optimizing Continuous Prompts for Generation belong to?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implemented in?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the rajpurkar/SQuAD-explorer repository?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Describe the facebookresearch/mae repository that implements Masked Autoencoders Are Scalable Vision Learners.",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "What programming language is DreamFusion: Text-to-3D using 2D Diffusion implemented in?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion implementation is written in Python."
  },
  {
    "input": "What programming language is ViT: An Image is Worth 16x16 Words implemented in?",
    "output": "The ViT: An Image is Worth 16x16 Words implementation is written in Jupyter Notebook."
  },
  {
    "input": "What topics does the Mistral 7B implementation cover?",
    "output": "The mistralai/mistral-inference repository covers: llm, llm-inference, mistralai."
  },
  {
    "input": "What is the GitHub repository for Denoising Diffusion Implicit Models?",
    "output": "The code for Denoising Diffusion Implicit Models is available at https://github.com/ermongroup/ddim (ermongroup/ddim)."
  },
  {
    "input": "What language is the machrisaa/tensorflow-vgg repository written in?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "What programming language is Playing Atari with Deep Reinforcement Learning implemented in?",
    "output": "The Playing Atari with Deep Reinforcement Learning implementation is written in Lua."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "Explain the tensorflow/models implementation of Wide & Deep Learning for Recommender Systems.",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1810.04805?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Where can I find the code for Neural Message Passing for Quantum Chemistry?",
    "output": "The code for Neural Message Passing for Quantum Chemistry is available at https://github.com/priba/nmp_qc (priba/nmp_qc)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,645 stars, indicating high community interest."
  },
  {
    "input": "What category is the ViT: An Image is Worth 16x16 Words paper in?",
    "output": "ViT: An Image is Worth 16x16 Words is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Mamba: Linear-Time Sequence Modeling with Selective State Spaces repository have?",
    "output": "The state-spaces/mamba repository has 16,662 stars and 1,529 forks on GitHub."
  },
  {
    "input": "What category is the Neural Message Passing for Quantum Chemistry paper in?",
    "output": "Neural Message Passing for Quantum Chemistry is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Explain the Densely Connected Convolutional Networks paper.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,848 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is available at https://github.com/tomekkorbak/pretraining-with-human-feedback (tomekkorbak/pretraining-with-human-feedback)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.08913?",
    "output": "RetNet: Retentive Network: A Successor to Transformer"
  },
  {
    "input": "What year was The Power of Scale for Parameter-Efficient Prompt Tuning released?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "What year was ControlNet: Adding Conditional Control to Text-to-Image Diffusion released?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "Which field does TinyBERT: Distilling BERT for Natural Language Understanding belong to?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at allenai/codescientist which codescientist: an automated scientific discovery system for code-based experiments It is written in Python. The repository has 303 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Depthwise Separable Convolutions for Neural Machine Translation paper.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.12895.",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 919 stars, indicating high community interest."
  },
  {
    "input": "What language is the kimiyoung/transformer-xl repository written in?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation is written in Python."
  },
  {
    "input": "What does the Megvii-BaseDetection/YOLOX repository do?",
    "output": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/"
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What language is the tkipf/gcn repository written in?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks implementation is written in Python."
  },
  {
    "input": "Where can I find the code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "Give me an overview of LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,924 stars, indicating high community interest."
  },
  {
    "input": "When was the ChemCrow: Augmenting large-language models with chemistry tools paper published?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.05977?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of MiniGPT-4: Enhancing Vision-Language Understanding.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,762 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Directional-Preference-Alignment which directional preference alignment The repository has 58 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2304.06767?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10"
  },
  {
    "input": "How popular is the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation?",
    "output": "The THUDM/P-tuning-v2 repository has 2,069 stars and 205 forks on GitHub."
  },
  {
    "input": "What language is the volcengine/verl repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2301.13188?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the IDEA-Research/GroundingDINO repository?",
    "output": "The IDEA-Research/GroundingDINO repository covers: object-detection, open-world, open-world-detection, vision-language, vision-language-transformer."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 repository have?",
    "output": "The nyu-mll/quality repository has 144 stars and 10 forks on GitHub."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.10496?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "Where can I find the code for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "How popular is the TinyBERT: Distilling BERT for Natural Language Understanding implementation?",
    "output": "The huawei-noah/Pretrained-Language-Model repository has 3,150 stars and 642 forks on GitHub."
  },
  {
    "input": "What year was Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech released?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "What language is the nlpxucan/WizardLM repository written in?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "Describe the google-research/bert repository that implements BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What paper has ArXiv ID 2309.10305?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "What is the GitHub repository for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "What year was JAX: Composable transformations of Python+NumPy programs released?",
    "output": "The JAX: Composable transformations of Python+NumPy programs paper was published in 2019."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is available at https://github.com/zai-org/ImageReward (zai-org/ImageReward)."
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "Explain the FlashAttention-2: Faster Attention with Better Parallelism paper.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 implementation cover?",
    "output": "The PKU-Alignment/align-anything repository covers: large-language-models, multimodal, rlhf, chameleon, dpo, vision-language-model."
  },
  {
    "input": "What category is the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper in?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/sentence-transformers which state-of-the-art text embeddings It is written in Python. The repository has 17,978 stars, indicating high community interest."
  },
  {
    "input": "What is Neural Collaborative Filtering about?",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,865 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2201.12086?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is available at https://github.com/Crista23/goal_directedness_llms (Crista23/goal_directedness_llms)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "In what year did AudioPaLM: A Large Language Model for Speech Understanding come out?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "What language is the OpenGVLab/LLaMA-Adapter repository written in?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 1804.09301.",
    "output": "Generating Wikipedia by Summarizing Long Sequences"
  },
  {
    "input": "What's the implementation repository for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at OptimalScale/LMFlow which an extensible toolkit for finetuning and inference of large foundation models. large models for all. It is written in Python. The repository has 8,489 stars, indicating high community interest."
  },
  {
    "input": "Describe the tensorflow/tensor2tensor repository that implements Attention Is All You Need.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What language is the mit-han-lab/offsite-tuning repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Generating Wikipedia by Summarizing Long Sequences come out?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/BLIP which pytorch code for blip: bootstrapping language-image pre-training for unified vision-language understanding and generation   It is written in Jupyter Notebook. The repository has 5,603 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Explain the DistilBERT: a distilled version of BERT paper.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 153,664 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Inductive Representation Learning on Large Graphs paper.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,636 stars, indicating high community interest."
  },
  {
    "input": "Which field does XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale belong to?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation?",
    "output": "The microsoft/DeBERTa repository has 2,177 stars and 241 forks on GitHub."
  },
  {
    "input": "When was the Neural Collaborative Filtering paper published?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.10512?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is RoBERTa: A Robustly Optimized BERT Pretraining Approach about?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2201.08239.",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 repository have?",
    "output": "The RLHFlow/Directional-Preference-Alignment repository has 58 stars and 3 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for LLaVA: Large Language and Vision Assistant?",
    "output": "The code for LLaVA: Large Language and Vision Assistant is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is available at https://github.com/google-research/pegasus (google-research/pegasus)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "Which field does Distilling the Knowledge in a Neural Network belong to?",
    "output": "Distilling the Knowledge in a Neural Network is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 repository have?",
    "output": "The uclaml/SPIN repository has 1,226 stars and 104 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the ArXiv category of Reformer: The Efficient Transformer?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the HazyResearch/safari repository that implements arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the nyu-mll/quality repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the nyu-mll/jiant implementation of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "Tell me the title of ArXiv paper 2012.00413.",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "What programming language is DeBERTa: Decoding-enhanced BERT with Disentangled Attention implemented in?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "Summarize the YOLOX: Exceeding YOLO Series in 2021 paper.",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,223 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2105.05233?",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/ConstitutionalHarmlessnessPaper The repository has 249 stars, indicating high community interest."
  },
  {
    "input": "Describe the openai/evals repository that implements GPT-4 Technical Report.",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "What paper has ArXiv ID 2310.06825?",
    "output": "Mistral 7B"
  },
  {
    "input": "What's the implementation repository for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "The code for Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is available at https://github.com/kimiyoung/transformer-xl (kimiyoung/transformer-xl)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 repository have?",
    "output": "The google/BIG-bench repository has 3,167 stars and 616 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the artidoro/qlora repository written in?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What's the implementation repository for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 repository have?",
    "output": "The OptimalScale/LMFlow repository has 8,489 stars and 834 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,056 stars, indicating high community interest."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.04324?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "Explain the Wide & Deep Learning for Recommender Systems paper.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,680 stars, indicating high community interest."
  },
  {
    "input": "Explain the Finetuned Language Models Are Zero-Shot Learners paper.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.04856.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "What programming language is Attention Is All You Need implemented in?",
    "output": "The Attention Is All You Need implementation is written in Python."
  },
  {
    "input": "Which field does Mamba: Linear-Time Sequence Modeling with Selective State Spaces belong to?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning repository have?",
    "output": "The rail-berkeley/softlearning repository has 1,377 stars and 250 forks on GitHub."
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is RoFormer: Enhanced Transformer with Rotary Position Embedding about?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,059 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.03387.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "What programming language is Neural Message Passing for Quantum Chemistry implemented in?",
    "output": "The Neural Message Passing for Quantum Chemistry implementation is written in Python."
  },
  {
    "input": "Give me an overview of Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 956 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the ysymyth/ReAct repository written in?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "When was the MMLU: Measuring Massive Multitask Language Understanding paper published?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding paper was published in 2021."
  },
  {
    "input": "Explain the Feature Pyramid Networks for Object Detection paper.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1412.6980.",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "What are the key topics in the microsoft/Swin-Transformer repository?",
    "output": "The microsoft/Swin-Transformer repository covers: swin-transformer, image-classification, object-detection, semantic-segmentation, imagenet, mscoco, ade20k, mask-rcnn."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.22678.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10"
  },
  {
    "input": "Summarize the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,804 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation?",
    "output": "The GAIR-NLP/LIMO repository has 1,054 stars and 52 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What topics does the LLaVA: Large Language and Vision Assistant implementation cover?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "Summarize the Dense Passage Retrieval for Open-Domain Question Answering paper.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "Which field does Contriever: Unsupervised Dense Information Retrieval belong to?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What year was Very Deep Convolutional Networks for Large-Scale Image Recognition released?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "Summarize the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is a 2023 paper in Machine Learning. The implementation is available at KimMeen/Time-LLM which [iclr 2024] official implementation of \" \ud83e\udd99 time-llm: time series forecasting by reprogramming large language models\" It is written in Python. The repository has 2,410 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the AlibabaResearch/DAMO-ConvAI repository?",
    "output": "The AlibabaResearch/DAMO-ConvAI repository covers: conversational-ai, deep-learning, natural-language-processing, dialog."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2009.06367?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "Describe the google/BIG-bench repository that implements arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10.",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the HuggingGPT: Solving AI Tasks with ChatGPT paper.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,488 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.04324.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "The code for Horovod: fast and easy distributed deep learning in TensorFlow is available at https://github.com/horovod/horovod (horovod/horovod)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the MMLU: Measuring Massive Multitask Language Understanding paper.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,528 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at uclaml/SPIN which the official implementation of self-play fine-tuning (spin) It is written in Python. The repository has 1,226 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2312.02783?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/ConstitutionalHarmlessnessPaper The repository has 249 stars, indicating high community interest."
  },
  {
    "input": "What is DistilBERT: a distilled version of BERT about?",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 153,664 stars, indicating high community interest."
  },
  {
    "input": "Which field does Horovod: fast and easy distributed deep learning in TensorFlow belong to?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of BERT Rediscovers the Classical NLP Pipeline?",
    "output": "BERT Rediscovers the Classical NLP Pipeline is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at PKU-Alignment/align-anything which align anything: training all-modality model with feedback It is written in Python. The repository has 4,601 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2301.13688?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "How popular is the Faster R-CNN: Towards Real-Time Object Detection implementation?",
    "output": "The rbgirshick/py-faster-rcnn repository has 8,278 stars and 4,104 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2406.10209?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "What does the AkariAsai/self-rag repository do?",
    "output": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi."
  },
  {
    "input": "Summarize the TinyBERT: Distilling BERT for Natural Language Understanding paper.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,150 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What language is the microsoft/JARVIS repository written in?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT implementation is written in Python."
  },
  {
    "input": "Which field does FlashAttention-2: Faster Attention with Better Parallelism belong to?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1802.05365.",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "What programming language is ColBERT: Efficient Passage Search via Contextualized Late Interaction implemented in?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation is written in Python."
  },
  {
    "input": "What programming language is MiniGPT-4: Enhancing Vision-Language Understanding implemented in?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding implementation is written in Python."
  },
  {
    "input": "In what year did ControlNet: Adding Conditional Control to Text-to-Image Diffusion come out?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Question and Answer Test-Train Overlap in Open-Domain QA Datasets.",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research-datasets/natural-questions which natural questions (nq) contains real user questions issued to google search, and answers found from wikipedia by annotators. nq is designed for the training and evaluation of automatic question answering systems. It is written in Python. The repository has 1,076 stars, indicating high community interest."
  },
  {
    "input": "What does the ermongroup/ddim repository do?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What language is the GAIR-NLP/LIMO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is in the cs.CY (cs.CY) category."
  },
  {
    "input": "What's the implementation repository for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.22954.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation?",
    "output": "The lucidrains/muse-maskgit-pytorch repository has 919 stars and 89 forks on GitHub."
  },
  {
    "input": "What does the tensorflow/tensor2tensor repository do?",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What year was Diffusion Models Beat GANs on Image Synthesis released?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "When was the ViT: An Image is Worth 16x16 Words paper published?",
    "output": "The ViT: An Image is Worth 16x16 Words paper was published in 2021."
  },
  {
    "input": "What's the implementation repository for Beir: A Heterogeneous Benchmark for IR?",
    "output": "The code for Beir: A Heterogeneous Benchmark for IR is available at https://github.com/beir-cellar/beir (beir-cellar/beir)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "When was the Reflexion: Language Agents with Verbal Reinforcement Learning paper published?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning paper was published in 2023."
  },
  {
    "input": "What category is the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper in?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of TensorFlow: A System for Large-Scale Machine Learning.",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,725 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What language is the deepspeedai/DeepSpeed repository written in?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 repository have?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Explain the bowang-lab/MedSAM implementation of Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2010.02502?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "Describe the facebookresearch/CovidPrognosis repository that implements arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Self-RAG: Learning to Retrieve, Generate, and Critique paper.",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,262 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "What programming language is Contriever: Unsupervised Dense Information Retrieval implemented in?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval implementation is written in Python."
  },
  {
    "input": "What category is the XLNet: Generalized Autoregressive Pretraining paper in?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "What are the key topics in the horovod/horovod repository?",
    "output": "The horovod/horovod repository covers: tensorflow, uber, machine-learning, machinelearning, mpi, baidu, deep-learning, deeplearning, keras, pytorch, mxnet, spark, ray."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "In what year did ZeRO: Memory Optimizations Toward Training Trillion Parameter Models come out?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What language is the tgxs002/align_sd repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.01693?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 repository have?",
    "output": "The volcengine/verl repository has 17,344 stars and 2,782 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the google-research/bert repository that implements arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "How many stars does the Codex: Evaluating Large Language Models Trained on Code repository have?",
    "output": "The openai/human-eval repository has 3,048 stars and 423 forks on GitHub."
  },
  {
    "input": "In what year did ColBERT: Efficient Passage Search via Contextualized Late Interaction come out?",
    "output": "The ColBERT: Efficient Passage Search via Contextualized Late Interaction paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Describe the arcee-ai/mergekit repository that implements arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10.",
    "output": "Tools for merging pretrained large language models."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "Describe the lucidrains/muse-maskgit-pytorch repository that implements arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10.",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tgxs002/align_sd which better aligning text-to-image models with human preference. iccv 2023 It is written in Python. The repository has 293 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech?",
    "output": "The code for Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of PaLM: Scaling Language Modeling with Pathways.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 827 stars, indicating high community interest."
  },
  {
    "input": "What language is the microsoft/DeBERTa repository written in?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation is written in Python."
  },
  {
    "input": "What does the volcengine/verl repository do?",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "What is PyTorch: An Imperative Style, High-Performance Deep Learning Library about?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 1409.1556?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the ChemCrow: Augmenting large-language models with chemistry tools paper.",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools is a 2023 paper in Artificial Intelligence. The implementation is available at ur-whitelab/chemcrow-public which chemcrow It is written in Python. The repository has 845 stars, indicating high community interest."
  },
  {
    "input": "When was the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper published?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "What programming language is Horovod: fast and easy distributed deep learning in TensorFlow implemented in?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow implementation is written in Python."
  },
  {
    "input": "What are the key topics in the google/automl repository?",
    "output": "The google/automl repository covers: automl, efficientdet, object-detection, efficientnet, efficientnetv2."
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "In what year did T5: Exploring the Limits of Transfer Learning come out?",
    "output": "The T5: Exploring the Limits of Transfer Learning paper was published in 2019."
  },
  {
    "input": "What does the machrisaa/tensorflow-vgg repository do?",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "What's the implementation repository for PyTorch: An Imperative Style, High-Performance Deep Learning Library?",
    "output": "The code for PyTorch: An Imperative Style, High-Performance Deep Learning Library is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.08485?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "What year was Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks released?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2210.01241.",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "The code for XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.03387?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10"
  },
  {
    "input": "Describe the hexiangnan/neural_collaborative_filtering repository that implements Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 repository have?",
    "output": "The simplescaling/s1 repository has 6,611 stars and 762 forks on GitHub."
  },
  {
    "input": "What year was BEiT: BERT Pre-Training of Image Transformers released?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at openai/following-instructions-human-feedback The repository has 1,251 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2110.07602?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Linformer: Self-Attention with Linear Complexity paper.",
    "output": "Linformer: Self-Attention with Linear Complexity is a 2021 paper in Machine Learning. The implementation is available at tatp22/linformer-pytorch which my take on a practical implementation of linformer for pytorch. It is written in Python. The repository has 421 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "When was the BERT: Pre-training of Deep Bidirectional Transformers paper published?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "What are the key topics in the pyg-team/pytorch_geometric repository?",
    "output": "The pyg-team/pytorch_geometric repository covers: pytorch, geometric-deep-learning, graph-neural-networks, deep-learning, graph-convolutional-networks."
  },
  {
    "input": "What paper has ArXiv ID 2302.10866?",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "Which field does Code Llama: Open Foundation Models for Code belong to?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 233 stars, indicating high community interest."
  },
  {
    "input": "What category is the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper in?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the Vision-CAIR/MiniGPT-4 repository written in?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding implementation is written in Python."
  },
  {
    "input": "How many stars does the DreamBooth: Fine Tuning Text-to-Image Diffusion Models repository have?",
    "output": "The google/dreambooth repository has 1,005 stars and 93 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What does the openai/summarize-from-feedback repository do?",
    "output": "Code for \"Learning to summarize from human feedback\""
  },
  {
    "input": "Explain the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,793 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Which field does Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context belong to?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the meta-llama/llama repository that implements LLaMA: Open and Efficient Foundation Language Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Explain the yafuly/TPO implementation of arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10.",
    "output": "Test-time preferenece optimization (ICML 2025)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.10209.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10"
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1703.04691?",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "What does the Kwai-YuanQi/MM-RLHF repository do?",
    "output": "The Next Step Forward in Multimodal LLM Alignment"
  },
  {
    "input": "Describe the ZhuiyiTechnology/roformer repository that implements RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "Rotary Transformer"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "How popular is the LLaVA: Large Language and Vision Assistant implementation?",
    "output": "The haotian-liu/LLaVA repository has 24,140 stars and 2,670 forks on GitHub."
  },
  {
    "input": "Describe the google/BIG-bench repository that implements arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10.",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "What is the ArXiv category of Efficient Estimation of Word Representations in Vector Space?",
    "output": "Efficient Estimation of Word Representations in Vector Space is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the KaimingHe/deep-residual-networks implementation of Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "How many stars does the Self-RAG: Learning to Retrieve, Generate, and Critique repository have?",
    "output": "The AkariAsai/self-rag repository has 2,262 stars and 214 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation?",
    "output": "The google-deepmind/tracr repository has 548 stars and 47 forks on GitHub."
  },
  {
    "input": "What year was MetaGPT: Meta Programming for Multi-Agent Systems released?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "Which field does PyTorch Geometric: Library for Geometric Deep Learning belong to?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation cover?",
    "output": "The deepspeedai/DeepSpeed repository covers: deep-learning, pytorch, gpu, machine-learning, billion-parameters, data-parallelism, model-parallelism, inference, pipeline-parallelism, compression, mixture-of-experts, trillion-parameters, zero."
  },
  {
    "input": "What is the ArXiv category of CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does LLaVA: Large Language and Vision Assistant belong to?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.05638?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "What is Reflexion: Language Agents with Verbal Reinforcement Learning about?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,981 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for SmoothQuant: Accurate and Efficient Post-Training Quantization?",
    "output": "The code for SmoothQuant: Accurate and Efficient Post-Training Quantization is available at https://github.com/mit-han-lab/smoothquant (mit-han-lab/smoothquant)."
  },
  {
    "input": "How many stars does the EfficientDet: Scalable and Efficient Object Detection repository have?",
    "output": "The google/automl repository has 6,430 stars and 1,460 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/DPR repository do?",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "What are the key topics in the tatsu-lab/alpaca_eval repository?",
    "output": "The tatsu-lab/alpaca_eval repository covers: deep-learning, evaluation, foundation-models, instruction-following, large-language-models, leaderboard, nlp, rlhf."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "Explain the huggingface/alignment-handbook implementation of Zephyr: Direct Distillation of LM Alignment.",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.09251?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10"
  },
  {
    "input": "What does the facebookresearch/DPR repository do?",
    "output": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task."
  },
  {
    "input": "What paper has ArXiv ID 2302.04023?",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,273 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "What paper has ArXiv ID 2306.04190?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "What is the ArXiv category of Competition-Level Code Generation with AlphaCode?",
    "output": "Competition-Level Code Generation with AlphaCode is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2306.00978?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What programming language is ALBERT: A Lite BERT implemented in?",
    "output": "The ALBERT: A Lite BERT implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2101.04909?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "What does the fastai/fastai repository do?",
    "output": "The fastai deep learning library"
  },
  {
    "input": "What category is the Flamingo: A Visual Language Model for Few-Shot Learning paper in?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 implementation?",
    "output": "The anthropics/hh-rlhf repository has 1,801 stars and 150 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.14899.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00303.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.03264.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the OpenGVLab/LLaMA-Adapter repository do?",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2308.08155?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems"
  },
  {
    "input": "Explain the meta-llama/llama implementation of LIMA: Less Is More for Alignment.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.09332?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10"
  },
  {
    "input": "Summarize the GPT-3: Language Models are Few-Shot Learners paper.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,774 stars, indicating high community interest."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the volcengine/verl repository do?",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "Summarize the P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,069 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1903.07293?",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning"
  },
  {
    "input": "Tell me the title of ArXiv paper 2212.04356.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "What are the key topics in the tomekkorbak/pretraining-with-human-feedback repository?",
    "output": "The tomekkorbak/pretraining-with-human-feedback repository covers: ai-alignment, ai-safety, decision-transformers, gpt, language-models, pretraining, reinforcement-learning, rlhf."
  },
  {
    "input": "How popular is the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation?",
    "output": "The microsoft/Swin-Transformer repository has 15,517 stars and 2,202 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2004.08492?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10"
  },
  {
    "input": "What is Denoising Diffusion Implicit Models about?",
    "output": "Denoising Diffusion Implicit Models is a 2020 paper in Machine Learning. The implementation is available at ermongroup/ddim which denoising diffusion implicit models It is written in Python. The repository has 1,752 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1706.02216.",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "What's the implementation repository for MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "The code for MiniGPT-4: Enhancing Vision-Language Understanding is available at https://github.com/Vision-CAIR/MiniGPT-4 (Vision-CAIR/MiniGPT-4)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2401.01335.",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.18290.",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10"
  },
  {
    "input": "Explain the facebookresearch/fastText implementation of Enriching Word Vectors with Subword Information.",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.10425.",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 repository have?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs is a 2023 paper in Machine Learning. The implementation is available at artidoro/qlora which qlora: efficient finetuning of quantized llms It is written in Jupyter Notebook. The repository has 10,782 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at OpenRLHF/OpenRLHF which an easy-to-use, scalable and high-performance rlhf framework based on ray (ppo & grpo & reinforce++ & vllm & ray & dynamic sampling & async agentic rl) It is written in Python. The repository has 8,562 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1512.03385?",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.00190?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the lucidrains/PaLM-pytorch repository written in?",
    "output": "The PaLM: Scaling Language Modeling with Pathways implementation is written in Python."
  },
  {
    "input": "What topics does the Lookahead Optimizer: k steps forward, 1 step back implementation cover?",
    "output": "The alphadl/lookahead.pytorch repository covers: pytorch, optimizer, lookahead."
  },
  {
    "input": "What is the ArXiv category of BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the ALBERT: A Lite BERT implementation?",
    "output": "The google-research/albert repository has 3,275 stars and 573 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2109.03264?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.14525.",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10"
  },
  {
    "input": "Which field does Inductive Representation Learning on Large Graphs belong to?",
    "output": "Inductive Representation Learning on Large Graphs is in the cs.SI (cs.SI) category."
  },
  {
    "input": "Give me an overview of LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 13,038 stars, indicating high community interest."
  },
  {
    "input": "Explain the autonomousvision/stylegan-t implementation of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Explain the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is a 2022 paper in Machine Learning. The implementation is available at bitsandbytes-foundation/bitsandbytes which accessible large language models via k-bit quantization for pytorch. It is written in Python. The repository has 7,804 stars, indicating high community interest."
  },
  {
    "input": "In what year did TensorFlow: A System for Large-Scale Machine Learning come out?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "How popular is the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models implementation?",
    "output": "The deepspeedai/DeepSpeed repository has 40,956 stars and 4,657 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the ArXiv category of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "The code for Mamba: Linear-Time Sequence Modeling with Selective State Spaces is available at https://github.com/state-spaces/mamba (state-spaces/mamba)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Where can I find the code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models?",
    "output": "The code for Time-LLM: Time Series Forecasting by Reprogramming Large Language Models is available at https://github.com/KimMeen/Time-LLM (KimMeen/Time-LLM)."
  },
  {
    "input": "What is the GitHub repository for Graph Attention Networks?",
    "output": "The code for Graph Attention Networks is available at https://github.com/PetarV-/GAT (PetarV-/GAT)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was PyTorch: An Imperative Style, High-Performance Deep Learning Library released?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "Give me an overview of Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,517 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Linear95/APO which code for acl2024 paper - adversarial preference optimization (apo). It is written in Python. The repository has 56 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.01652?",
    "output": "Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at yafuly/TPO which test-time preferenece optimization (icml 2025). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1907.11692?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the pytorch/pytorch repository that implements Decoupled Weight Decay Regularization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "Which field does ZeRO: Memory Optimizations Toward Training Trillion Parameter Models belong to?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision?",
    "output": "The code for Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is available at https://github.com/openai/whisper (openai/whisper)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.03262?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10"
  },
  {
    "input": "Explain the Mistral 7B paper.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,577 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What category is the LLM.int8(): 8-bit Matrix Multiplication for Transformers paper in?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "In what year did Segment Anything come out?",
    "output": "The Segment Anything paper was published in 2023."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 implementation?",
    "output": "The Kwai-YuanQi/MM-RLHF repository has 189 stars and 8 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Explain the PaLM: Scaling Language Modeling with Pathways paper.",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 827 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "Describe the llava-rlhf/LLaVA-RLHF repository that implements arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10.",
    "output": "Aligning LMMs with Factually Augmented RLHF"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Flamingo: A Visual Language Model for Few-Shot Learning implementation?",
    "output": "The lucidrains/flamingo-pytorch repository has 1,273 stars and 66 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the NVlabs/prismer repository that implements arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What is Llama 2: Open Foundation and Fine-Tuned Chat Models about?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation?",
    "output": "The siddk/voltron-robotics repository has 233 stars and 24 forks on GitHub."
  },
  {
    "input": "What are the key topics in the mit-han-lab/offsite-tuning repository?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "In what year did FlashAttention-2: Faster Attention with Better Parallelism come out?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1906.04341?",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "What language is the haitongli/knowledge-distillation-pytorch repository written in?",
    "output": "The Distilling the Knowledge in a Neural Network implementation is written in Python."
  },
  {
    "input": "What category is the Proximal Policy Optimization Algorithms paper in?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper published?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "How many stars does the RetNet: Retentive Network: A Successor to Transformer repository have?",
    "output": "The microsoft/torchscale repository has 3,124 stars and 222 forks on GitHub."
  },
  {
    "input": "What year was Decoupled Weight Decay Regularization released?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "Give me an overview of JAX: Composable transformations of Python+NumPy programs.",
    "output": "JAX: Composable transformations of Python+NumPy programs is a 2019 paper in cs.MS. The implementation is available at jax-ml/jax which composable transformations of python+numpy programs: differentiate, vectorize, jit to gpu/tpu, and more It is written in Python. The repository has 34,268 stars, indicating high community interest."
  },
  {
    "input": "What is LLaVA-1.5: Improved Baselines with Visual Instruction Tuning about?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What is WizardCoder: Empowering Code LLMs with Evol-Instruct about?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,471 stars, indicating high community interest."
  },
  {
    "input": "When was the Denoising Diffusion Probabilistic Models paper published?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "Explain the StarCoder: May the source be with you! paper.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,478 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "What category is the Synthetic Data Generation with Large Language Models paper in?",
    "output": "Synthetic Data Generation with Large Language Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "What is XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale about?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What is An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale about?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 repository have?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at yafuly/TPO which test-time preferenece optimization (icml 2025). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2308.12950?",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Describe the IDEA-Research/GroundingDINO repository that implements Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What language is the uclaml/SPIN repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 repository have?",
    "output": "The llava-rlhf/LLaVA-RLHF repository has 387 stars and 26 forks on GitHub."
  },
  {
    "input": "Which field does Graph Attention Networks belong to?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2409.19256?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Describe the openai/gpt-3 repository that implements GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What is Very Deep Convolutional Networks for Large-Scale Image Recognition about?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is a 2014 paper in Computer Vision and Pattern Recognition. The implementation is available at machrisaa/tensorflow-vgg which vgg19 and vgg16 on tensorflow It is written in Python. The repository has 2,240 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1910.01741?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "Explain the jacopotagliabue/you-dont-need-a-bigger-boat implementation of arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "Explain the ali-vilab/composer implementation of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\""
  },
  {
    "input": "Where can I find the code for ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "The code for ReAct: Synergizing Reasoning and Acting in Language Models is available at https://github.com/ysymyth/ReAct (ysymyth/ReAct)."
  },
  {
    "input": "How many stars does the Neural Machine Translation by Jointly Learning to Align and Translate repository have?",
    "output": "The lisa-groundhog/GroundHog repository has 613 stars and 230 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the FMInference/FlexLLMGen repository that implements arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "How popular is the BERT: Pre-training of Deep Bidirectional Transformers implementation?",
    "output": "The google-research/bert repository has 39,724 stars and 9,710 forks on GitHub."
  },
  {
    "input": "What year was Prefix-Tuning: Optimizing Continuous Prompts for Generation released?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation paper was published in 2021."
  },
  {
    "input": "What topics does the DeBERTa: Decoding-enhanced BERT with Disentangled Attention implementation cover?",
    "output": "The microsoft/DeBERTa repository covers: bert, deeplearning, representation-learning, roberta, language-model, natural-language-understanding, self-attention, transformer-encoder."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of MiniGPT-4: Enhancing Vision-Language Understanding?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2210.01241?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10"
  },
  {
    "input": "Explain the hendrycks/test implementation of MMLU: Measuring Massive Multitask Language Understanding.",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is available at https://github.com/openai/frontier-evals (openai/frontier-evals)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 582 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Segment Anything in Medical Images implemented in?",
    "output": "The Segment Anything in Medical Images implementation is written in Jupyter Notebook."
  },
  {
    "input": "Summarize the Flamingo: A Visual Language Model for Few-Shot Learning paper.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at lucidrains/flamingo-pytorch which implementation of \ud83e\udda9 flamingo, state-of-the-art few-shot visual question answering attention net out of deepmind, in pytorch It is written in Python. The repository has 1,273 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the tgxs002/align_sd repository do?",
    "output": "Better Aligning Text-to-Image Models with Human Preference. ICCV 2023"
  },
  {
    "input": "What paper has ArXiv ID 2209.07858?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of Zephyr: Direct Distillation of LM Alignment.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,444 stars, indicating high community interest."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at JohnNay/llm-lobbyist which code for the paper: \"large language models as corporate lobbyists\" (2023). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2010.11929?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What year was Synthetic Data Generation with Large Language Models released?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "Explain the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "What year was P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning released?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "How many stars does the TensorFlow: A System for Large-Scale Machine Learning repository have?",
    "output": "The tensorflow/tensorflow repository has 192,725 stars and 75,036 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "In what year did RoFormer: Enhanced Transformer with Rotary Position Embedding come out?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2502.14815?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1808.07042?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "What programming language is StarCoder: May the source be with you! implemented in?",
    "output": "The StarCoder: May the source be with you! implementation is written in Python."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of Decoupled Weight Decay Regularization.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "How popular is the Segment Anything implementation?",
    "output": "The facebookresearch/segment-anything repository has 52,837 stars and 6,170 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.04190?",
    "output": "ChemCrow: Augmenting large-language models with chemistry tools"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is available at https://github.com/GanjinZero/RRHF (GanjinZero/RRHF)."
  },
  {
    "input": "What category is the Decoupled Weight Decay Regularization paper in?",
    "output": "Decoupled Weight Decay Regularization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding about?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,674 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.12597?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.02828.",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "In what year did Diffusion Models Beat GANs on Image Synthesis come out?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "What programming language is Adam: A Method for Stochastic Optimization implemented in?",
    "output": "The Adam: A Method for Stochastic Optimization implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "What year was Denoising Diffusion Implicit Models released?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "What language is the google-research/pegasus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Contriever: Unsupervised Dense Information Retrieval?",
    "output": "The code for Contriever: Unsupervised Dense Information Retrieval is available at https://github.com/facebookresearch/contriever (facebookresearch/contriever)."
  },
  {
    "input": "What category is the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper in?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2303.03378?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "Which field does Self-RAG: Learning to Retrieve, Generate, and Critique belong to?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2106.05237?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Summarize the Reflexion: Language Agents with Verbal Reinforcement Learning paper.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,981 stars, indicating high community interest."
  },
  {
    "input": "Which field does MetaGPT: Meta Programming for Multi-Agent Systems belong to?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What programming language is Neural Machine Translation by Jointly Learning to Align and Translate implemented in?",
    "output": "The Neural Machine Translation by Jointly Learning to Align and Translate implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2212.08073?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.07863.",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "Explain the RLHFlow/Directional-Preference-Alignment implementation of arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10.",
    "output": "Directional Preference Alignment"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 958 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did BEiT: BERT Pre-Training of Image Transformers come out?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers paper was published in 2021."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.01693.",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 implementation cover?",
    "output": "The uclaml/SPIN repository covers: deep-learning, fine-tuning, large-language-models, self-play."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was QLoRA: Efficient Finetuning of Quantized LLMs released?",
    "output": "The QLoRA: Efficient Finetuning of Quantized LLMs paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at llava-rlhf/LLaVA-RLHF which aligning lmms with factually augmented rlhf It is written in Python. The repository has 387 stars, indicating high community interest."
  },
  {
    "input": "How popular is the YOLOX: Exceeding YOLO Series in 2021 implementation?",
    "output": "The Megvii-BaseDetection/YOLOX repository has 10,223 stars and 2,419 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "How popular is the MRKL Systems: A modular approach to AGI implementation?",
    "output": "The langchain-ai/langchain repository has 121,514 stars and 20,041 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at zai-org/ImageReward which [neurips 2023] imagereward: learning and evaluating human preferences for text-to-image generation It is written in Python. The repository has 1,594 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation cover?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "Summarize the Attention Is All You Need paper.",
    "output": "Attention Is All You Need is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,800 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2504.11844?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2012.06009?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "What language is the GanjinZero/RRHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Reflexion: Language Agents with Verbal Reinforcement Learning implemented in?",
    "output": "The Reflexion: Language Agents with Verbal Reinforcement Learning implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the arcee-ai/mergekit repository do?",
    "output": "Tools for merging pretrained large language models."
  },
  {
    "input": "What is ZeRO: Memory Optimizations Toward Training Trillion Parameter Models about?",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,956 stars, indicating high community interest."
  },
  {
    "input": "What does the 0nutation/SpeechGPT repository do?",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "Give me an overview of Distilling the Knowledge in a Neural Network.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,974 stars, indicating high community interest."
  },
  {
    "input": "What category is the Gorilla: Large Language Model Connected with Massive APIs paper in?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Neural Collaborative Filtering released?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "What is Diffusion Models Beat GANs on Image Synthesis about?",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,183 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Enriching Word Vectors with Subword Information implemented in?",
    "output": "The Enriching Word Vectors with Subword Information implementation is written in HTML."
  },
  {
    "input": "Explain the allenai/RL4LMs implementation of arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10.",
    "output": "A modular RL library to fine-tune language models to human preferences"
  },
  {
    "input": "What's the implementation repository for Vicuna: An Open-Source Chatbot?",
    "output": "The code for Vicuna: An Open-Source Chatbot is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.03708?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for MRKL Systems: A modular approach to AGI?",
    "output": "The code for MRKL Systems: A modular approach to AGI is available at https://github.com/langchain-ai/langchain (langchain-ai/langchain)."
  },
  {
    "input": "What category is the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper in?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "What year was DreamFusion: Text-to-3D using 2D Diffusion released?",
    "output": "The DreamFusion: Text-to-3D using 2D Diffusion paper was published in 2022."
  },
  {
    "input": "What year was Generating Wikipedia by Summarizing Long Sequences released?",
    "output": "The Generating Wikipedia by Summarizing Long Sequences paper was published in 2018."
  },
  {
    "input": "When was the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper published?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "How popular is the ColBERT: Efficient Passage Search via Contextualized Late Interaction implementation?",
    "output": "The stanford-futuredata/ColBERT repository has 3,724 stars and 462 forks on GitHub."
  },
  {
    "input": "Describe the Linear95/APO repository that implements arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10.",
    "output": "Code for ACL2024 paper - Adversarial Preference Optimization (APO)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the openai/human-eval repository that implements Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Code for the paper \"Evaluating Large Language Models Trained on Code\""
  },
  {
    "input": "In what year did BERT: Pre-training of Deep Bidirectional Transformers come out?",
    "output": "The BERT: Pre-training of Deep Bidirectional Transformers paper was published in 2018."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.05638.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
  },
  {
    "input": "Describe the tatp22/linformer-pytorch repository that implements Linformer: Self-Attention with Linear Complexity.",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the Diffusion Models Beat GANs on Image Synthesis paper published?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis paper was published in 2021."
  },
  {
    "input": "What paper has ArXiv ID 2203.05556?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.13971.",
    "output": "LLaMA: Open and Efficient Foundation Language Models"
  },
  {
    "input": "What is the GitHub repository for BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "The code for BERT: Pre-training of Deep Bidirectional Transformers is available at https://github.com/google-research/bert (google-research/bert)."
  },
  {
    "input": "Describe the salesforce/LAVIS repository that implements InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the GPT-4 Technical Report paper.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,406 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2208.02294?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the ur-whitelab/chemcrow-public implementation of ChemCrow: Augmenting large-language models with chemistry tools.",
    "output": "Chemcrow"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What does the lm-sys/FastChat repository do?",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Which field does Whisper: Robust Speech Recognition via Large-Scale Weak Supervision belong to?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What is Prefix-Tuning: Optimizing Continuous Prompts for Generation about?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 956 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2308.12950?",
    "output": "Code Llama: Open Foundation Models for Code"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What topics does the BLIP-2: Bootstrapping Language-Image Pre-training implementation cover?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "Where can I find the code for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Where can I find the code for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What language is the siddk/voltron-robotics repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "What are the key topics in the alphadl/lookahead.pytorch repository?",
    "output": "The alphadl/lookahead.pytorch repository covers: pytorch, optimizer, lookahead."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is available at https://github.com/volcengine/verl (volcengine/verl)."
  },
  {
    "input": "What does the namisan/mt-dnn repository do?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "In what year did Distilling the Knowledge in a Neural Network come out?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Summarize the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper.",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Which field does Densely Connected Convolutional Networks belong to?",
    "output": "Densely Connected Convolutional Networks is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What does the microsoft/Swin-Transformer repository do?",
    "output": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\"."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 repository have?",
    "output": "The JohnNay/llm-lobbyist repository has 172 stars and 15 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1802.05365?",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tgxs002/align_sd which better aligning text-to-image models with human preference. iccv 2023 It is written in Python. The repository has 293 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "When was the Flamingo: A Visual Language Model for Few-Shot Learning paper published?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "Explain the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is a 2023 paper in Machine Learning. The implementation is available at state-spaces/mamba which mamba ssm architecture It is written in Python. The repository has 16,662 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1312.5602?",
    "output": "Playing Atari with Deep Reinforcement Learning"
  },
  {
    "input": "How popular is the T5: Exploring the Limits of Transfer Learning implementation?",
    "output": "The google-research/text-to-text-transfer-transformer repository has 6,458 stars and 788 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "Summarize the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper.",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at OpenGVLab/LLaMA-Adapter which [iclr 2024] fine-tuning llama to follow instructions within 1 hour and 1.2m parameters It is written in Python. The repository has 5,924 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is available at https://github.com/tianjunz/HIR (tianjunz/HIR)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What year was TensorFlow: A System for Large-Scale Machine Learning released?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "Where can I find the code for RoBERTa: A Robustly Optimized BERT Pretraining Approach?",
    "output": "The code for RoBERTa: A Robustly Optimized BERT Pretraining Approach is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Give me an overview of InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Neural Message Passing for Quantum Chemistry.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 344 stars, indicating high community interest."
  },
  {
    "input": "What year was Reformer: The Efficient Transformer released?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.05206.",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale?",
    "output": "The code for An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "Explain the facebookresearch/segment-anything implementation of Segment Anything.",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.14838.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for MetaGPT: Meta Programming for Multi-Agent Systems?",
    "output": "The code for MetaGPT: Meta Programming for Multi-Agent Systems is available at https://github.com/FoundationAgents/MetaGPT (FoundationAgents/MetaGPT)."
  },
  {
    "input": "What topics does the Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "When was the Feature Pyramid Networks for Object Detection paper published?",
    "output": "The Feature Pyramid Networks for Object Detection paper was published in 2016."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.05685?",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
  },
  {
    "input": "What does the google/BIG-bench repository do?",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "What year was BLIP: Bootstrapping Language-Image Pre-training released?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training paper was published in 2022."
  },
  {
    "input": "Explain the ysymyth/ReAct implementation of ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.09431.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "What programming language is JAX: Composable transformations of Python+NumPy programs implemented in?",
    "output": "The JAX: Composable transformations of Python+NumPy programs implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 repository have?",
    "output": "The rll-research/BPref repository has 133 stars and 31 forks on GitHub."
  },
  {
    "input": "What language is the oughtinc/ice repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the nlpxucan/WizardLM repository that implements WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04226.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.05062?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the haotian-liu/LLaVA implementation of LLaVA: Large Language and Vision Assistant.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "Describe the beir-cellar/beir repository that implements Beir: A Heterogeneous Benchmark for IR.",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "What paper has ArXiv ID 2302.08575?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "What does the haotian-liu/LLaVA repository do?",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "Explain the NVlabs/prismer implementation of arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1909.10351?",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "What is the GitHub repository for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "What language is the AkariAsai/self-rag repository written in?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is SimCSE: Simple Contrastive Learning of Sentence Embeddings about?",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at princeton-nlp/SimCSE which [emnlp 2021] simcse: simple contrastive learning of sentence embeddings https://arxiv.org/abs/2104.08821 It is written in Python. The repository has 3,623 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 implementation?",
    "output": "The turboderp-org/exllamav2 repository has 4,380 stars and 325 forks on GitHub."
  },
  {
    "input": "What is Decoupled Weight Decay Regularization about?",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "How popular is the ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation?",
    "output": "The lllyasviel/ControlNet repository has 33,415 stars and 2,992 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "What year was ReAct: Synergizing Reasoning and Acting in Language Models released?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "What topics does the Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation cover?",
    "output": "The microsoft/unilm repository covers: nlp, pre-trained-model, unilm, minilm, layoutlm, layoutxlm, beit, document-ai, trocr, beit-3, foundation-models, xlm-e, deepnet, llm, multimodal, mllm, kosmos, kosmos-1, textdiffuser, bitnet."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "Which field does SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding belong to?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Adam: A Method for Stochastic Optimization belong to?",
    "output": "Adam: A Method for Stochastic Optimization is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Graph Attention Networks?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the microsoft/torchscale repository do?",
    "output": "Foundation Architecture for (M)LLMs"
  },
  {
    "input": "When was the Denoising Diffusion Implicit Models paper published?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/CovidPrognosis repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2504.21801?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10"
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "How popular is the S4: Efficiently Modeling Long Sequences implementation?",
    "output": "The state-spaces/s4 repository has 2,794 stars and 349 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "What are the key topics in the facebookresearch/CovidPrognosis repository?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "In what year did Neural Collaborative Filtering come out?",
    "output": "The Neural Collaborative Filtering paper was published in 2017."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "What topics does the AlpacaEval: An Automatic Evaluator for Instruction-following implementation cover?",
    "output": "The tatsu-lab/alpaca_eval repository covers: deep-learning, evaluation, foundation-models, instruction-following, large-language-models, leaderboard, nlp, rlhf."
  },
  {
    "input": "What year was ChemCrow: Augmenting large-language models with chemistry tools released?",
    "output": "The ChemCrow: Augmenting large-language models with chemistry tools paper was published in 2023."
  },
  {
    "input": "What language is the farizrahman4u/seq2seq repository written in?",
    "output": "The Sequence to Sequence Learning with Neural Networks implementation is written in Python."
  },
  {
    "input": "What year was Grounding DINO: Marrying DINO with Grounded Pre-Training released?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Summarize the Gorilla: Large Language Model Connected with Massive APIs paper.",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at ShishirPatil/gorilla which gorilla: training and evaluating llms for function calls (tool calls) It is written in Python. The repository has 12,603 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2112.09332?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the deepspeedai/DeepSpeed repository?",
    "output": "The deepspeedai/DeepSpeed repository covers: deep-learning, pytorch, gpu, machine-learning, billion-parameters, data-parallelism, model-parallelism, inference, pipeline-parallelism, compression, mixture-of-experts, trillion-parameters, zero."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 implementation?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What is the GitHub repository for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "How many stars does the Adam: A Method for Stochastic Optimization repository have?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding?",
    "output": "The code for SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is available at https://github.com/nyu-mll/jiant (nyu-mll/jiant)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1512.03385.",
    "output": "Deep Residual Learning for Image Recognition"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Feature Pyramid Networks for Object Detection?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the lm-sys/FastChat repository written in?",
    "output": "The Vicuna: An Open-Source Chatbot implementation is written in Python."
  },
  {
    "input": "Explain the volcengine/verl implementation of arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10.",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "What's the implementation repository for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What does the tensorflow/tensorflow repository do?",
    "output": "An Open Source Machine Learning Framework for Everyone"
  },
  {
    "input": "What is the GitHub repository for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at allenai/RL4LMs which a modular rl library to fine-tune language models to human preferences It is written in Python. The repository has 2,372 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2303.09431?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 1902.05188?",
    "output": "JAX: Composable transformations of Python+NumPy programs"
  },
  {
    "input": "Summarize the Denoising Diffusion Probabilistic Models paper.",
    "output": "Denoising Diffusion Probabilistic Models is a 2020 paper in Machine Learning. The implementation is available at hojonathanho/diffusion which denoising diffusion probabilistic models It is written in Python. The repository has 4,906 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for StarCoder: May the source be with you!?",
    "output": "The code for StarCoder: May the source be with you! is available at https://github.com/bigcode-project/starcoder (bigcode-project/starcoder)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.07185.",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1912.01703?",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the facebookresearch/faiss repository that implements Billion-scale similarity search with GPUs.",
    "output": "A library for efficient similarity search and clustering of dense vectors."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models repository have?",
    "output": "The KimMeen/Time-LLM repository has 2,410 stars and 424 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does YOLOX: Exceeding YOLO Series in 2021 belong to?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does ColBERT: Efficient Passage Search via Contextualized Late Interaction belong to?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 repository have?",
    "output": "The frankxu2004/knnlm-why repository has 59 stars and 3 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,341 stars, indicating high community interest."
  },
  {
    "input": "What year was Codex: Evaluating Large Language Models Trained on Code released?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "Where can I find the code for PyTorch Geometric: Library for Geometric Deep Learning?",
    "output": "The code for PyTorch Geometric: Library for Geometric Deep Learning is available at https://github.com/pyg-team/pytorch_geometric (pyg-team/pytorch_geometric)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 repository have?",
    "output": "The siddk/voltron-robotics repository has 233 stars and 24 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the openai/CLIP repository written in?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language implementation is written in Jupyter Notebook."
  },
  {
    "input": "What paper has ArXiv ID 2405.04434?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at yafuly/TPO which test-time preferenece optimization (icml 2025). It is written in Jupyter Notebook. The repository has 172 stars, indicating high community interest."
  },
  {
    "input": "What does the bigcode-project/starcoder repository do?",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "What is the GitHub repository for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "What does the shizhediao/active-prompt repository do?",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation?",
    "output": "The MAXNORM8650/MedAgentSim repository has 101 stars and 12 forks on GitHub."
  },
  {
    "input": "Give me an overview of Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,680 stars, indicating high community interest."
  },
  {
    "input": "What does the ContextualAI/HALOs repository do?",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Explain the LTH14/fractalgen implementation of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of YOLOX: Exceeding YOLO Series in 2021?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the InPars: Data Augmentation for Information Retrieval paper.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at GanjinZero/RRHF which [nips2023] rrhf & wombat It is written in Python. The repository has 811 stars, indicating high community interest."
  },
  {
    "input": "Which field does AlpacaEval: An Automatic Evaluator for Instruction-following belong to?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was DreamBooth: Fine Tuning Text-to-Image Diffusion Models released?",
    "output": "The DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper was published in 2022."
  },
  {
    "input": "Summarize the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,724 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is available at https://github.com/anthropics/ConstitutionalHarmlessnessPaper (anthropics/ConstitutionalHarmlessnessPaper)."
  },
  {
    "input": "What does the lucidrains/audiolm-pytorch repository do?",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.05206?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "How many stars does the Deep Graph Library: A Graph-Centric, Highly-Performant Package repository have?",
    "output": "The dmlc/dgl repository has 14,165 stars and 3,056 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Explain the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "Explain the ZhuiyiTechnology/roformer implementation of RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "Rotary Transformer"
  },
  {
    "input": "Where can I find the code for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "What paper has ArXiv ID 2303.08518?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "What language is the Dao-AILab/flash-attention repository written in?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.20201.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10"
  },
  {
    "input": "What is The Flan Collection: Designing Data and Methods for Effective Instruction Tuning about?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "What is T5: Exploring the Limits of Transfer Learning about?",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,458 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2209.00796?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion"
  },
  {
    "input": "What year was Multi-Task Deep Neural Networks for Natural Language Understanding released?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding paper was published in 2021."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.00687?",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package"
  },
  {
    "input": "What's the implementation repository for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "Explain the Semi-Supervised Classification with Graph Convolutional Networks paper.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,339 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2109.04908?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "The code for DeBERTa: Decoding-enhanced BERT with Disentangled Attention is available at https://github.com/microsoft/DeBERTa (microsoft/DeBERTa)."
  },
  {
    "input": "What category is the LLaMA-Adapter: Efficient Fine-tuning of Language Models paper in?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was HuggingGPT: Solving AI Tasks with ChatGPT released?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 implementation?",
    "output": "The uclaml/SPIN repository has 1,226 stars and 104 forks on GitHub."
  },
  {
    "input": "What year was SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities released?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of S4: Efficiently Modeling Long Sequences?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Neural Collaborative Filtering belong to?",
    "output": "Neural Collaborative Filtering is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is available at https://github.com/OptimalScale/LMFlow (OptimalScale/LMFlow)."
  },
  {
    "input": "What are the key topics in the pytorch/pytorch repository?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation?",
    "output": "The openai/frontier-evals repository has 958 stars and 111 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04671.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2210.17323?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of PaLM: Scaling Language Modeling with Pathways?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.05583?",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "What does the rajpurkar/SQuAD-explorer repository do?",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What's the implementation repository for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1609.02907?",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks"
  },
  {
    "input": "What's the implementation repository for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "What is the ArXiv category of SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "Give me an overview of Vicuna: An Open-Source Chatbot.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation?",
    "output": "The huggingface/sentence-transformers repository has 17,978 stars and 2,715 forks on GitHub."
  },
  {
    "input": "Summarize the Neural Message Passing for Quantum Chemistry paper.",
    "output": "Neural Message Passing for Quantum Chemistry is a 2017 paper in Machine Learning. The implementation is available at priba/nmp_qc which our own implementation of neural message passing for computer vision paper It is written in Python. The repository has 344 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the RLHFlow/Directional-Preference-Alignment repository do?",
    "output": "Directional Preference Alignment"
  },
  {
    "input": "What language is the google-deepmind/tracr repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.01141.",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.07842.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "What does the zai-org/ImageReward repository do?",
    "output": "[NeurIPS 2023] ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation?",
    "output": "The ruixin31/Spurious_Rewards repository has 345 stars and 20 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "Give me an overview of PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,255 stars, indicating high community interest."
  },
  {
    "input": "Which field does Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning belong to?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the zetaalphavector/InPars repository written in?",
    "output": "The InPars: Data Augmentation for Information Retrieval implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.04870.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.19328?",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10"
  },
  {
    "input": "Summarize the PyTorch Geometric: Library for Geometric Deep Learning paper.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,255 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/evals The repository has 315 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is available at https://github.com/RLHFlow/Online-RLHF (RLHFlow/Online-RLHF)."
  },
  {
    "input": "What topics does the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding implementation cover?",
    "output": "The nyu-mll/jiant repository covers: nlp, sentence-representation, bert, multitask-learning, transformers, transfer-learning."
  },
  {
    "input": "What does the facebookresearch/fastText repository do?",
    "output": "Library for fast text representation and classification."
  },
  {
    "input": "What does the sentient-agi/OpenDeepSearch repository do?",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "What year was DeBERTa: Decoding-enhanced BERT with Disentangled Attention released?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "Describe the zihangdai/xlnet repository that implements XLNet: Generalized Autoregressive Pretraining.",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is available at https://github.com/OptimalScale/LMFlow (OptimalScale/LMFlow)."
  },
  {
    "input": "What's the implementation repository for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Describe the volcengine/verl repository that implements arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10.",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Summarize the InstructBLIP: Towards General-purpose Vision-Language Models paper.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "What does the deepspeedai/DeepSpeed repository do?",
    "output": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Describe the jax-ml/jax repository that implements JAX: Composable transformations of Python+NumPy programs.",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "What's the implementation repository for InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "The code for InstructBLIP: Towards General-purpose Vision-Language Models is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "When was the WizardCoder: Empowering Code LLMs with Evol-Instruct paper published?",
    "output": "The WizardCoder: Empowering Code LLMs with Evol-Instruct paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.09418.",
    "output": "BERT Rediscovers the Classical NLP Pipeline"
  },
  {
    "input": "What language is the williamleif/GraphSAGE repository written in?",
    "output": "The Inductive Representation Learning on Large Graphs implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at OpenRLHF/OpenRLHF which an easy-to-use, scalable and high-performance rlhf framework based on ray (ppo & grpo & reinforce++ & vllm & ray & dynamic sampling & async agentic rl) It is written in Python. The repository has 8,562 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 implementation?",
    "output": "The lamini-ai/Lamini-Memory-Tuning repository has 275 stars and 12 forks on GitHub."
  },
  {
    "input": "Which field does BLIP-2: Bootstrapping Language-Image Pre-training belong to?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What is the ArXiv category of Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for The Power of Scale for Parameter-Efficient Prompt Tuning?",
    "output": "The code for The Power of Scale for Parameter-Efficient Prompt Tuning is available at https://github.com/google-research/prompt-tuning (google-research/prompt-tuning)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.08691.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "How popular is the RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "When was the Med-PaLM 2: Towards Expert-Level Medical Question Answering paper published?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "What language is the Linear95/APO repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the tgxs002/align_sd repository?",
    "output": "The tgxs002/align_sd repository covers: stable-diffusion."
  },
  {
    "input": "When was the ELECTRA: Pre-training Text Encoders as Discriminators paper published?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of Proximal Policy Optimization Algorithms?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.04129?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the chenfei-wu/TaskMatrix repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2209.07858.",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10"
  },
  {
    "input": "Explain the Dao-AILab/flash-attention implementation of FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is Faster R-CNN: Towards Real-Time Object Detection about?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,278 stars, indicating high community interest."
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,377 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.13007?",
    "output": "Synthetic Data Generation with Large Language Models"
  },
  {
    "input": "Summarize the ViT: An Image is Worth 16x16 Words paper.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "Describe the oughtinc/ice repository that implements arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Where can I find the code for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "What are the key topics in the huawei-noah/Pretrained-Language-Model repository?",
    "output": "The huawei-noah/Pretrained-Language-Model repository covers: knowledge-distillation, model-compression, quantization, pretrained-models, large-scale-distributed."
  },
  {
    "input": "Explain the Grounding DINO: Marrying DINO with Grounded Pre-Training paper.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,401 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at allenai/FineGrainedRLHF It is written in Python. The repository has 281 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at llava-rlhf/LLaVA-RLHF which aligning lmms with factually augmented rlhf It is written in Python. The repository has 387 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the lucidrains/audiolm-pytorch repository?",
    "output": "The lucidrains/audiolm-pytorch repository covers: artificial-intelligence, attention-mechanisms, audio-synthesis, deep-learning, transformers."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.03917.",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 implementation cover?",
    "output": "The lucidrains/muse-maskgit-pytorch repository covers: artificial-intelligence, deep-learning, text-to-image, attention-mechanisms, transformers."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "What language is the ruixin31/Spurious_Rewards repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.11067.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10"
  },
  {
    "input": "What programming language is Efficient Estimation of Word Representations in Vector Space implemented in?",
    "output": "The Efficient Estimation of Word Representations in Vector Space implementation is written in C."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1503.02531?",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does Reformer: The Efficient Transformer belong to?",
    "output": "Reformer: The Efficient Transformer is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning implementation cover?",
    "output": "The rail-berkeley/softlearning repository covers: reinforcement-learning, soft-actor-critic, deep-learning, deep-reinforcement-learning, deep-neural-networks, machine-learning."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "Explain the Faster R-CNN: Towards Real-Time Object Detection paper.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,278 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is available at https://github.com/sylinrl/TruthfulQA (sylinrl/TruthfulQA)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the allenai/FineGrainedRLHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the ELECTRA: Pre-training Text Encoders as Discriminators implementation?",
    "output": "The google-research/electra repository has 2,365 stars and 349 forks on GitHub."
  },
  {
    "input": "In what year did Dense Passage Retrieval for Open-Domain Question Answering come out?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "What is Depthwise Separable Convolutions for Neural Machine Translation about?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2107.03374.",
    "output": "Codex: Evaluating Large Language Models Trained on Code"
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2503.23513.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10"
  },
  {
    "input": "What is ColBERT: Efficient Passage Search via Contextualized Late Interaction about?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,724 stars, indicating high community interest."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the Analyzing the Structure of Attention in a Transformer Language Model paper.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "Which field does The Power of Scale for Parameter-Efficient Prompt Tuning belong to?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at salesforce/LAVIS which lavis - a one-stop library for language-vision intelligence It is written in Jupyter Notebook. The repository has 11,064 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1808.07042?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD"
  },
  {
    "input": "Give me an overview of Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is a 2023 paper in Artificial Intelligence. The implementation is available at noahshinn/reflexion which [neurips 2023] reflexion: language agents with verbal reinforcement learning It is written in Python. The repository has 2,981 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "In what year did SmoothQuant: Accurate and Efficient Post-Training Quantization come out?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Summarize the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,956 stars, indicating high community interest."
  },
  {
    "input": "What is Generating Wikipedia by Summarizing Long Sequences about?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,801 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 repository have?",
    "output": "The google-research/bert repository has 39,724 stars and 9,710 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation cover?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What year was InstructBLIP: Towards General-purpose Vision-Language Models released?",
    "output": "The InstructBLIP: Towards General-purpose Vision-Language Models paper was published in 2023."
  },
  {
    "input": "What are the key topics in the jax-ml/jax repository?",
    "output": "The jax-ml/jax repository covers: jax."
  },
  {
    "input": "What does the tatp22/linformer-pytorch repository do?",
    "output": "My take on a practical implementation of Linformer for Pytorch."
  },
  {
    "input": "What is Feature Pyramid Networks for Object Detection about?",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "What programming language is ControlNet: Adding Conditional Control to Text-to-Image Diffusion implemented in?",
    "output": "The ControlNet: Adding Conditional Control to Text-to-Image Diffusion implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the OptimalScale/LMFlow repository do?",
    "output": "An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All."
  },
  {
    "input": "How popular is the RAG: Retrieval-Augmented Generation implementation?",
    "output": "The facebookresearch/DPR repository has 1,851 stars and 316 forks on GitHub."
  },
  {
    "input": "What language is the facebookresearch/fastText repository written in?",
    "output": "The Enriching Word Vectors with Subword Information implementation is written in HTML."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "The code for RoFormer: Enhanced Transformer with Rotary Position Embedding is available at https://github.com/ZhuiyiTechnology/roformer (ZhuiyiTechnology/roformer)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "Explain the state-spaces/mamba implementation of Mamba: Linear-Time Sequence Modeling with Selective State Spaces.",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is available at https://github.com/ali-vilab/composer (ali-vilab/composer)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of WizardCoder: Empowering Code LLMs with Evol-Instruct.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,471 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "Give me an overview of FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is a 2022 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.01068?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2001.04451?",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "Tell me the title of ArXiv paper 1707.06347.",
    "output": "Proximal Policy Optimization Algorithms"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "Describe the HumanAIGC/AnimateAnyone repository that implements Animate Anyone: Consistent and Controllable Image-to-Video Synthesis.",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "In what year did FlashAttention: Fast and Memory-Efficient Exact Attention come out?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What is YOLOX: Exceeding YOLO Series in 2021 about?",
    "output": "YOLOX: Exceeding YOLO Series in 2021 is a 2018 paper in Computer Vision and Pattern Recognition. The implementation is available at Megvii-BaseDetection/YOLOX which yolox is a high-performance anchor-free yolo, exceeding yolov3~v5 with megengine, onnx, tensorrt, ncnn, and openvino supported. documentation: https://yolox.readthedocs.io/ It is written in Python. The repository has 10,223 stars, indicating high community interest."
  },
  {
    "input": "What language is the pyg-team/pytorch_geometric repository written in?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning implementation is written in Python."
  },
  {
    "input": "Explain the openai/frontier-evals implementation of arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10.",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Give me an overview of CLIP: Learning Transferable Visual Models From Natural Language.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,905 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Densely Connected Convolutional Networks repository have?",
    "output": "The liuzhuang13/DenseNet repository has 4,848 stars and 1,071 forks on GitHub."
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is HuggingGPT: Solving AI Tasks with ChatGPT about?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,488 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Depthwise Separable Convolutions for Neural Machine Translation come out?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "In what year did HuggingGPT: Solving AI Tasks with ChatGPT come out?",
    "output": "The HuggingGPT: Solving AI Tasks with ChatGPT paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "Explain the Proximal Policy Optimization Algorithms paper.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,588 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Neural Collaborative Filtering?",
    "output": "The code for Neural Collaborative Filtering is available at https://github.com/hexiangnan/neural_collaborative_filtering (hexiangnan/neural_collaborative_filtering)."
  },
  {
    "input": "What is the ArXiv category of FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2105.05233?",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "Explain the google-research/google-research implementation of Med-PaLM 2: Towards Expert-Level Medical Question Answering.",
    "output": "Google Research"
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.08691.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism"
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "What language is the huggingface/transformers repository written in?",
    "output": "The DistilBERT: a distilled version of BERT implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Which field does TensorFlow: A System for Large-Scale Machine Learning belong to?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the rll-research/BPref repository that implements arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10.",
    "output": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments."
  },
  {
    "input": "What paper has ArXiv ID 1608.06993?",
    "output": "Densely Connected Convolutional Networks"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the ContextualAI/HALOs repository?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "What paper has ArXiv ID 2101.00190?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "What does the pytorch/pytorch repository do?",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.00109?",
    "output": "Med-PaLM 2: Towards Expert-Level Medical Question Answering"
  },
  {
    "input": "What's the implementation repository for Adam: A Method for Stochastic Optimization?",
    "output": "The code for Adam: A Method for Stochastic Optimization is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "What's the implementation repository for Depthwise Separable Convolutions for Neural Machine Translation?",
    "output": "The code for Depthwise Separable Convolutions for Neural Machine Translation is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is available at https://github.com/tgxs002/align_sd (tgxs002/align_sd)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "What category is the Mamba: Linear-Time Sequence Modeling with Selective State Spaces paper in?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Horovod: fast and easy distributed deep learning in TensorFlow paper in?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "Which field does Wide & Deep Learning for Recommender Systems belong to?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "When was the Self-RAG: Learning to Retrieve, Generate, and Critique paper published?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique paper was published in 2022."
  },
  {
    "input": "What programming language is Graph Attention Networks implemented in?",
    "output": "The Graph Attention Networks implementation is written in Python."
  },
  {
    "input": "Explain the OpenGVLab/LLaMA-Adapter implementation of LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "What is MMLU: Measuring Massive Multitask Language Understanding about?",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,528 stars, indicating high community interest."
  },
  {
    "input": "In what year did AlpacaEval: An Automatic Evaluator for Instruction-following come out?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "How many stars does the QLoRA: Efficient Finetuning of Quantized LLMs repository have?",
    "output": "The artidoro/qlora repository has 10,782 stars and 866 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of Beir: A Heterogeneous Benchmark for IR?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation cover?",
    "output": "The Shark-NLP/OpenICL repository covers: in-context-learning, nlp, language-model."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the FlashAttention: Fast and Memory-Efficient Exact Attention paper in?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for Faster R-CNN: Towards Real-Time Object Detection?",
    "output": "The code for Faster R-CNN: Towards Real-Time Object Detection is available at https://github.com/rbgirshick/py-faster-rcnn (rbgirshick/py-faster-rcnn)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.01947?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Summarize the Distilling the Knowledge in a Neural Network paper.",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,974 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Linear95/APO which code for acl2024 paper - adversarial preference optimization (apo). It is written in Python. The repository has 56 stars, indicating high community interest."
  },
  {
    "input": "Explain the MRKL Systems: A modular approach to AGI paper.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 121,514 stars, indicating high community interest."
  },
  {
    "input": "What does the ysymyth/ReAct repository do?",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.09332.",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10"
  },
  {
    "input": "What topics does the Flamingo: A Visual Language Model for Few-Shot Learning implementation cover?",
    "output": "The lucidrains/flamingo-pytorch repository covers: artificial-intelligence, attention-mechanism, deep-learning, transformers, visual-question-answering."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Explain the meta-llama/llama implementation of LLaMA: Open and Efficient Foundation Language Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "When was the AlpacaEval: An Automatic Evaluator for Instruction-following paper published?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following paper was published in 2023."
  },
  {
    "input": "Explain the TsinghuaAI/CPM-1-Generate implementation of arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10.",
    "output": "Chinese Pre-Trained Language Models (CPM-LM) Version-I"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the salesforce/LAVIS repository?",
    "output": "The salesforce/LAVIS repository covers: deep-learning, deep-learning-library, image-captioning, salesforce, vision-and-language, vision-framework, vision-language-pretraining, vision-language-transformer, visual-question-anwsering, multimodal-datasets, multimodal-deep-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1409.1556?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
  },
  {
    "input": "Describe the pyg-team/pytorch_geometric repository that implements PyTorch Geometric: Library for Geometric Deep Learning.",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What are the key topics in the zihangdai/xlnet repository?",
    "output": "The zihangdai/xlnet repository covers: tensorflow, nlp, deep-learning."
  },
  {
    "input": "Where can I find the code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "In what year did Grounding DINO: Marrying DINO with Grounded Pre-Training come out?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Synthetic Data Generation with Large Language Models?",
    "output": "The code for Synthetic Data Generation with Large Language Models is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "When was the Proximal Policy Optimization Algorithms paper published?",
    "output": "The Proximal Policy Optimization Algorithms paper was published in 2017."
  },
  {
    "input": "Tell me the title of ArXiv paper 2010.02502.",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What does the tensorflow/tensor2tensor repository do?",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,735 stars, indicating high community interest."
  },
  {
    "input": "Describe the priba/nmp_qc repository that implements Neural Message Passing for Quantum Chemistry.",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "Summarize the Faster R-CNN: Towards Real-Time Object Detection paper.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,278 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.03520.",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10"
  },
  {
    "input": "How many stars does the TinyBERT: Distilling BERT for Natural Language Understanding repository have?",
    "output": "The huawei-noah/Pretrained-Language-Model repository has 3,150 stars and 642 forks on GitHub."
  },
  {
    "input": "What is Longformer: The Long-Document Transformer about?",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,176 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation?",
    "output": "The KimMeen/Time-LLM repository has 2,410 stars and 424 forks on GitHub."
  },
  {
    "input": "Explain the Generating Wikipedia by Summarizing Long Sequences paper.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,801 stars, indicating high community interest."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding repository have?",
    "output": "The nyu-mll/jiant repository has 1,674 stars and 297 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What year was Contriever: Unsupervised Dense Information Retrieval released?",
    "output": "The Contriever: Unsupervised Dense Information Retrieval paper was published in 2022."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 repository have?",
    "output": "The autonomousvision/stylegan-t repository has 1,198 stars and 61 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Deep Residual Learning for Image Recognition paper in?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How many stars does the Prefix-Tuning: Optimizing Continuous Prompts for Generation repository have?",
    "output": "The XiangLi1999/PrefixTuning repository has 956 stars and 164 forks on GitHub."
  },
  {
    "input": "What year was Swin Transformer: Hierarchical Vision Transformer using Shifted Windows released?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "What topics does the RoBERTa: A Robustly Optimized BERT Pretraining Approach implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2501.13824?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tomekkorbak/pretraining-with-human-feedback which code accompanying the paper pretraining language models with human preferences It is written in Python. The repository has 180 stars, indicating high community interest."
  },
  {
    "input": "In what year did Masked Autoencoders Are Scalable Vision Learners come out?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners paper was published in 2022."
  },
  {
    "input": "What programming language is Codex: Evaluating Large Language Models Trained on Code implemented in?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "What is DeBERTa: Decoding-enhanced BERT with Disentangled Attention about?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Longformer: The Long-Document Transformer?",
    "output": "Longformer: The Long-Document Transformer is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How popular is the Universal Language Model Fine-tuning for Text Classification implementation?",
    "output": "The fastai/fastai repository has 27,653 stars and 7,666 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Describe the tgxs002/align_sd repository that implements arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10.",
    "output": "Better Aligning Text-to-Image Models with Human Preference. ICCV 2023"
  },
  {
    "input": "What does the state-spaces/mamba repository do?",
    "output": "Mamba SSM architecture"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation?",
    "output": "The salesforce/GeDi repository has 210 stars and 45 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at openai/summarize-from-feedback which code for \"learning to summarize from human feedback\" It is written in Python. The repository has 1,056 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.08100?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Linear95/APO which code for acl2024 paper - adversarial preference optimization (apo). It is written in Python. The repository has 56 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Directional-Preference-Alignment which directional preference alignment The repository has 58 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Contriever: Unsupervised Dense Information Retrieval repository have?",
    "output": "The facebookresearch/contriever repository has 766 stars and 69 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.00698?",
    "output": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10"
  },
  {
    "input": "What category is the S4: Efficiently Modeling Long Sequences paper in?",
    "output": "S4: Efficiently Modeling Long Sequences is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Med-PaLM 2: Towards Expert-Level Medical Question Answering come out?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "What does the AlibabaResearch/DAMO-ConvAI repository do?",
    "output": "DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI."
  },
  {
    "input": "What paper has ArXiv ID 2303.08774?",
    "output": "GPT-4 Technical Report"
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Summarize the Prefix-Tuning: Optimizing Continuous Prompts for Generation paper.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at XiangLi1999/PrefixTuning which prefix-tuning: optimizing continuous prompts for generation It is written in Python. The repository has 956 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2305.07185?",
    "output": "LIMA: Less Is More for Alignment"
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.03270.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "Give me an overview of AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,610 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Graph Attention Networks.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,469 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Playing Atari with Deep Reinforcement Learning.",
    "output": "Playing Atari with Deep Reinforcement Learning is a 2013 paper in Machine Learning. The implementation is available at google-deepmind/dqn which lua/torch implementation of dqn (nature, 2015) It is written in Lua. The repository has 618 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 582 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.11316.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the SmoothQuant: Accurate and Efficient Post-Training Quantization paper published?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization paper was published in 2022."
  },
  {
    "input": "What category is the BLIP-2: Bootstrapping Language-Image Pre-training paper in?",
    "output": "BLIP-2: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What language is the plurai-ai/intellagent repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the langchain-ai/langchain repository do?",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "What does the meta-llama/codellama repository do?",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "Explain the facebookresearch/mae implementation of Masked Autoencoders Are Scalable Vision Learners.",
    "output": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377"
  },
  {
    "input": "Summarize the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper.",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,793 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.10683?",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is available at https://github.com/Linear95/APO (Linear95/APO)."
  },
  {
    "input": "What paper has ArXiv ID 2307.01952?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is a 2024 paper in Computation and Language (Natural Language Processing). The implementation is available at deepseek-ai/DeepSeek-Coder-V2 which deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence The repository has 6,281 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 implementation?",
    "output": "The turboderp-org/exllamav2 repository has 4,380 stars and 325 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What is Deep Residual Learning for Image Recognition about?",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,671 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of EfficientDet: Scalable and Efficient Object Detection.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,430 stars, indicating high community interest."
  },
  {
    "input": "What category is the The Power of Scale for Parameter-Efficient Prompt Tuning paper in?",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "What paper has ArXiv ID 2301.00808?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2311.03099.",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "Summarize the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00808.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of Mistral 7B.",
    "output": "Mistral 7B is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at mistralai/mistral-inference which official inference library for mistral models It is written in Jupyter Notebook. The repository has 10,577 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the oughtinc/ice repository that implements arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Tell me the title of ArXiv paper 2312.00752.",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "What year was Vicuna: An Open-Source Chatbot released?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 repository have?",
    "output": "The MoonshotAI/Kimi-Audio repository has 4,372 stars and 319 forks on GitHub."
  },
  {
    "input": "In what year did Neural Message Passing for Quantum Chemistry come out?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.02732?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "Explain the mit-han-lab/offsite-tuning implementation of arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.01469.",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 repository have?",
    "output": "The GanjinZero/RRHF repository has 811 stars and 45 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2110.07602?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning"
  },
  {
    "input": "What is the ArXiv category of InstructBLIP: Towards General-purpose Vision-Language Models?",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper in?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the THUDM/P-tuning-v2 repository written in?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.11366?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What does the openai/frontier-evals repository do?",
    "output": "OpenAI Frontier Evals"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.02828?",
    "output": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Segment Anything in Medical Images paper.",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,974 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for EfficientDet: Scalable and Efficient Object Detection?",
    "output": "The code for EfficientDet: Scalable and Efficient Object Detection is available at https://github.com/google/automl (google/automl)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tgxs002/align_sd which better aligning text-to-image models with human preference. iccv 2023 It is written in Python. The repository has 293 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Generating Wikipedia by Summarizing Long Sequences belong to?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is available at https://github.com/openai/following-instructions-human-feedback (openai/following-instructions-human-feedback)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/ConstitutionalHarmlessnessPaper The repository has 249 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Faster R-CNN: Towards Real-Time Object Detection implemented in?",
    "output": "The Faster R-CNN: Towards Real-Time Object Detection implementation is written in Python."
  },
  {
    "input": "What does the rajpurkar/SQuAD-explorer repository do?",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2002.05202?",
    "output": "Reformer: The Efficient Transformer"
  },
  {
    "input": "What paper has ArXiv ID 2303.04671?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 implementation?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "What are the key topics in the google-research/electra repository?",
    "output": "The google-research/electra repository covers: nlp, deep-learning, tensorflow."
  },
  {
    "input": "How many stars does the StarCoder: May the source be with you! repository have?",
    "output": "The bigcode-project/starcoder repository has 7,478 stars and 531 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "What paper has ArXiv ID 2004.04906?",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 repository have?",
    "output": "The GAIR-NLP/LIMO repository has 1,054 stars and 52 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2205.01917?",
    "output": "Masked Autoencoders Are Scalable Vision Learners"
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.01061.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10"
  },
  {
    "input": "Which field does Linformer: Self-Attention with Linear Complexity belong to?",
    "output": "Linformer: Self-Attention with Linear Complexity is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.04768.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "What is the ArXiv category of ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the Efficient Estimation of Word Representations in Vector Space paper.",
    "output": "Efficient Estimation of Word Representations in Vector Space is a 2013 paper in Computation and Language (Natural Language Processing). The implementation is available at tmikolov/word2vec which automatically exported from code.google.com/p/word2vec It is written in C. The repository has 1,572 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.03511?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Attention Is All You Need?",
    "output": "The code for Attention Is All You Need is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 958 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning?",
    "output": "The code for P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is available at https://github.com/THUDM/P-tuning-v2 (THUDM/P-tuning-v2)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 implementation?",
    "output": "The Open-Reasoner-Zero/Open-Reasoner-Zero repository has 2,074 stars and 118 forks on GitHub."
  },
  {
    "input": "Give me an overview of The Flan Collection: Designing Data and Methods for Effective Instruction Tuning.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Finetuned Language Models Are Zero-Shot Learners.",
    "output": "Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the NVlabs/prismer repository?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What topics does the PaLM: Scaling Language Modeling with Pathways implementation cover?",
    "output": "The lucidrains/PaLM-pytorch repository covers: deep-learning, transformers, attention-mechanism, artificial-general-intelligence."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.12872?",
    "output": "EfficientDet: Scalable and Efficient Object Detection"
  },
  {
    "input": "How popular is the InPars: Data Augmentation for Information Retrieval implementation?",
    "output": "The zetaalphavector/InPars repository has 198 stars and 24 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "How popular is the GPT-4 Technical Report implementation?",
    "output": "The openai/evals repository has 17,406 stars and 2,848 forks on GitHub."
  },
  {
    "input": "Explain the Decoupled Weight Decay Regularization paper.",
    "output": "Decoupled Weight Decay Regularization is a 2017 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What does the jax-ml/jax repository do?",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "How many stars does the Deep Residual Learning for Image Recognition repository have?",
    "output": "The KaimingHe/deep-residual-networks repository has 6,671 stars and 2,232 forks on GitHub."
  },
  {
    "input": "Which field does Very Deep Convolutional Networks for Large-Scale Image Recognition belong to?",
    "output": "Very Deep Convolutional Networks for Large-Scale Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the Enriching Word Vectors with Subword Information paper.",
    "output": "Enriching Word Vectors with Subword Information is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fastText which library for fast text representation and classification. It is written in HTML. The repository has 26,445 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is available at https://github.com/uber/orbit (uber/orbit)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for RAG: Retrieval-Augmented Generation?",
    "output": "The code for RAG: Retrieval-Augmented Generation is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "How popular is the Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation?",
    "output": "The microsoft/unilm repository has 21,873 stars and 2,681 forks on GitHub."
  },
  {
    "input": "What language is the nyu-mll/quality repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 1606.07792.",
    "output": "Wide & Deep Learning for Recommender Systems"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "In what year did Beir: A Heterogeneous Benchmark for IR come out?",
    "output": "The Beir: A Heterogeneous Benchmark for IR paper was published in 2022."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at frankxu2004/knnlm-why which repo for icml23 \"why do nearest neighbor language models work?\" It is written in Python. The repository has 59 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is BEiT: BERT Pre-Training of Image Transformers about?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/unilm which large-scale self-supervised pre-training across tasks, languages, and modalities It is written in Python. The repository has 21,873 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is MiniGPT-4: Enhancing Vision-Language Understanding about?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,762 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of BEiT: BERT Pre-Training of Image Transformers?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation?",
    "output": "The state-spaces/mamba repository has 16,662 stars and 1,529 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,611 stars, indicating high community interest."
  },
  {
    "input": "Explain the WizardCoder: Empowering Code LLMs with Evol-Instruct paper.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,471 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 is available at https://github.com/salesforce/GeDi (salesforce/GeDi)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is available at https://github.com/TsinghuaAI/CPM-1-Generate (TsinghuaAI/CPM-1-Generate)."
  },
  {
    "input": "What topics does the Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation cover?",
    "output": "The dmlc/dgl repository covers: deep-learning, graph-neural-networks."
  },
  {
    "input": "What is the GitHub repository for Deep Residual Learning for Image Recognition?",
    "output": "The code for Deep Residual Learning for Image Recognition is available at https://github.com/KaimingHe/deep-residual-networks (KaimingHe/deep-residual-networks)."
  },
  {
    "input": "What category is the Vicuna: An Open-Source Chatbot paper in?",
    "output": "Vicuna: An Open-Source Chatbot is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at facebookresearch/ConvNeXt-V2 which code release for convnext v2 model It is written in Python. The repository has 1,917 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2005.08100?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets"
  },
  {
    "input": "How many stars does the Dream to Control: Learning Behaviors by Latent Imagination repository have?",
    "output": "The danijar/dreamer repository has 570 stars and 113 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is available at https://github.com/ruixin31/Spurious_Rewards (ruixin31/Spurious_Rewards)."
  },
  {
    "input": "What topics does the DistilBERT: a distilled version of BERT implementation cover?",
    "output": "The huggingface/transformers repository covers: nlp, natural-language-processing, pytorch, pytorch-transformers, transformer, model-hub, pretrained-models, speech-recognition, hacktoberfest, python, machine-learning, deep-learning, audio, deepseek, gemma, glm, llm, qwen, vlm."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.10752?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.03846?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is available at https://github.com/lamini-ai/Lamini-Memory-Tuning (lamini-ai/Lamini-Memory-Tuning)."
  },
  {
    "input": "What year was SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding released?",
    "output": "The SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding paper was published in 2019."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "What are the key topics in the liuzhuang13/DenseNet repository?",
    "output": "The liuzhuang13/DenseNet repository covers: deep-learning."
  },
  {
    "input": "What is the GitHub repository for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "What does the google-research/albert repository do?",
    "output": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What year was DistilBERT: a distilled version of BERT released?",
    "output": "The DistilBERT: a distilled version of BERT paper was published in 2019."
  },
  {
    "input": "What paper has ArXiv ID 2205.05131?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 implementation?",
    "output": "The Linear95/APO repository has 56 stars and 3 forks on GitHub."
  },
  {
    "input": "What is Segment Anything in Medical Images about?",
    "output": "Segment Anything in Medical Images is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at bowang-lab/MedSAM which segment anything in medical images It is written in Jupyter Notebook. The repository has 3,974 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 repository have?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.16779?",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/prompt-tuning which original implementation of prompt tuning from lester, et al, 2021 It is written in Python. The repository has 699 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "The code for DreamBooth: Fine Tuning Text-to-Image Diffusion Models is available at https://github.com/google/dreambooth (google/dreambooth)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ZHZisZZ/modpo which [acl'24] beyond one-preference-fits-all alignment: multi-objective direct preference optimization It is written in Python. The repository has 93 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at OpenBMB/Eurus It is written in Python. The repository has 320 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 1612.03144.",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "Which field does Feature Pyramid Networks for Object Detection belong to?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.01741.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "Describe the NVlabs/prismer repository that implements arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "What's the implementation repository for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "How popular is the QLoRA: Efficient Finetuning of Quantized LLMs implementation?",
    "output": "The artidoro/qlora repository has 10,782 stars and 866 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 1506.02640?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "Explain the MAXNORM8650/MedAgentSim implementation of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "What language is the hendrycks/test repository written in?",
    "output": "The MMLU: Measuring Massive Multitask Language Understanding implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.03917?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10"
  },
  {
    "input": "How popular is the Very Deep Convolutional Networks for Large-Scale Image Recognition implementation?",
    "output": "The machrisaa/tensorflow-vgg repository has 2,240 stars and 1,072 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of GPT-4 Technical Report?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "What does the jasonwei20/eda_nlp repository do?",
    "output": "Data augmentation for NLP, presented at EMNLP 2019"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1901.02860?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "What is the GitHub repository for Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "The code for Gorilla: Large Language Model Connected with Massive APIs is available at https://github.com/ShishirPatil/gorilla (ShishirPatil/gorilla)."
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Explain the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper.",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text is a 2016 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What does the turboderp-org/exllamav2 repository do?",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "Describe the ysymyth/ReAct repository that implements ReAct: Synergizing Reasoning and Acting in Language Models.",
    "output": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Efficient Estimation of Word Representations in Vector Space implementation?",
    "output": "The tmikolov/word2vec repository has 1,572 stars and 544 forks on GitHub."
  },
  {
    "input": "How many stars does the ColBERT: Efficient Passage Search via Contextualized Late Interaction repository have?",
    "output": "The stanford-futuredata/ColBERT repository has 3,724 stars and 462 forks on GitHub."
  },
  {
    "input": "Which field does StarCoder: May the source be with you! belong to?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the ELECTRA: Pre-training Text Encoders as Discriminators paper in?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.14233.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2505.23006.",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for AudioPaLM: A Large Language Model for Speech Understanding?",
    "output": "The code for AudioPaLM: A Large Language Model for Speech Understanding is available at https://github.com/lucidrains/audiolm-pytorch (lucidrains/audiolm-pytorch)."
  },
  {
    "input": "Where can I find the code for Know What You Don't Know: Unanswerable Questions for SQuAD?",
    "output": "The code for Know What You Don't Know: Unanswerable Questions for SQuAD is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "When was the Dense Passage Retrieval for Open-Domain Question Answering paper published?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the YOLOX: Exceeding YOLO Series in 2021 paper published?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 paper was published in 2018."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper.",
    "output": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "What language is the facebookresearch/fairseq repository written in?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction implementation is written in Python."
  },
  {
    "input": "What programming language is Multi-Task Deep Neural Networks for Natural Language Understanding implemented in?",
    "output": "The Multi-Task Deep Neural Networks for Natural Language Understanding implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "What paper has ArXiv ID 2403.17887?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Lookahead Optimizer: k steps forward, 1 step back?",
    "output": "The code for Lookahead Optimizer: k steps forward, 1 step back is available at https://github.com/alphadl/lookahead.pytorch (alphadl/lookahead.pytorch)."
  },
  {
    "input": "What language is the clarkkev/attention-analysis repository written in?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline implementation is written in Jupyter Notebook."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "In what year did BERT Rediscovers the Classical NLP Pipeline come out?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "Give me an overview of Beir: A Heterogeneous Benchmark for IR.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,014 stars, indicating high community interest."
  },
  {
    "input": "Describe the facebookresearch/TransCoder repository that implements arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What's the implementation repository for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What paper has ArXiv ID 2303.14420?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the Zephyr: Direct Distillation of LM Alignment paper.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,444 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at arcee-ai/mergekit which tools for merging pretrained large language models. It is written in Python. The repository has 6,564 stars, indicating high community interest."
  },
  {
    "input": "Summarize the WizardCoder: Empowering Code LLMs with Evol-Instruct paper.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at nlpxucan/WizardLM which llms build upon evol insturct: wizardlm, wizardcoder, wizardmath It is written in Python. The repository has 9,471 stars, indicating high community interest."
  },
  {
    "input": "Explain the tatsu-lab/alpaca_eval implementation of AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "Where can I find the code for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/evals The repository has 315 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for HuggingGPT: Solving AI Tasks with ChatGPT?",
    "output": "The code for HuggingGPT: Solving AI Tasks with ChatGPT is available at https://github.com/microsoft/JARVIS (microsoft/JARVIS)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 implementation cover?",
    "output": "The tgxs002/align_sd repository covers: stable-diffusion."
  },
  {
    "input": "Explain the beir-cellar/beir implementation of Beir: A Heterogeneous Benchmark for IR.",
    "output": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets."
  },
  {
    "input": "How popular is the Proximal Policy Optimization Algorithms implementation?",
    "output": "The openai/baselines repository has 16,588 stars and 4,951 forks on GitHub."
  },
  {
    "input": "How many stars does the InPars: Data Augmentation for Information Retrieval repository have?",
    "output": "The zetaalphavector/InPars repository has 198 stars and 24 forks on GitHub."
  },
  {
    "input": "What is Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning about?",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is a 2018 paper in Machine Learning. The implementation is available at rail-berkeley/softlearning which softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. includes the official implementation of the soft actor-critic algorithm. It is written in Python. The repository has 1,377 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2311.03099?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.04434?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Billion-scale similarity search with GPUs paper.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,366 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale released?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What category is the Text Embeddings by Weakly-Supervised Contrastive Pre-training paper in?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "When was the TensorFlow: A System for Large-Scale Machine Learning paper published?",
    "output": "The TensorFlow: A System for Large-Scale Machine Learning paper was published in 2016."
  },
  {
    "input": "Describe the OptimalScale/LMFlow repository that implements arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10.",
    "output": "An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All."
  },
  {
    "input": "Explain the google-research/prompt-tuning implementation of The Power of Scale for Parameter-Efficient Prompt Tuning.",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "What does the haoliuhl/language-quantized-autoencoders repository do?",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What does the PKU-Alignment/align-anything repository do?",
    "output": "Align Anything: Training All-modality Model with Feedback"
  },
  {
    "input": "What year was AudioPaLM: A Large Language Model for Speech Understanding released?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "Summarize the Diffusion Models Beat GANs on Image Synthesis paper.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,183 stars, indicating high community interest."
  },
  {
    "input": "What category is the Beir: A Heterogeneous Benchmark for IR paper in?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Summarize the Lookahead Optimizer: k steps forward, 1 step back paper.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 338 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.11239?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "What language is the facebookresearch/DPR repository written in?",
    "output": "The RAG: Retrieval-Augmented Generation implementation is written in Python."
  },
  {
    "input": "What language is the hexiangnan/neural_collaborative_filtering repository written in?",
    "output": "The Neural Collaborative Filtering implementation is written in Python."
  },
  {
    "input": "Summarize the T5: Exploring the Limits of Transfer Learning paper.",
    "output": "T5: Exploring the Limits of Transfer Learning is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/text-to-text-transfer-transformer which code for the paper \"exploring the limits of transfer learning with a unified text-to-text transformer\" It is written in Python. The repository has 6,458 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.10343.",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of XLNet: Generalized Autoregressive Pretraining?",
    "output": "XLNet: Generalized Autoregressive Pretraining is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the Shark-NLP/OpenICL repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Distilling the Knowledge in a Neural Network released?",
    "output": "The Distilling the Knowledge in a Neural Network paper was published in 2015."
  },
  {
    "input": "What year was FLAN: Finetuned Language Models Are Zero-Shot Learners released?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "How many stars does the HuggingGPT: Solving AI Tasks with ChatGPT repository have?",
    "output": "The microsoft/JARVIS repository has 24,488 stars and 2,059 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What does the pyg-team/pytorch_geometric repository do?",
    "output": "Graph Neural Network Library for PyTorch"
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 implementation cover?",
    "output": "The oughtinc/ice repository covers: debugging, gpt-3, python, language-model."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "Where can I find the code for GPT-4 Technical Report?",
    "output": "The code for GPT-4 Technical Report is available at https://github.com/openai/evals (openai/evals)."
  },
  {
    "input": "How many stars does the High-Resolution Image Synthesis with Latent Diffusion Models repository have?",
    "output": "The CompVis/latent-diffusion repository has 13,646 stars and 1,702 forks on GitHub."
  },
  {
    "input": "Explain the shizhediao/active-prompt implementation of arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10.",
    "output": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\""
  },
  {
    "input": "How popular is the Beir: A Heterogeneous Benchmark for IR implementation?",
    "output": "The beir-cellar/beir repository has 2,014 stars and 225 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Segment Anything?",
    "output": "The code for Segment Anything is available at https://github.com/facebookresearch/segment-anything (facebookresearch/segment-anything)."
  },
  {
    "input": "Which field does DistilBERT: a distilled version of BERT belong to?",
    "output": "DistilBERT: a distilled version of BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the danijar/dreamer implementation of Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "How popular is the Dense Passage Retrieval for Open-Domain Question Answering implementation?",
    "output": "The facebookresearch/DPR repository has 1,851 stars and 316 forks on GitHub."
  },
  {
    "input": "How many stars does the Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is available at https://github.com/yafuly/TPO (yafuly/TPO)."
  },
  {
    "input": "Describe the Dao-AILab/flash-attention repository that implements FlashAttention: Fast and Memory-Efficient Exact Attention.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Describe the ZHZisZZ/modpo repository that implements arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10.",
    "output": "[ACL'24] Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is available at https://github.com/GXimingLu/Quark (GXimingLu/Quark)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2112.04426?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction"
  },
  {
    "input": "What does the deepseek-ai/DeepSeek-Coder-V2 repository do?",
    "output": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence"
  },
  {
    "input": "What category is the CLIP: Learning Transferable Visual Models From Natural Language paper in?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What are the key topics in the ashawkey/stable-dreamfusion repository?",
    "output": "The ashawkey/stable-dreamfusion repository covers: text-to-3d, gui, nerf, stable-diffusion, dreamfusion, image-to-3d."
  },
  {
    "input": "What topics does the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation cover?",
    "output": "The haotian-liu/LLaVA repository covers: gpt-4, chatbot, chatgpt, llama, multimodal, llava, foundation-models, instruction-tuning, multi-modality, visual-language-learning, llama-2, llama2, vision-language-model."
  },
  {
    "input": "What paper has ArXiv ID 2310.03744?",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "Give me an overview of Lookahead Optimizer: k steps forward, 1 step back.",
    "output": "Lookahead Optimizer: k steps forward, 1 step back is a 2020 paper in Machine Learning. The implementation is available at alphadl/lookahead.pytorch which lookahead optimizer (lookahead optimizer: k steps forward, 1 step back) for pytorch  It is written in Python. The repository has 338 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is available at https://github.com/deepseek-ai/DeepSeek-Coder-V2 (deepseek-ai/DeepSeek-Coder-V2)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "What category is the TinyBERT: Distilling BERT for Natural Language Understanding paper in?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 1703.04691.",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "Explain the rbgirshick/py-faster-rcnn implementation of Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "How many stars does the ViT: An Image is Worth 16x16 Words repository have?",
    "output": "The google-research/vision_transformer repository has 12,102 stars and 1,428 forks on GitHub."
  },
  {
    "input": "Which field does An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale belong to?",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What topics does the Generating Wikipedia by Summarizing Long Sequences implementation cover?",
    "output": "The tensorflow/tensor2tensor repository covers: machine-learning, machine-translation, deep-learning, reinforcement-learning, tpu."
  },
  {
    "input": "In what year did Denoising Diffusion Probabilistic Models come out?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "Where can I find the code for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "How many stars does the WizardCoder: Empowering Code LLMs with Evol-Instruct repository have?",
    "output": "The nlpxucan/WizardLM repository has 9,471 stars and 751 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is available at https://github.com/chenfei-wu/TaskMatrix (chenfei-wu/TaskMatrix)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the Depthwise Separable Convolutions for Neural Machine Translation paper.",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation is a 2017 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Densely Connected Convolutional Networks implementation cover?",
    "output": "The liuzhuang13/DenseNet repository covers: deep-learning."
  },
  {
    "input": "What's the implementation repository for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "Summarize the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Describe the haotian-liu/LLaVA repository that implements LLaVA-1.5: Improved Baselines with Visual Instruction Tuning.",
    "output": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the google-research/bert implementation of arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10.",
    "output": "TensorFlow code and pre-trained models for BERT"
  },
  {
    "input": "When was the Horovod: fast and easy distributed deep learning in TensorFlow paper published?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow paper was published in 2017."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "What language is the openai/summarize-from-feedback repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is available at https://github.com/ContextualAI/HALOs (ContextualAI/HALOs)."
  },
  {
    "input": "What is the GitHub repository for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "What is the GitHub repository for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "What paper has ArXiv ID 2304.10592?",
    "output": "LLaVA: Large Language and Vision Assistant"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "When was the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper published?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "How many stars does the YOLOX: Exceeding YOLO Series in 2021 repository have?",
    "output": "The Megvii-BaseDetection/YOLOX repository has 10,223 stars and 2,419 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "Explain the amazon-science/mm-cot implementation of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1907.11692.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Finetuned Language Models Are Zero-Shot Learners?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the RoBERTa: A Robustly Optimized BERT Pretraining Approach repository have?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "Describe the namisan/mt-dnn repository that implements Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "Give me an overview of Multi-Task Deep Neural Networks for Natural Language Understanding.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "Where can I find the code for GPT-3: Language Models are Few-Shot Learners?",
    "output": "The code for GPT-3: Language Models are Few-Shot Learners is available at https://github.com/openai/gpt-3 (openai/gpt-3)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Zephyr: Direct Distillation of LM Alignment implementation cover?",
    "output": "The huggingface/alignment-handbook repository covers: llm, rlhf, transformers."
  },
  {
    "input": "What's the implementation repository for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "How many stars does the BERT Rediscovers the Classical NLP Pipeline repository have?",
    "output": "The clarkkev/attention-analysis repository has 469 stars and 82 forks on GitHub."
  },
  {
    "input": "Where can I find the code for DistilBERT: a distilled version of BERT?",
    "output": "The code for DistilBERT: a distilled version of BERT is available at https://github.com/huggingface/transformers (huggingface/transformers)."
  },
  {
    "input": "How popular is the BLIP-2: Bootstrapping Language-Image Pre-training implementation?",
    "output": "The salesforce/LAVIS repository has 11,064 stars and 1,087 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Universal Language Model Fine-tuning for Text Classification repository have?",
    "output": "The fastai/fastai repository has 27,653 stars and 7,666 forks on GitHub."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What topics does the MetaGPT: Meta Programming for Multi-Agent Systems implementation cover?",
    "output": "The FoundationAgents/MetaGPT repository covers: agent, gpt, llm, metagpt, multi-agent."
  },
  {
    "input": "How popular is the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation?",
    "output": "The lm-sys/FastChat repository has 39,298 stars and 4,778 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for SQuAD: 100,000+ Questions for Machine Comprehension of Text?",
    "output": "The code for SQuAD: 100,000+ Questions for Machine Comprehension of Text is available at https://github.com/rajpurkar/SQuAD-explorer (rajpurkar/SQuAD-explorer)."
  },
  {
    "input": "Give me an overview of Dream to Control: Learning Behaviors by Latent Imagination.",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "Summarize the Proximal Policy Optimization Algorithms paper.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,588 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 is available at https://github.com/openai/summarize-from-feedback (openai/summarize-from-feedback)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What language is the openai/frontier-evals repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the openai/evals implementation of GPT-4 Technical Report.",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 repository have?",
    "output": "The yafuly/TPO repository has 172 stars and 11 forks on GitHub."
  },
  {
    "input": "What year was Flamingo: A Visual Language Model for Few-Shot Learning released?",
    "output": "The Flamingo: A Visual Language Model for Few-Shot Learning paper was published in 2022."
  },
  {
    "input": "What year was LLaMA-Adapter: Efficient Fine-tuning of Language Models released?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models paper was published in 2023."
  },
  {
    "input": "Give me an overview of ViT: An Image is Worth 16x16 Words.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2204.14198?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "What does the KimMeen/Time-LLM repository do?",
    "output": "[ICLR 2024] Official implementation of \" \ud83e\udd99 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\""
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at uclaml/SPIN which the official implementation of self-play fine-tuning (spin) It is written in Python. The repository has 1,226 stars, indicating high community interest."
  },
  {
    "input": "What does the ruiqi-zhong/D5 repository do?",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What programming language is Med-PaLM 2: Towards Expert-Level Medical Question Answering implemented in?",
    "output": "The Med-PaLM 2: Towards Expert-Level Medical Question Answering implementation is written in Jupyter Notebook."
  },
  {
    "input": "Tell me the title of ArXiv paper 1901.02860.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at arcee-ai/mergekit which tools for merging pretrained large language models. It is written in Python. The repository has 6,564 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models?",
    "output": "The code for ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is available at https://github.com/deepspeedai/DeepSpeed (deepspeedai/DeepSpeed)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the pytorch/pytorch implementation of PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
    "output": "Tensors and Dynamic neural networks in Python with strong GPU acceleration"
  },
  {
    "input": "What is the ArXiv category of LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the facebookresearch/fairseq repository that implements arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What language is the FMInference/FlexLLMGen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2303.10512?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models"
  },
  {
    "input": "What does the oughtinc/ice repository do?",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is available at https://github.com/llava-rlhf/LLaVA-RLHF (llava-rlhf/LLaVA-RLHF)."
  },
  {
    "input": "Explain the ibm-granite/granite-code-models implementation of arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Where can I find the code for ViT: An Image is Worth 16x16 Words?",
    "output": "The code for ViT: An Image is Worth 16x16 Words is available at https://github.com/google-research/vision_transformer (google-research/vision_transformer)."
  },
  {
    "input": "Explain the Sequence to Sequence Learning with Neural Networks paper.",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,175 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Playing Atari with Deep Reinforcement Learning implementation?",
    "output": "The google-deepmind/dqn repository has 618 stars and 165 forks on GitHub."
  },
  {
    "input": "What topics does the Graph Attention Networks implementation cover?",
    "output": "The PetarV-/GAT repository covers: graph-attention-networks, attention-mechanism, self-attention, tensorflow, neural-networks, python."
  },
  {
    "input": "Describe the GanjinZero/RRHF repository that implements arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10.",
    "output": "[NIPS2023] RRHF & Wombat"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is available at https://github.com/llava-rlhf/LLaVA-RLHF (llava-rlhf/LLaVA-RLHF)."
  },
  {
    "input": "Describe the PetarV-/GAT repository that implements Graph Attention Networks.",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.17580?",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "What does the rbgirshick/py-faster-rcnn repository do?",
    "output": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1412.6980?",
    "output": "Adam: A Method for Stochastic Optimization"
  },
  {
    "input": "What year was RoFormer: Enhanced Transformer with Rotary Position Embedding released?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "Explain the mistralai/mistral-inference implementation of Mistral 7B.",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "What paper has ArXiv ID 1606.05250?",
    "output": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the PetarV-/GAT repository?",
    "output": "The PetarV-/GAT repository covers: graph-attention-networks, attention-mechanism, self-attention, tensorflow, neural-networks, python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 implementation?",
    "output": "The google/BIG-bench repository has 3,167 stars and 616 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2206.01861.",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "What does the ur-whitelab/chemcrow-public repository do?",
    "output": "Chemcrow"
  },
  {
    "input": "What programming language is Self-RAG: Learning to Retrieve, Generate, and Critique implemented in?",
    "output": "The Self-RAG: Learning to Retrieve, Generate, and Critique implementation is written in Python."
  },
  {
    "input": "Explain the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper.",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Playing Atari with Deep Reinforcement Learning paper in?",
    "output": "Playing Atari with Deep Reinforcement Learning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the BLIP: Bootstrapping Language-Image Pre-training implementation?",
    "output": "The salesforce/BLIP repository has 5,603 stars and 749 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.17580.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What is AlpacaEval: An Automatic Evaluator for Instruction-following about?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at tatsu-lab/alpaca_eval which an automatic evaluator for instruction-following language models. human-validated, high-quality, cheap, and fast. It is written in Jupyter Notebook. The repository has 1,924 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Proximal Policy Optimization Algorithms.",
    "output": "Proximal Policy Optimization Algorithms is a 2017 paper in Machine Learning. The implementation is available at openai/baselines which openai baselines: high-quality implementations of reinforcement learning algorithms It is written in Python. The repository has 16,588 stars, indicating high community interest."
  },
  {
    "input": "What does the allenai/codescientist repository do?",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "What paper has ArXiv ID 2503.01141?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2306.17492.",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2302.07459?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10"
  },
  {
    "input": "When was the AudioPaLM: A Large Language Model for Speech Understanding paper published?",
    "output": "The AudioPaLM: A Large Language Model for Speech Understanding paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the MiniGPT-4: Enhancing Vision-Language Understanding paper.",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at Vision-CAIR/MiniGPT-4 which open-sourced codes for minigpt-4 and minigpt-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/) It is written in Python. The repository has 25,762 stars, indicating high community interest."
  },
  {
    "input": "When was the Codex: Evaluating Large Language Models Trained on Code paper published?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code paper was published in 2021."
  },
  {
    "input": "What is the ArXiv category of DreamBooth: Fine Tuning Text-to-Image Diffusion Models?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of Code Llama: Open Foundation Models for Code.",
    "output": "Code Llama: Open Foundation Models for Code is a 2023 paper in Machine Learning. The implementation is available at meta-llama/codellama which inference code for codellama models It is written in Python. The repository has 16,365 stars, indicating high community interest."
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What are the key topics in the huggingface/alignment-handbook repository?",
    "output": "The huggingface/alignment-handbook repository covers: llm, rlhf, transformers."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What topics does the ReAct: Synergizing Reasoning and Acting in Language Models implementation cover?",
    "output": "The ysymyth/ReAct repository covers: decision-making, large-language-models, llm, prompting, reasoning."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What paper has ArXiv ID 2006.11239?",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 implementation?",
    "output": "The GanjinZero/RRHF repository has 811 stars and 45 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2301.00704?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 implementation cover?",
    "output": "The RLHFlow/Directional-Preference-Alignment repository covers: rlhf, ai-alignment, large-language-models."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at siddk/voltron-robotics which voltron: language-driven representation learning for robotics It is written in Python. The repository has 233 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Wide & Deep Learning for Recommender Systems?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What language is the Megvii-BaseDetection/YOLOX repository written in?",
    "output": "The YOLOX: Exceeding YOLO Series in 2021 implementation is written in Python."
  },
  {
    "input": "Explain the ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,956 stars, indicating high community interest."
  },
  {
    "input": "What language is the microsoft/unilm repository written in?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.12115.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.08575.",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities"
  },
  {
    "input": "What is the ArXiv category of Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 repository have?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What are the key topics in the hexiangnan/neural_collaborative_filtering repository?",
    "output": "The hexiangnan/neural_collaborative_filtering repository covers: deep-learning, recommender-system, collaborative-filtering."
  },
  {
    "input": "Explain the jax-ml/jax implementation of JAX: Composable transformations of Python+NumPy programs.",
    "output": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at GanjinZero/RRHF which [nips2023] rrhf & wombat It is written in Python. The repository has 811 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for BEiT: BERT Pre-Training of Image Transformers?",
    "output": "The code for BEiT: BERT Pre-Training of Image Transformers is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "How many stars does the Text Embeddings by Weakly-Supervised Contrastive Pre-training repository have?",
    "output": "The microsoft/unilm repository has 21,873 stars and 2,681 forks on GitHub."
  },
  {
    "input": "Give me an overview of Analyzing the Structure of Attention in a Transformer Language Model.",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Billion-scale similarity search with GPUs repository have?",
    "output": "The facebookresearch/faiss repository has 38,366 stars and 4,147 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.10683.",
    "output": "T5: Exploring the Limits of Transfer Learning"
  },
  {
    "input": "What language is the 0nutation/SpeechGPT repository written in?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation is written in Python."
  },
  {
    "input": "Explain the Diffusion Models Beat GANs on Image Synthesis paper.",
    "output": "Diffusion Models Beat GANs on Image Synthesis is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/guided-diffusion It is written in Python. The repository has 7,183 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/segment-anything repository do?",
    "output": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model."
  },
  {
    "input": "Explain the Zephyr: Direct Distillation of LM Alignment paper.",
    "output": "Zephyr: Direct Distillation of LM Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/alignment-handbook which robust recipes to align language models with human and ai preferences It is written in Python. The repository has 5,444 stars, indicating high community interest."
  },
  {
    "input": "What language is the bitsandbytes-foundation/bitsandbytes repository written in?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation is written in Python."
  },
  {
    "input": "What year was Zephyr: Direct Distillation of LM Alignment released?",
    "output": "The Zephyr: Direct Distillation of LM Alignment paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "Describe the princeton-nlp/SimCSE repository that implements SimCSE: Simple Contrastive Learning of Sentence Embeddings.",
    "output": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2204.05862?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10"
  },
  {
    "input": "What programming language is Linformer: Self-Attention with Linear Complexity implemented in?",
    "output": "The Linformer: Self-Attention with Linear Complexity implementation is written in Python."
  },
  {
    "input": "What does the microsoft/unilm repository do?",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What does the nyu-mll/jiant repository do?",
    "output": "jiant is an nlp toolkit"
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.10305.",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct"
  },
  {
    "input": "What is the ArXiv category of TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2302.00923?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Deep Residual Learning for Image Recognition?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did Enriching Word Vectors with Subword Information come out?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "What is the GitHub repository for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.08242?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Dream to Control: Learning Behaviors by Latent Imagination?",
    "output": "The code for Dream to Control: Learning Behaviors by Latent Imagination is available at https://github.com/danijar/dreamer (danijar/dreamer)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at arcee-ai/mergekit which tools for merging pretrained large language models. It is written in Python. The repository has 6,564 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Grounding DINO: Marrying DINO with Grounded Pre-Training.",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at IDEA-Research/GroundingDINO which [eccv 2024] official implementation of the paper \"grounding dino: marrying dino with grounded pre-training for open-set object detection\" It is written in Python. The repository has 9,401 stars, indicating high community interest."
  },
  {
    "input": "Summarize the FlashAttention-2: Faster Attention with Better Parallelism paper.",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tgxs002/align_sd which better aligning text-to-image models with human preference. iccv 2023 It is written in Python. The repository has 293 stars, indicating high community interest."
  },
  {
    "input": "What does the llava-rlhf/LLaVA-RLHF repository do?",
    "output": "Aligning LMMs with Factually Augmented RLHF"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the lucidrains/PaLM-pytorch implementation of PaLM: Scaling Language Modeling with Pathways.",
    "output": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.14314.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "When was the Decoupled Weight Decay Regularization paper published?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "When was the Depthwise Separable Convolutions for Neural Machine Translation paper published?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation paper was published in 2017."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 implementation cover?",
    "output": "The FMInference/FlexLLMGen repository covers: deep-learning, gpt-3, high-throughput, large-language-models, machine-learning, offloading, opt."
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "What does the openai/CLIP repository do?",
    "output": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image"
  },
  {
    "input": "Explain the turboderp-org/exllamav2 implementation of arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10.",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "What language is the autonomousvision/stylegan-t repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.15004.",
    "output": "Vicuna: An Open-Source Chatbot"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What does the salesforce/GeDi repository do?",
    "output": "GeDi: Generative Discriminator Guided Sequence Generation"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What does the microsoft/unilm repository do?",
    "output": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities"
  },
  {
    "input": "What is the ArXiv category of Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 implementation?",
    "output": "The mit-han-lab/offsite-tuning repository has 383 stars and 39 forks on GitHub."
  },
  {
    "input": "What language is the allenai/codescientist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the SmoothQuant: Accurate and Efficient Post-Training Quantization paper.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,567 stars, indicating high community interest."
  },
  {
    "input": "Describe the huawei-noah/Pretrained-Language-Model repository that implements TinyBERT: Distilling BERT for Natural Language Understanding.",
    "output": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the PKU-Alignment/align-anything repository?",
    "output": "The PKU-Alignment/align-anything repository covers: large-language-models, multimodal, rlhf, chameleon, dpo, vision-language-model."
  },
  {
    "input": "What category is the BERT: Pre-training of Deep Bidirectional Transformers paper in?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "How many stars does the Denoising Diffusion Implicit Models repository have?",
    "output": "The ermongroup/ddim repository has 1,752 stars and 229 forks on GitHub."
  },
  {
    "input": "Summarize the Billion-scale similarity search with GPUs paper.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,366 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What language is the meta-llama/codellama repository written in?",
    "output": "The Code Llama: Open Foundation Models for Code implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Describe the anthropics/hh-rlhf repository that implements arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10.",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "Summarize the LoRA: Low-Rank Adaptation of Large Language Models paper.",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 13,038 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.02732.",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 implementation?",
    "output": "The google-research/bert repository has 39,724 stars and 9,710 forks on GitHub."
  },
  {
    "input": "Describe the facebookresearch/ConvNeXt-V2 repository that implements arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Universal Language Model Fine-tuning for Text Classification released?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "What are the key topics in the MAXNORM8650/MedAgentSim repository?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "Summarize the Deep Residual Learning for Image Recognition paper.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,671 stars, indicating high community interest."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Explain the Deep Graph Library: A Graph-Centric, Highly-Performant Package paper.",
    "output": "Deep Graph Library: A Graph-Centric, Highly-Performant Package is a 2020 paper in Machine Learning. The implementation is available at dmlc/dgl which python package built to ease deep learning on graph, on top of existing dl frameworks. It is written in Python. The repository has 14,165 stars, indicating high community interest."
  },
  {
    "input": "What does the facebookresearch/Detectron repository do?",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "How many stars does the InstructBLIP: Towards General-purpose Vision-Language Models repository have?",
    "output": "The salesforce/LAVIS repository has 11,064 stars and 1,087 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 repository have?",
    "output": "The anthropics/hh-rlhf repository has 1,801 stars and 150 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1909.11942?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators"
  },
  {
    "input": "Give me an overview of SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,567 stars, indicating high community interest."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "When was the Analyzing the Structure of Attention in a Transformer Language Model paper published?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "When was the LLaMA: Open and Efficient Foundation Language Models paper published?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.08582.",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10"
  },
  {
    "input": "What category is the GPT-4 Technical Report paper in?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the Vicuna: An Open-Source Chatbot paper.",
    "output": "Vicuna: An Open-Source Chatbot is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lm-sys/FastChat which an open platform for training, serving, and evaluating large language models. release repo for vicuna and chatbot arena. It is written in Python. The repository has 39,298 stars, indicating high community interest."
  },
  {
    "input": "Explain the zai-org/ImageReward implementation of arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10.",
    "output": "[NeurIPS 2023] ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation"
  },
  {
    "input": "Which field does PaLM: Scaling Language Modeling with Pathways belong to?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the microsoft/DeBERTa repository?",
    "output": "The microsoft/DeBERTa repository covers: bert, deeplearning, representation-learning, roberta, language-model, natural-language-understanding, self-attention, transformer-encoder."
  },
  {
    "input": "What's the implementation repository for Analyzing the Structure of Attention in a Transformer Language Model?",
    "output": "The code for Analyzing the Structure of Attention in a Transformer Language Model is available at https://github.com/clarkkev/attention-analysis (clarkkev/attention-analysis)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is available at https://github.com/uclaml/SPIN (uclaml/SPIN)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-3.0-language-models (ibm-granite/granite-3.0-language-models)."
  },
  {
    "input": "When was the Neural Message Passing for Quantum Chemistry paper published?",
    "output": "The Neural Message Passing for Quantum Chemistry paper was published in 2017."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is available at https://github.com/google/BIG-bench (google/BIG-bench)."
  },
  {
    "input": "What language is the state-spaces/s4 repository written in?",
    "output": "The S4: Efficiently Modeling Long Sequences implementation is written in Jupyter Notebook."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.03011.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "Explain the rajpurkar/SQuAD-explorer implementation of SQuAD: 100,000+ Questions for Machine Comprehension of Text.",
    "output": "Visually Explore the Stanford Question Answering Dataset"
  },
  {
    "input": "What programming language is SQuAD: 100,000+ Questions for Machine Comprehension of Text implemented in?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation is written in JavaScript."
  },
  {
    "input": "Summarize the DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper.",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/DeBERTa which the implementation of deberta It is written in Python. The repository has 2,177 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 266 stars and 28 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What year was Denoising Diffusion Probabilistic Models released?",
    "output": "The Denoising Diffusion Probabilistic Models paper was published in 2020."
  },
  {
    "input": "What language is the google-deepmind/dqn repository written in?",
    "output": "The Playing Atari with Deep Reinforcement Learning implementation is written in Lua."
  },
  {
    "input": "What programming language is BLIP: Bootstrapping Language-Image Pre-training implemented in?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "Which field does BLIP: Bootstrapping Language-Image Pre-training belong to?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is available at https://github.com/AlibabaResearch/DAMO-ConvAI (AlibabaResearch/DAMO-ConvAI)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the meta-llama/codellama repository that implements Code Llama: Open Foundation Models for Code.",
    "output": "Inference code for CodeLlama models"
  },
  {
    "input": "What are the key topics in the google-research/bert repository?",
    "output": "The google-research/bert repository covers: nlp, google, natural-language-processing, natural-language-understanding, tensorflow."
  },
  {
    "input": "What programming language is LLaMA: Open and Efficient Foundation Language Models implemented in?",
    "output": "The LLaMA: Open and Efficient Foundation Language Models implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at GXimingLu/Quark It is written in Python. The repository has 75 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Know What You Don't Know: Unanswerable Questions for SQuAD repository have?",
    "output": "The rajpurkar/SQuAD-explorer repository has 569 stars and 121 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1802.05365?",
    "output": "Universal Language Model Fine-tuning for Text Classification"
  },
  {
    "input": "In what year did XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale come out?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale paper was published in 2021."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is available at https://github.com/allenai/RL4LMs (allenai/RL4LMs)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 repository have?",
    "output": "The turboderp-org/exllamav2 repository has 4,380 stars and 325 forks on GitHub."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The Decoupled Weight Decay Regularization implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "Give me an overview of Dense Passage Retrieval for Open-Domain Question Answering.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2111.00396?",
    "output": "S4: Efficiently Modeling Long Sequences"
  },
  {
    "input": "Which field does BERT: Pre-training of Deep Bidirectional Transformers belong to?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What are the key topics in the huggingface/transformers repository?",
    "output": "The huggingface/transformers repository covers: nlp, natural-language-processing, pytorch, pytorch-transformers, transformer, model-hub, pretrained-models, speech-recognition, hacktoberfest, python, machine-learning, deep-learning, audio, deepseek, gemma, glm, llm, qwen, vlm."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at OpenRLHF/OpenRLHF which an easy-to-use, scalable and high-performance rlhf framework based on ray (ppo & grpo & reinforce++ & vllm & ray & dynamic sampling & async agentic rl) It is written in Python. The repository has 8,562 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of DistilBERT: a distilled version of BERT.",
    "output": "DistilBERT: a distilled version of BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huggingface/transformers which \ud83e\udd17 transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  It is written in Python. The repository has 153,664 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Know What You Don't Know: Unanswerable Questions for SQuAD paper in?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.00190.",
    "output": "The Power of Scale for Parameter-Efficient Prompt Tuning"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 implementation cover?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "What does the mit-han-lab/offsite-tuning repository do?",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Which field does Beir: A Heterogeneous Benchmark for IR belong to?",
    "output": "Beir: A Heterogeneous Benchmark for IR is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What does the alphadl/lookahead.pytorch repository do?",
    "output": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch "
  },
  {
    "input": "What language is the rajpurkar/SQuAD-explorer repository written in?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation is written in JavaScript."
  },
  {
    "input": "Give me an overview of Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,801 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the S4: Efficiently Modeling Long Sequences repository have?",
    "output": "The state-spaces/s4 repository has 2,794 stars and 349 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "Explain the mit-han-lab/smoothquant implementation of SmoothQuant: Accurate and Efficient Post-Training Quantization.",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "Which field does EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification belong to?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 1905.05583?",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at GanjinZero/RRHF which [nips2023] rrhf & wombat It is written in Python. The repository has 811 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 is available at https://github.com/allenai/RL4LMs (allenai/RL4LMs)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the facebookresearch/TransCoder repository that implements arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10.",
    "output": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is available at https://github.com/nyu-mll/quality (nyu-mll/quality)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What language is the LTH14/fractalgen repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does RAG: Retrieval-Augmented Generation belong to?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Wide & Deep Learning for Recommender Systems released?",
    "output": "The Wide & Deep Learning for Recommender Systems paper was published in 2016."
  },
  {
    "input": "What language is the tmikolov/word2vec repository written in?",
    "output": "The Efficient Estimation of Word Representations in Vector Space implementation is written in C."
  },
  {
    "input": "What language is the tatp22/linformer-pytorch repository written in?",
    "output": "The Linformer: Self-Attention with Linear Complexity implementation is written in Python."
  },
  {
    "input": "Explain the PyTorch Geometric: Library for Geometric Deep Learning paper.",
    "output": "PyTorch Geometric: Library for Geometric Deep Learning is a 2019 paper in Machine Learning. The implementation is available at pyg-team/pytorch_geometric which graph neural network library for pytorch It is written in Python. The repository has 23,255 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Code Llama: Open Foundation Models for Code?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the S4: Efficiently Modeling Long Sequences paper.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,794 stars, indicating high community interest."
  },
  {
    "input": "What topics does the Know What You Don't Know: Unanswerable Questions for SQuAD implementation cover?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at GAIR-NLP/LIMO which [colm 2025] limo: less is more for reasoning It is written in Python. The repository has 1,054 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10 implementation?",
    "output": "The nyu-mll/quality repository has 144 stars and 10 forks on GitHub."
  },
  {
    "input": "Describe the CompVis/latent-diffusion repository that implements High-Resolution Image Synthesis with Latent Diffusion Models.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "What programming language is Decoupled Weight Decay Regularization implemented in?",
    "output": "The Decoupled Weight Decay Regularization implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of RoFormer: Enhanced Transformer with Rotary Position Embedding.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,059 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of Gorilla: Large Language Model Connected with Massive APIs?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "What year was Efficient Estimation of Word Representations in Vector Space released?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.04129.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10"
  },
  {
    "input": "Explain the oughtinc/ice implementation of arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10.",
    "output": "Interactive Composition Explorer: a debugger for compositional language model programs"
  },
  {
    "input": "Explain the FoundationAgents/MetaGPT implementation of MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "What language is the google-research/vision_transformer repository written in?",
    "output": "The ViT: An Image is Worth 16x16 Words implementation is written in Jupyter Notebook."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is ControlNet: Adding Conditional Control to Text-to-Image Diffusion about?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at lllyasviel/ControlNet which let us control diffusion models! It is written in Python. The repository has 33,415 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Flamingo: A Visual Language Model for Few-Shot Learning?",
    "output": "The code for Flamingo: A Visual Language Model for Few-Shot Learning is available at https://github.com/lucidrains/flamingo-pytorch (lucidrains/flamingo-pytorch)."
  },
  {
    "input": "Describe the tatsu-lab/alpaca_eval repository that implements AlpacaEval: An Automatic Evaluator for Instruction-following.",
    "output": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast."
  },
  {
    "input": "What does the simplescaling/s1 repository do?",
    "output": "s1: Simple test-time scaling"
  },
  {
    "input": "What language is the XiangLi1999/PrefixTuning repository written in?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation is written in Python."
  },
  {
    "input": "In what year did LLM.int8(): 8-bit Matrix Multiplication for Transformers come out?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper published?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper was published in 2021."
  },
  {
    "input": "What language is the OpenBMB/Eurus repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the Reformer: The Efficient Transformer repository have?",
    "output": "The google/trax repository has 8,294 stars and 827 forks on GitHub."
  },
  {
    "input": "Which field does Denoising Diffusion Implicit Models belong to?",
    "output": "Denoising Diffusion Implicit Models is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the FlashAttention: Fast and Memory-Efficient Exact Attention repository have?",
    "output": "The Dao-AILab/flash-attention repository has 20,993 stars and 2,199 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does WizardCoder: Empowering Code LLMs with Evol-Instruct belong to?",
    "output": "WizardCoder: Empowering Code LLMs with Evol-Instruct is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "How popular is the Vicuna: An Open-Source Chatbot implementation?",
    "output": "The lm-sys/FastChat repository has 39,298 stars and 4,778 forks on GitHub."
  },
  {
    "input": "What is SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities about?",
    "output": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at 0nutation/SpeechGPT which speechgpt series: speech large language models It is written in Python. The repository has 1,396 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Dense Passage Retrieval for Open-Domain Question Answering released?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering paper was published in 2020."
  },
  {
    "input": "How popular is the LIMA: Less Is More for Alignment implementation?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "What is Distilling the Knowledge in a Neural Network about?",
    "output": "Distilling the Knowledge in a Neural Network is a 2015 paper in Machine Learning. The implementation is available at haitongli/knowledge-distillation-pytorch which a pytorch implementation for exploring deep and shallow knowledge distillation (kd) experiments with flexibility It is written in Python. The repository has 1,974 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 repository have?",
    "output": "The chenfei-wu/TaskMatrix repository has 34,341 stars and 3,268 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the farizrahman4u/seq2seq repository do?",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at llava-rlhf/LLaVA-RLHF which aligning lmms with factually augmented rlhf It is written in Python. The repository has 387 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "Describe the frankxu2004/knnlm-why repository that implements arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena come out?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "What language is the horovod/horovod repository written in?",
    "output": "The Horovod: fast and easy distributed deep learning in TensorFlow implementation is written in Python."
  },
  {
    "input": "What category is the MiniGPT-4: Enhancing Vision-Language Understanding paper in?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Explain the tensorflow/tensor2tensor implementation of Attention Is All You Need.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the farizrahman4u/seq2seq implementation of Sequence to Sequence Learning with Neural Networks.",
    "output": "Sequence to Sequence Learning with Keras"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Tell me the title of ArXiv paper 2304.05977.",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10"
  },
  {
    "input": "What programming language is LoRA: Low-Rank Adaptation of Large Language Models implemented in?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models implementation is written in Python."
  },
  {
    "input": "Explain the frankxu2004/knnlm-why implementation of arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1905.07830?",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2310.06825?",
    "output": "Mistral 7B"
  },
  {
    "input": "What does the google-research/google-research repository do?",
    "output": "Google Research"
  },
  {
    "input": "What paper has ArXiv ID 2102.02779?",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "What year was LoRA: Low-Rank Adaptation of Large Language Models released?",
    "output": "The LoRA: Low-Rank Adaptation of Large Language Models paper was published in 2021."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "In what year did Synthetic Data Generation with Large Language Models come out?",
    "output": "The Synthetic Data Generation with Large Language Models paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.14030.",
    "output": "BEiT: BERT Pre-Training of Image Transformers"
  },
  {
    "input": "How many stars does the Segment Anything repository have?",
    "output": "The facebookresearch/segment-anything repository has 52,837 stars and 6,170 forks on GitHub."
  },
  {
    "input": "How popular is the Linformer: Self-Attention with Linear Complexity implementation?",
    "output": "The tatp22/linformer-pytorch repository has 421 stars and 37 forks on GitHub."
  },
  {
    "input": "Explain the anthropics/hh-rlhf implementation of arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10.",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "How many stars does the Beir: A Heterogeneous Benchmark for IR repository have?",
    "output": "The beir-cellar/beir repository has 2,014 stars and 225 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 repository have?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning?",
    "output": "The code for The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "Give me an overview of BERT Rediscovers the Classical NLP Pipeline.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "What category is the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper in?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "In what year did Denoising Diffusion Implicit Models come out?",
    "output": "The Denoising Diffusion Implicit Models paper was published in 2020."
  },
  {
    "input": "When was the Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper published?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at sylinrl/TruthfulQA which truthfulqa: measuring how models imitate human falsehoods It is written in Jupyter Notebook. The repository has 854 stars, indicating high community interest."
  },
  {
    "input": "Explain the openai/baselines implementation of Proximal Policy Optimization Algorithms.",
    "output": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms"
  },
  {
    "input": "What year was Code Llama: Open Foundation Models for Code released?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2112.11446?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "In what year did Gorilla: Large Language Model Connected with Massive APIs come out?",
    "output": "The Gorilla: Large Language Model Connected with Massive APIs paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at GXimingLu/Quark It is written in Python. The repository has 75 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "How many stars does the LLaMA-Adapter: Efficient Fine-tuning of Language Models repository have?",
    "output": "The OpenGVLab/LLaMA-Adapter repository has 5,924 stars and 384 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Analyzing the Structure of Attention in a Transformer Language Model released?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "Explain the DreamBooth: Fine Tuning Text-to-Image Diffusion Models paper.",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at google/dreambooth The repository has 1,005 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "How popular is the Code Llama: Open Foundation Models for Code implementation?",
    "output": "The meta-llama/codellama repository has 16,365 stars and 1,946 forks on GitHub."
  },
  {
    "input": "Explain the rail-berkeley/softlearning implementation of Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "What is PaLM: Scaling Language Modeling with Pathways about?",
    "output": "PaLM: Scaling Language Modeling with Pathways is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/PaLM-pytorch which implementation of the specific transformer architecture from palm - scaling language modeling with pathways It is written in Python. The repository has 827 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Semi-Supervised Classification with Graph Convolutional Networks implemented in?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.06825.",
    "output": "Mistral 7B"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 implementation?",
    "output": "The ali-vilab/composer repository has 1,560 stars and 49 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 is available at https://github.com/siddk/voltron-robotics (siddk/voltron-robotics)."
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Explain the Deep Residual Learning for Image Recognition paper.",
    "output": "Deep Residual Learning for Image Recognition is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at KaimingHe/deep-residual-networks which deep residual learning for image recognition  The repository has 6,671 stars, indicating high community interest."
  },
  {
    "input": "Explain the Linear95/APO implementation of arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10.",
    "output": "Code for ACL2024 paper - Adversarial Preference Optimization (APO)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation cover?",
    "output": "The MAXNORM8650/MedAgentSim repository covers: multi-agent-simulations."
  },
  {
    "input": "How popular is the Diffusion Models Beat GANs on Image Synthesis implementation?",
    "output": "The openai/guided-diffusion repository has 7,183 stars and 887 forks on GitHub."
  },
  {
    "input": "What does the HumanAIGC/AnimateAnyone repository do?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
  },
  {
    "input": "How many stars does the Horovod: fast and easy distributed deep learning in TensorFlow repository have?",
    "output": "The horovod/horovod repository has 14,642 stars and 2,257 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-code-models repository has 1,245 stars and 87 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.07842?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning"
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What paper has ArXiv ID 2303.01469?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation?",
    "output": "The chenfei-wu/TaskMatrix repository has 34,341 stars and 3,268 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at lucidrains/muse-maskgit-pytorch which implementation of muse: text-to-image generation via masked generative transformers, in pytorch It is written in Python. The repository has 919 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 implementation cover?",
    "output": "The mit-han-lab/offsite-tuning repository covers: deep-learning, transfer-learning."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.10375?",
    "output": "InPars: Data Augmentation for Information Retrieval"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at OptimalScale/LMFlow which an extensible toolkit for finetuning and inference of large foundation models. large models for all. It is written in Python. The repository has 8,489 stars, indicating high community interest."
  },
  {
    "input": "What does the autonomousvision/stylegan-t repository do?",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.03011?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2310.11511?",
    "output": "Zephyr: Direct Distillation of LM Alignment"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.11844?",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10"
  },
  {
    "input": "In what year did Efficient Estimation of Word Representations in Vector Space come out?",
    "output": "The Efficient Estimation of Word Representations in Vector Space paper was published in 2013."
  },
  {
    "input": "Explain the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2006.04768?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
  },
  {
    "input": "How popular is the Synthetic Data Generation with Large Language Models implementation?",
    "output": "The google-research/google-research repository has 36,869 stars and 8,267 forks on GitHub."
  },
  {
    "input": "What language is the openai/guided-diffusion repository written in?",
    "output": "The Diffusion Models Beat GANs on Image Synthesis implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is available at https://github.com/FMInference/FlexLLMGen (FMInference/FlexLLMGen)."
  },
  {
    "input": "Explain the uclaml/SPIN implementation of arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10.",
    "output": "The official implementation of Self-Play Fine-Tuning (SPIN)"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "How many stars does the Neural Collaborative Filtering repository have?",
    "output": "The hexiangnan/neural_collaborative_filtering repository has 1,865 stars and 666 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did LIMA: Less Is More for Alignment come out?",
    "output": "The LIMA: Less Is More for Alignment paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.07958.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What category is the Whisper: Robust Speech Recognition via Large-Scale Weak Supervision paper in?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the pytorch/pytorch repository written in?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Shark-NLP/OpenICL which openicl is an open-source framework to facilitate research, development, and prototyping of in-context learning. It is written in Python. The repository has 582 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the RetNet: Retentive Network: A Successor to Transformer paper published?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Semi-Supervised Classification with Graph Convolutional Networks implementation?",
    "output": "The tkipf/gcn repository has 7,339 stars and 2,013 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2311.08045?",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2502.09992?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the ELECTRA: Pre-training Text Encoders as Discriminators implementation cover?",
    "output": "The google-research/electra repository covers: nlp, deep-learning, tensorflow."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10 is available at https://github.com/rll-research/BPref (rll-research/BPref)."
  },
  {
    "input": "Give me an overview of Billion-scale similarity search with GPUs.",
    "output": "Billion-scale similarity search with GPUs is a 2019 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/faiss which a library for efficient similarity search and clustering of dense vectors. It is written in C++. The repository has 38,366 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1703.04691?",
    "output": "Neural Message Passing for Quantum Chemistry"
  },
  {
    "input": "How popular is the High-Resolution Image Synthesis with Latent Diffusion Models implementation?",
    "output": "The CompVis/latent-diffusion repository has 13,646 stars and 1,702 forks on GitHub."
  },
  {
    "input": "Describe the mistralai/mistral-inference repository that implements Mistral 7B.",
    "output": "Official inference library for Mistral models"
  },
  {
    "input": "Where can I find the code for YOLOX: Exceeding YOLO Series in 2021?",
    "output": "The code for YOLOX: Exceeding YOLO Series in 2021 is available at https://github.com/Megvii-BaseDetection/YOLOX (Megvii-BaseDetection/YOLOX)."
  },
  {
    "input": "Describe the uber/orbit repository that implements arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What year was LLaVA: Large Language and Vision Assistant released?",
    "output": "The LLaVA: Large Language and Vision Assistant paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.05302?",
    "output": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10"
  },
  {
    "input": "What does the ibm-granite/granite-code-models repository do?",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "Summarize the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,517 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at simplescaling/s1 which s1: simple test-time scaling It is written in Python. The repository has 6,611 stars, indicating high community interest."
  },
  {
    "input": "Explain the BERT Rediscovers the Classical NLP Pipeline paper.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2304.08485?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding"
  },
  {
    "input": "What is the ArXiv category of ColBERT: Efficient Passage Search via Contextualized Late Interaction?",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Directional-Preference-Alignment which directional preference alignment The repository has 58 stars, indicating high community interest."
  },
  {
    "input": "Describe the mit-han-lab/offsite-tuning repository that implements arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10.",
    "output": "Offsite-Tuning: Transfer Learning without Full Model"
  },
  {
    "input": "Describe the ShishirPatil/gorilla repository that implements Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "Which field does LoRA: Low-Rank Adaptation of Large Language Models belong to?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel released?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "What is the ArXiv category of Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech.",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Explain the THUDM/P-tuning-v2 implementation of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at salesforce/booksum It is written in Python. The repository has 195 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the LLaVA: Large Language and Vision Assistant repository have?",
    "output": "The haotian-liu/LLaVA repository has 24,140 stars and 2,670 forks on GitHub."
  },
  {
    "input": "In what year did Competition-Level Code Generation with AlphaCode come out?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "What's the implementation repository for Multi-Task Deep Neural Networks for Natural Language Understanding?",
    "output": "The code for Multi-Task Deep Neural Networks for Natural Language Understanding is available at https://github.com/namisan/mt-dnn (namisan/mt-dnn)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 implementation?",
    "output": "The GXimingLu/Quark repository has 75 stars and 10 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What language is the MAXNORM8650/MedAgentSim repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Deep Residual Learning for Image Recognition come out?",
    "output": "The Deep Residual Learning for Image Recognition paper was published in 2015."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.00027.",
    "output": "SimCSE: Simple Contrastive Learning of Sentence Embeddings"
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Synthetic Data Generation with Large Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "Describe the plurai-ai/intellagent repository that implements arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Summarize the Semi-Supervised Classification with Graph Convolutional Networks paper.",
    "output": "Semi-Supervised Classification with Graph Convolutional Networks is a 2016 paper in Machine Learning. The implementation is available at tkipf/gcn which implementation of graph convolutional networks in tensorflow It is written in Python. The repository has 7,339 stars, indicating high community interest."
  },
  {
    "input": "What are the key topics in the zai-org/ImageReward repository?",
    "output": "The zai-org/ImageReward repository covers: diffusion-models, generative-model, rlhf, human-preferences."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What category is the Codex: Evaluating Large Language Models Trained on Code paper in?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2101.04909?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 implementation?",
    "output": "The TsinghuaAI/CPM-1-Generate repository has 1,582 stars and 211 forks on GitHub."
  },
  {
    "input": "Describe the liuzhuang13/DenseNet repository that implements Densely Connected Convolutional Networks.",
    "output": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award)."
  },
  {
    "input": "How popular is the LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation?",
    "output": "The OpenGVLab/LLaMA-Adapter repository has 5,924 stars and 384 forks on GitHub."
  },
  {
    "input": "Describe the GAIR-NLP/LIMO repository that implements arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "What programming language is Mistral 7B implemented in?",
    "output": "The Mistral 7B implementation is written in Jupyter Notebook."
  },
  {
    "input": "Describe the Shark-NLP/OpenICL repository that implements arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10.",
    "output": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at volcengine/verl which verl: volcano engine reinforcement learning for llms It is written in Python. The repository has 17,344 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2405.07863?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of ReAct: Synergizing Reasoning and Acting in Language Models?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2106.09685?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models"
  },
  {
    "input": "Summarize the HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper.",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2210.03629?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models"
  },
  {
    "input": "When was the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper published?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper was published in 2022."
  },
  {
    "input": "Describe the salesforce/LAVIS repository that implements BLIP-2: Bootstrapping Language-Image Pre-training.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.00902?",
    "output": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 1708.05031.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What topics does the Horovod: fast and easy distributed deep learning in TensorFlow implementation cover?",
    "output": "The horovod/horovod repository covers: tensorflow, uber, machine-learning, machinelearning, mpi, baidu, deep-learning, deeplearning, keras, pytorch, mxnet, spark, ray."
  },
  {
    "input": "What language is the haoliuhl/language-quantized-autoencoders repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the amazon-science/mm-cot repository do?",
    "output": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2302.10866.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for TinyBERT: Distilling BERT for Natural Language Understanding?",
    "output": "The code for TinyBERT: Distilling BERT for Natural Language Understanding is available at https://github.com/huawei-noah/Pretrained-Language-Model (huawei-noah/Pretrained-Language-Model)."
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Distilling the Knowledge in a Neural Network implementation?",
    "output": "The haitongli/knowledge-distillation-pytorch repository has 1,974 stars and 351 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2304.05977?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "How popular is the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning implementation?",
    "output": "The haotian-liu/LLaVA repository has 24,140 stars and 2,670 forks on GitHub."
  },
  {
    "input": "Give me an overview of Inductive Representation Learning on Large Graphs.",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,636 stars, indicating high community interest."
  },
  {
    "input": "What is Segment Anything about?",
    "output": "Segment Anything is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/segment-anything which the repository provides code for running inference with the segmentanything model (sam), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. It is written in Jupyter Notebook. The repository has 52,837 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.06500.",
    "output": "InstructBLIP: Towards General-purpose Vision-Language Models"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 repository have?",
    "output": "The tianjunz/HIR repository has 159 stars and 11 forks on GitHub."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "In what year did Decoupled Weight Decay Regularization come out?",
    "output": "The Decoupled Weight Decay Regularization paper was published in 2017."
  },
  {
    "input": "Explain the AlibabaResearch/DAMO-ConvAI implementation of arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10.",
    "output": "DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 is available at https://github.com/OpenRLHF/OpenRLHF (OpenRLHF/OpenRLHF)."
  },
  {
    "input": "How popular is the Zephyr: Direct Distillation of LM Alignment implementation?",
    "output": "The huggingface/alignment-handbook repository has 5,444 stars and 464 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What language is the CompVis/latent-diffusion repository written in?",
    "output": "The High-Resolution Image Synthesis with Latent Diffusion Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "Explain the machrisaa/tensorflow-vgg implementation of Very Deep Convolutional Networks for Large-Scale Image Recognition.",
    "output": "VGG19 and VGG16 on Tensorflow"
  },
  {
    "input": "Explain the AudioPaLM: A Large Language Model for Speech Understanding paper.",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,610 stars, indicating high community interest."
  },
  {
    "input": "Summarize the LLaMA: Open and Efficient Foundation Language Models paper.",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1711.05101.",
    "output": "Decoupled Weight Decay Regularization"
  },
  {
    "input": "Describe the ibm-granite/granite-code-models repository that implements arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10.",
    "output": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence"
  },
  {
    "input": "What does the meta-llama/llama repository do?",
    "output": "Inference code for Llama models"
  },
  {
    "input": "Explain the LIMA: Less Is More for Alignment paper.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What are the key topics in the google-research/prompt-tuning repository?",
    "output": "The google-research/prompt-tuning repository covers: prompt-tuning, nlp, machine-learning, jax, flax, language-model."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is available at https://github.com/RLHFlow/Online-RLHF (RLHFlow/Online-RLHF)."
  },
  {
    "input": "What paper has ArXiv ID 2504.01848?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for ALBERT: A Lite BERT?",
    "output": "The code for ALBERT: A Lite BERT is available at https://github.com/google-research/albert (google-research/albert)."
  },
  {
    "input": "How popular is the Multi-Task Deep Neural Networks for Natural Language Understanding implementation?",
    "output": "The namisan/mt-dnn repository has 2,258 stars and 413 forks on GitHub."
  },
  {
    "input": "What language is the google-research/google-research repository written in?",
    "output": "The Depthwise Separable Convolutions for Neural Machine Translation implementation is written in Jupyter Notebook."
  },
  {
    "input": "How popular is the RoFormer: Enhanced Transformer with Rotary Position Embedding implementation?",
    "output": "The ZhuiyiTechnology/roformer repository has 1,059 stars and 59 forks on GitHub."
  },
  {
    "input": "Summarize the ALBERT: A Lite BERT paper.",
    "output": "ALBERT: A Lite BERT is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/albert which albert: a lite bert for self-supervised learning of language representations It is written in Python. The repository has 3,275 stars, indicating high community interest."
  },
  {
    "input": "In what year did MetaGPT: Meta Programming for Multi-Agent Systems come out?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2005.14165?",
    "output": "GPT-3: Language Models are Few-Shot Learners"
  },
  {
    "input": "What topics does the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel implementation cover?",
    "output": "The pytorch/pytorch repository covers: neural-network, autograd, gpu, numpy, deep-learning, tensor, python, machine-learning."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at PKU-Alignment/align-anything which align anything: training all-modality model with feedback It is written in Python. The repository has 4,601 stars, indicating high community interest."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "Give me an overview of ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.",
    "output": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models is a 2019 paper in Machine Learning. The implementation is available at deepspeedai/DeepSpeed which deepspeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective. It is written in Python. The repository has 40,956 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for LLaMA: Open and Efficient Foundation Language Models?",
    "output": "The code for LLaMA: Open and Efficient Foundation Language Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Attention Is All You Need implementation?",
    "output": "The tensorflow/tensor2tensor repository has 16,800 stars and 3,698 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 repository have?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is available at https://github.com/tomekkorbak/pretraining-with-human-feedback (tomekkorbak/pretraining-with-human-feedback)."
  },
  {
    "input": "What is RetNet: Retentive Network: A Successor to Transformer about?",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,124 stars, indicating high community interest."
  },
  {
    "input": "What does the lucidrains/flamingo-pytorch repository do?",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "Summarize the Generating Wikipedia by Summarizing Long Sequences paper.",
    "output": "Generating Wikipedia by Summarizing Long Sequences is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at tensorflow/tensor2tensor which library of deep learning models and datasets designed to make deep learning more accessible and accelerate ml research. It is written in Python. The repository has 16,801 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the DreamFusion: Text-to-3D using 2D Diffusion repository have?",
    "output": "The ashawkey/stable-dreamfusion repository has 8,785 stars and 771 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 is available at https://github.com/anthropics/evals (anthropics/evals)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.03378?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is available at https://github.com/haoliuhl/language-quantized-autoencoders (haoliuhl/language-quantized-autoencoders)."
  },
  {
    "input": "Explain the GPT-3: Language Models are Few-Shot Learners paper.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,774 stars, indicating high community interest."
  },
  {
    "input": "Explain the salesforce/BLIP implementation of BLIP: Bootstrapping Language-Image Pre-training.",
    "output": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  "
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What language is the jax-ml/jax repository written in?",
    "output": "The JAX: Composable transformations of Python+NumPy programs implementation is written in Python."
  },
  {
    "input": "What topics does the Depthwise Separable Convolutions for Neural Machine Translation implementation cover?",
    "output": "The google-research/google-research repository covers: machine-learning, ai, research."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/TransCoder which public release of the transcoder research project https://arxiv.org/pdf/2006.03511.pdf It is written in Python. The repository has 1,724 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the autonomousvision/stylegan-t repository that implements arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10.",
    "output": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at TsinghuaAI/CPM-1-Generate which chinese pre-trained language models (cpm-lm) version-i It is written in Python. The repository has 1,582 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2504.19413.",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10"
  },
  {
    "input": "What language is the Open-Reasoner-Zero/Open-Reasoner-Zero repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "In what year did Code Llama: Open Foundation Models for Code come out?",
    "output": "The Code Llama: Open Foundation Models for Code paper was published in 2023."
  },
  {
    "input": "Which field does Attention Is All You Need belong to?",
    "output": "Attention Is All You Need is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What year was Text Embeddings by Weakly-Supervised Contrastive Pre-training released?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training paper was published in 2022."
  },
  {
    "input": "What category is the Segment Anything in Medical Images paper in?",
    "output": "Segment Anything in Medical Images is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 implementation?",
    "output": "The yafuly/TPO repository has 172 stars and 11 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2010.02502?",
    "output": "Denoising Diffusion Implicit Models"
  },
  {
    "input": "What programming language is Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implemented in?",
    "output": "The Time-LLM: Time Series Forecasting by Reprogramming Large Language Models implementation is written in Python."
  },
  {
    "input": "What language is the openai/human-eval repository written in?",
    "output": "The Codex: Evaluating Large Language Models Trained on Code implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 repository have?",
    "output": "The volcengine/verl repository has 17,344 stars and 2,782 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is available at https://github.com/OpenBMB/Eurus (OpenBMB/Eurus)."
  },
  {
    "input": "What is SmoothQuant: Accurate and Efficient Post-Training Quantization about?",
    "output": "SmoothQuant: Accurate and Efficient Post-Training Quantization is a 2022 paper in Machine Learning. The implementation is available at mit-han-lab/smoothquant which [icml 2023] smoothquant: accurate and efficient post-training quantization for large language models It is written in Python. The repository has 1,567 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 1905.05583.",
    "output": "Billion-scale similarity search with GPUs"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 repository have?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "Explain the salesforce/LAVIS implementation of InstructBLIP: Towards General-purpose Vision-Language Models.",
    "output": "LAVIS - A One-stop Library for Language-Vision Intelligence"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What does the zihangdai/xlnet repository do?",
    "output": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
  },
  {
    "input": "Describe the KaimingHe/deep-residual-networks repository that implements Deep Residual Learning for Image Recognition.",
    "output": "Deep Residual Learning for Image Recognition "
  },
  {
    "input": "Where can I find the code for Zephyr: Direct Distillation of LM Alignment?",
    "output": "The code for Zephyr: Direct Distillation of LM Alignment is available at https://github.com/huggingface/alignment-handbook (huggingface/alignment-handbook)."
  },
  {
    "input": "How many stars does the SimCSE: Simple Contrastive Learning of Sentence Embeddings repository have?",
    "output": "The princeton-nlp/SimCSE repository has 3,623 stars and 533 forks on GitHub."
  },
  {
    "input": "In what year did DeBERTa: Decoding-enhanced BERT with Disentangled Attention come out?",
    "output": "The DeBERTa: Decoding-enhanced BERT with Disentangled Attention paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 2204.02311?",
    "output": "PaLM: Scaling Language Modeling with Pathways"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.17437.",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10"
  },
  {
    "input": "Describe the lm-sys/FastChat repository that implements Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2502.09992.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "Explain the hpcaitech/Open-Sora implementation of arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "When was the StarCoder: May the source be with you! paper published?",
    "output": "The StarCoder: May the source be with you! paper was published in 2023."
  },
  {
    "input": "What is the ArXiv category of Reflexion: Language Agents with Verbal Reinforcement Learning?",
    "output": "Reflexion: Language Agents with Verbal Reinforcement Learning is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Which field does Know What You Don't Know: Unanswerable Questions for SQuAD belong to?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 repository have?",
    "output": "The OpenRLHF/OpenRLHF repository has 8,562 stars and 826 forks on GitHub."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2305.10425?",
    "output": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is Inductive Representation Learning on Large Graphs about?",
    "output": "Inductive Representation Learning on Large Graphs is a 2017 paper in cs.SI. The implementation is available at williamleif/GraphSAGE which representation learning on large graphs using stochastic graph convolutions. It is written in Python. The repository has 3,636 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implemented in?",
    "output": "The Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.08073?",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.04104.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What year was RoBERTa: A Robustly Optimized BERT Pretraining Approach released?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "What are the key topics in the ysymyth/ReAct repository?",
    "output": "The ysymyth/ReAct repository covers: decision-making, large-language-models, llm, prompting, reasoning."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 repository have?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is available at https://github.com/zai-org/ImageReward (zai-org/ImageReward)."
  },
  {
    "input": "What language is the AlibabaResearch/DAMO-ConvAI repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What paper has ArXiv ID 2302.03011?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10"
  },
  {
    "input": "What is Know What You Don't Know: Unanswerable Questions for SQuAD about?",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "How popular is the Horovod: fast and easy distributed deep learning in TensorFlow implementation?",
    "output": "The horovod/horovod repository has 14,642 stars and 2,257 forks on GitHub."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.22708?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10"
  },
  {
    "input": "Describe the allenai/longformer repository that implements Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer"
  },
  {
    "input": "When was the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper published?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena paper was published in 2023."
  },
  {
    "input": "When was the MRKL Systems: A modular approach to AGI paper published?",
    "output": "The MRKL Systems: A modular approach to AGI paper was published in 2022."
  },
  {
    "input": "How many stars does the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks repository have?",
    "output": "The huggingface/sentence-transformers repository has 17,978 stars and 2,715 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the MetaGPT: Meta Programming for Multi-Agent Systems paper.",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 60,328 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1909.10351?",
    "output": "DistilBERT: a distilled version of BERT"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at GXimingLu/Quark It is written in Python. The repository has 75 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Longformer: The Long-Document Transformer.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,176 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "What is the GitHub repository for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 repository have?",
    "output": "The ruiqi-zhong/D5 repository has 71 stars and 13 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2310.03744.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning"
  },
  {
    "input": "What does the microsoft/JARVIS repository do?",
    "output": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf"
  },
  {
    "input": "What year was Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning released?",
    "output": "The Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning paper was published in 2018."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is in the Distributed, Parallel, and Cluster Computing (cs.DC) category."
  },
  {
    "input": "When was the SQuAD: 100,000+ Questions for Machine Comprehension of Text paper published?",
    "output": "The SQuAD: 100,000+ Questions for Machine Comprehension of Text paper was published in 2016."
  },
  {
    "input": "Describe the LTH14/fractalgen repository that implements arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10.",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What topics does the Reformer: The Efficient Transformer implementation cover?",
    "output": "The google/trax repository covers: jax, numpy, deep-learning, deep-reinforcement-learning, machine-learning, transformer, reinforcement-learning."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at tomekkorbak/pretraining-with-human-feedback which code accompanying the paper pretraining language models with human preferences It is written in Python. The repository has 180 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Universal Language Model Fine-tuning for Text Classification?",
    "output": "The code for Universal Language Model Fine-tuning for Text Classification is available at https://github.com/fastai/fastai (fastai/fastai)."
  },
  {
    "input": "Explain the FMInference/FlexLLMGen implementation of arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10.",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Summarize the MRKL Systems: A modular approach to AGI paper.",
    "output": "MRKL Systems: A modular approach to AGI is a 2022 paper in Artificial Intelligence. The implementation is available at langchain-ai/langchain which \ud83e\udd9c\ud83d\udd17 the platform for reliable agents. It is written in Python. The repository has 121,514 stars, indicating high community interest."
  },
  {
    "input": "Which field does MRKL Systems: A modular approach to AGI belong to?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "Where can I find the code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "What does the Open-Reasoner-Zero/Open-Reasoner-Zero repository do?",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What is the GitHub repository for Longformer: The Long-Document Transformer?",
    "output": "The code for Longformer: The Long-Document Transformer is available at https://github.com/allenai/longformer (allenai/longformer)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at nyu-mll/quality It is written in Python. The repository has 144 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2307.09288?",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What programming language is Question and Answer Test-Train Overlap in Open-Domain QA Datasets implemented in?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 repository have?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "Summarize the Densely Connected Convolutional Networks paper.",
    "output": "Densely Connected Convolutional Networks is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at liuzhuang13/DenseNet which densely connected convolutional networks, in cvpr 2017 (best paper award). It is written in Lua. The repository has 4,848 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Dense Passage Retrieval for Open-Domain Question Answering implemented in?",
    "output": "The Dense Passage Retrieval for Open-Domain Question Answering implementation is written in Python."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Explain the PetarV-/GAT implementation of Graph Attention Networks.",
    "output": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)"
  },
  {
    "input": "When was the Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper published?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks paper was published in 2019."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at lamini-ai/Lamini-Memory-Tuning which banishing llm hallucinations requires rethinking generalization The repository has 275 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of RoFormer: Enhanced Transformer with Rotary Position Embedding?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of BERT: Pre-training of Deep Bidirectional Transformers.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/bert which tensorflow code and pre-trained models for bert It is written in Python. The repository has 39,724 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for QLoRA: Efficient Finetuning of Quantized LLMs?",
    "output": "The code for QLoRA: Efficient Finetuning of Quantized LLMs is available at https://github.com/artidoro/qlora (artidoro/qlora)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is available at https://github.com/AlibabaResearch/DAMO-ConvAI (AlibabaResearch/DAMO-ConvAI)."
  },
  {
    "input": "Where can I find the code for XLNet: Generalized Autoregressive Pretraining?",
    "output": "The code for XLNet: Generalized Autoregressive Pretraining is available at https://github.com/zihangdai/xlnet (zihangdai/xlnet)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00303?",
    "output": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10"
  },
  {
    "input": "How popular is the LoRA: Low-Rank Adaptation of Large Language Models implementation?",
    "output": "The microsoft/LoRA repository has 13,038 stars and 866 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of GPT-3: Language Models are Few-Shot Learners?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.04104?",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10"
  },
  {
    "input": "What language is the Dao-AILab/flash-attention repository written in?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2004.03270?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model"
  },
  {
    "input": "What does the IDEA-Research/GroundingDINO repository do?",
    "output": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\""
  },
  {
    "input": "Which field does GPT-3: Language Models are Few-Shot Learners belong to?",
    "output": "GPT-3: Language Models are Few-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the MMLU: Measuring Massive Multitask Language Understanding paper.",
    "output": "MMLU: Measuring Massive Multitask Language Understanding is a 2021 paper in cs.CY. The implementation is available at hendrycks/test which measuring massive multitask language understanding | iclr 2021 It is written in Python. The repository has 1,528 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of LLaVA: Large Language and Vision Assistant.",
    "output": "LLaVA: Large Language and Vision Assistant is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1706.03059?",
    "output": "Depthwise Separable Convolutions for Neural Machine Translation"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2205.05131?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10"
  },
  {
    "input": "Which field does DreamFusion: Text-to-3D using 2D Diffusion belong to?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Give me an overview of StarCoder: May the source be with you!.",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,478 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Describe the RLHFlow/Online-RLHF repository that implements arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10.",
    "output": "A recipe for online RLHF and online iterative DPO."
  },
  {
    "input": "What programming language is FlashAttention: Fast and Memory-Efficient Exact Attention implemented in?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Densely Connected Convolutional Networks?",
    "output": "The code for Densely Connected Convolutional Networks is available at https://github.com/liuzhuang13/DenseNet (liuzhuang13/DenseNet)."
  },
  {
    "input": "What language is the PetarV-/GAT repository written in?",
    "output": "The Graph Attention Networks implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2101.04909.",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.06865?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "Explain the GAIR-NLP/LIMO implementation of arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10.",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Which field does GPT-4 Technical Report belong to?",
    "output": "GPT-4 Technical Report is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is available at https://github.com/RLHFlow/Directional-Preference-Alignment (RLHFlow/Directional-Preference-Alignment)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2410.18050.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at plurai-ai/intellagent which a framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions It is written in Python. The repository has 1,154 stars, indicating high community interest."
  },
  {
    "input": "In what year did PaLM: Scaling Language Modeling with Pathways come out?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "What is CLIP: Learning Transferable Visual Models From Natural Language about?",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,905 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the DeBERTa: Decoding-enhanced BERT with Disentangled Attention repository have?",
    "output": "The microsoft/DeBERTa repository has 2,177 stars and 241 forks on GitHub."
  },
  {
    "input": "Explain the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper.",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "Explain the Beir: A Heterogeneous Benchmark for IR paper.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,014 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 2501.19393?",
    "output": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10"
  },
  {
    "input": "What does the MAXNORM8650/MedAgentSim repository do?",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "In what year did The Power of Scale for Parameter-Efficient Prompt Tuning come out?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning paper was published in 2021."
  },
  {
    "input": "Summarize the PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper.",
    "output": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is a 2022 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Denoising Diffusion Probabilistic Models repository have?",
    "output": "The hojonathanho/diffusion repository has 4,906 stars and 461 forks on GitHub."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "What's the implementation repository for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena?",
    "output": "The code for Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena is available at https://github.com/lm-sys/FastChat (lm-sys/FastChat)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "How popular is the MiniGPT-4: Enhancing Vision-Language Understanding implementation?",
    "output": "The Vision-CAIR/MiniGPT-4 repository has 25,762 stars and 2,929 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2305.14314?",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2403.17887?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10"
  },
  {
    "input": "Tell me the title of ArXiv paper 2203.07814.",
    "output": "Competition-Level Code Generation with AlphaCode"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2102.02779?",
    "output": "ViT: An Image is Worth 16x16 Words"
  },
  {
    "input": "In what year did EfficientDet: Scalable and Efficient Object Detection come out?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "What programming language is Vicuna: An Open-Source Chatbot implemented in?",
    "output": "The Vicuna: An Open-Source Chatbot implementation is written in Python."
  },
  {
    "input": "Give me an overview of ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,724 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Feature Pyramid Networks for Object Detection.",
    "output": "Feature Pyramid Networks for Object Detection is a 2016 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/Detectron which fair's research platform for object detection research, implementing popular algorithms like mask r-cnn and retinanet. It is written in Python. The repository has 26,396 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation?",
    "output": "The OpenBMB/Eurus repository has 320 stars and 14 forks on GitHub."
  },
  {
    "input": "Where can I find the code for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "In what year did Reformer: The Efficient Transformer come out?",
    "output": "The Reformer: The Efficient Transformer paper was published in 2020."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 2502.09992?",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2302.08242?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.16084?",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is AudioPaLM: A Large Language Model for Speech Understanding about?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at lucidrains/audiolm-pytorch which implementation of audiolm, a sota language modeling approach to audio generation out of google research, in pytorch It is written in Python. The repository has 2,610 stars, indicating high community interest."
  },
  {
    "input": "Describe the horovod/horovod repository that implements Horovod: fast and easy distributed deep learning in TensorFlow.",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10 implementation?",
    "output": "The allenai/RL4LMs repository has 2,372 stars and 203 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is available at https://github.com/MoonshotAI/Kimi-Audio (MoonshotAI/Kimi-Audio)."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 implementation cover?",
    "output": "The NVlabs/prismer repository covers: image-captioning, language-model, multi-modal-learning, multi-task-learning, vision-language-model, vision-and-language, vqa."
  },
  {
    "input": "What's the implementation repository for Feature Pyramid Networks for Object Detection?",
    "output": "The code for Feature Pyramid Networks for Object Detection is available at https://github.com/facebookresearch/Detectron (facebookresearch/Detectron)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Zephyr: Direct Distillation of LM Alignment belong to?",
    "output": "Zephyr: Direct Distillation of LM Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the Sequence to Sequence Learning with Neural Networks repository have?",
    "output": "The farizrahman4u/seq2seq repository has 3,175 stars and 837 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "Which field does Analyzing the Structure of Attention in a Transformer Language Model belong to?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What paper has ArXiv ID 2305.06161?",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "How many stars does the EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification repository have?",
    "output": "The jasonwei20/eda_nlp repository has 1,649 stars and 317 forks on GitHub."
  },
  {
    "input": "What programming language is Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implemented in?",
    "output": "The Swin Transformer: Hierarchical Vision Transformer using Shifted Windows implementation is written in Python."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is available at https://github.com/facebookresearch/ConvNeXt-V2 (facebookresearch/ConvNeXt-V2)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities?",
    "output": "The code for SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities is available at https://github.com/0nutation/SpeechGPT (0nutation/SpeechGPT)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What topics does the Linformer: Self-Attention with Linear Complexity implementation cover?",
    "output": "The tatp22/linformer-pytorch repository covers: artificial-intelligence, deep-learning, attention-mechanism, pytorch, machine-learning, linformer, paper."
  },
  {
    "input": "When was the Enriching Word Vectors with Subword Information paper published?",
    "output": "The Enriching Word Vectors with Subword Information paper was published in 2016."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "Which field does ReAct: Synergizing Reasoning and Acting in Language Models belong to?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 implementation?",
    "output": "The FMInference/FlexLLMGen repository has 9,381 stars and 588 forks on GitHub."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How many stars does the Playing Atari with Deep Reinforcement Learning repository have?",
    "output": "The google-deepmind/dqn repository has 618 stars and 165 forks on GitHub."
  },
  {
    "input": "How popular is the Wide & Deep Learning for Recommender Systems implementation?",
    "output": "The tensorflow/models repository has 77,680 stars and 45,382 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is StarCoder: May the source be with you! about?",
    "output": "StarCoder: May the source be with you! is a 2023 paper in Machine Learning. The implementation is available at bigcode-project/starcoder which home of starcoder: fine-tuning & inference! It is written in Python. The repository has 7,478 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning?",
    "output": "The code for Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning is available at https://github.com/rail-berkeley/softlearning (rail-berkeley/softlearning)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Summarize the Multi-Task Deep Neural Networks for Natural Language Understanding paper.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at namisan/mt-dnn which multi-task deep neural networks for natural language understanding It is written in Python. The repository has 2,258 stars, indicating high community interest."
  },
  {
    "input": "What does the FMInference/FlexLLMGen repository do?",
    "output": "Running large language models on a single GPU for throughput-oriented scenarios."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of BERT: Pre-training of Deep Bidirectional Transformers?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the hexiangnan/neural_collaborative_filtering implementation of Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "Tell me the title of ArXiv paper 2209.14375.",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10"
  },
  {
    "input": "Give me an overview of S4: Efficiently Modeling Long Sequences.",
    "output": "S4: Efficiently Modeling Long Sequences is a 2021 paper in Machine Learning. The implementation is available at state-spaces/s4 which structured state space sequence models It is written in Jupyter Notebook. The repository has 2,794 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "Where can I find the code for PaLM: Scaling Language Modeling with Pathways?",
    "output": "The code for PaLM: Scaling Language Modeling with Pathways is available at https://github.com/lucidrains/PaLM-pytorch (lucidrains/PaLM-pytorch)."
  },
  {
    "input": "What year was PaLM: Scaling Language Modeling with Pathways released?",
    "output": "The PaLM: Scaling Language Modeling with Pathways paper was published in 2022."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 repository have?",
    "output": "The google-deepmind/tracr repository has 548 stars and 47 forks on GitHub."
  },
  {
    "input": "What does the mit-han-lab/smoothquant repository do?",
    "output": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Describe the jacopotagliabue/you-dont-need-a-bigger-boat repository that implements arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10.",
    "output": "An end-to-end implementation of intent prediction with Metaflow and other cool tools"
  },
  {
    "input": "Describe the google-research/electra repository that implements ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "What language is the lm-sys/FastChat repository written in?",
    "output": "The Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2103.00020.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language"
  },
  {
    "input": "Tell me the title of ArXiv paper 2405.02803.",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10"
  },
  {
    "input": "When was the BERT Rediscovers the Classical NLP Pipeline paper published?",
    "output": "The BERT Rediscovers the Classical NLP Pipeline paper was published in 2019."
  },
  {
    "input": "What language is the ruiqi-zhong/D5 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation?",
    "output": "The 0nutation/SpeechGPT repository has 1,396 stars and 95 forks on GitHub."
  },
  {
    "input": "What is Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context about?",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,680 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2312.00752?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces"
  },
  {
    "input": "What does the anthropics/hh-rlhf repository do?",
    "output": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\""
  },
  {
    "input": "Give me an overview of Faster R-CNN: Towards Real-Time Object Detection.",
    "output": "Faster R-CNN: Towards Real-Time Object Detection is a 2015 paper in Computer Vision and Pattern Recognition. The implementation is available at rbgirshick/py-faster-rcnn which faster r-cnn (python implementation) -- see https://github.com/shaoqingren/faster_rcnn for the official matlab version It is written in Python. The repository has 8,278 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2111.03026?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2303.11331?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training"
  },
  {
    "input": "How many stars does the ControlNet: Adding Conditional Control to Text-to-Image Diffusion repository have?",
    "output": "The lllyasviel/ControlNet repository has 33,415 stars and 2,992 forks on GitHub."
  },
  {
    "input": "What are the key topics in the rajpurkar/SQuAD-explorer repository?",
    "output": "The rajpurkar/SQuAD-explorer repository covers: dataset, visual-analysis, leaderboard."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the ContextualAI/HALOs implementation of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What year was BLIP-2: Bootstrapping Language-Image Pre-training released?",
    "output": "The BLIP-2: Bootstrapping Language-Image Pre-training paper was published in 2023."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Kwai-YuanQi/MM-RLHF which the next step forward in multimodal llm alignment It is written in Python. The repository has 189 stars, indicating high community interest."
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at LTH14/fractalgen which pytorch implementation of fractalgen https://arxiv.org/abs/2502.17437 It is written in Python. The repository has 1,206 stars, indicating high community interest."
  },
  {
    "input": "What does the HazyResearch/safari repository do?",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "What are the key topics in the ContextualAI/HALOs repository?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "What does the google-research/electra repository do?",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
  },
  {
    "input": "Describe the TsinghuaAI/CPM-1-Generate repository that implements arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10.",
    "output": "Chinese Pre-Trained Language Models (CPM-LM) Version-I"
  },
  {
    "input": "What category is the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper in?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the GitHub repository for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "What language is the simplescaling/s1 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "How many stars does the GPT-4 Technical Report repository have?",
    "output": "The openai/evals repository has 17,406 stars and 2,848 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is available at https://github.com/uclaml/SPIN (uclaml/SPIN)."
  },
  {
    "input": "What is the ArXiv category of MRKL Systems: A modular approach to AGI?",
    "output": "MRKL Systems: A modular approach to AGI is in the Artificial Intelligence (cs.AI) category."
  },
  {
    "input": "What year was Semi-Supervised Classification with Graph Convolutional Networks released?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 is available at https://github.com/allenai/codescientist (allenai/codescientist)."
  },
  {
    "input": "What programming language is RAG: Retrieval-Augmented Generation implemented in?",
    "output": "The RAG: Retrieval-Augmented Generation implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/ConvNeXt-V2 repository has 1,917 stars and 157 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.11147?",
    "output": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for Sequence to Sequence Learning with Neural Networks?",
    "output": "The code for Sequence to Sequence Learning with Neural Networks is available at https://github.com/farizrahman4u/seq2seq (farizrahman4u/seq2seq)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "Summarize the EfficientDet: Scalable and Efficient Object Detection paper.",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,430 stars, indicating high community interest."
  },
  {
    "input": "What language is the salesforce/GeDi repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the GXimingLu/Quark repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for Very Deep Convolutional Networks for Large-Scale Image Recognition?",
    "output": "The code for Very Deep Convolutional Networks for Large-Scale Image Recognition is available at https://github.com/machrisaa/tensorflow-vgg (machrisaa/tensorflow-vgg)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion?",
    "output": "The code for ControlNet: Adding Conditional Control to Text-to-Image Diffusion is available at https://github.com/lllyasviel/ControlNet (lllyasviel/ControlNet)."
  },
  {
    "input": "Tell me the title of ArXiv paper 1801.01290.",
    "output": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Attention Is All You Need paper published?",
    "output": "The Attention Is All You Need paper was published in 2017."
  },
  {
    "input": "Describe the hpcaitech/Open-Sora repository that implements arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10.",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the Longformer: The Long-Document Transformer implementation?",
    "output": "The allenai/longformer repository has 2,176 stars and 288 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 1810.04805?",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "Give me an overview of Masked Autoencoders Are Scalable Vision Learners.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,127 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.03846.",
    "output": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10"
  },
  {
    "input": "Describe the tensorflow/models repository that implements Wide & Deep Learning for Recommender Systems.",
    "output": "Models and examples built with TensorFlow"
  },
  {
    "input": "Describe the OpenGVLab/LLaMA-Adapter repository that implements LLaMA-Adapter: Efficient Fine-tuning of Language Models.",
    "output": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at AlibabaResearch/DAMO-ConvAI which damo-convai: the official repository which contains the codebase for alibaba damo conversational ai. It is written in Python. The repository has 1,508 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What is Self-RAG: Learning to Retrieve, Generate, and Critique about?",
    "output": "Self-RAG: Learning to Retrieve, Generate, and Critique is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at AkariAsai/self-rag which this includes the original implementation of self-rag: learning to retrieve, generate and critique through self-reflection by akari asai, zeqiu wu, yizhong wang, avirup sil, and hannaneh hajishirzi. It is written in Python. The repository has 2,262 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/TransCoder repository has 1,724 stars and 263 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ali-vilab/composer which official implementation of \"composer: creative and controllable image synthesis with composable conditions\" The repository has 1,560 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was MiniGPT-4: Enhancing Vision-Language Understanding released?",
    "output": "The MiniGPT-4: Enhancing Vision-Language Understanding paper was published in 2023."
  },
  {
    "input": "What is DreamFusion: Text-to-3D using 2D Diffusion about?",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,785 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-research/bert repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does Enriching Word Vectors with Subword Information belong to?",
    "output": "Enriching Word Vectors with Subword Information is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 repository have?",
    "output": "The NVlabs/prismer repository has 1,307 stars and 74 forks on GitHub."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 repository have?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Tell me the title of ArXiv paper 2205.05131.",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10 is available at https://github.com/hpcaitech/Open-Sora (hpcaitech/Open-Sora)."
  },
  {
    "input": "Explain the NVlabs/prismer implementation of arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10.",
    "output": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\"."
  },
  {
    "input": "Where can I find the code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction?",
    "output": "The code for HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "In what year did Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech come out?",
    "output": "The Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech paper was published in 2021."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "When was the FLAN: Finetuned Language Models Are Zero-Shot Learners paper published?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "Tell me the title of ArXiv paper 2501.03262.",
    "output": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at sentient-agi/OpenDeepSearch which sota search powered llm It is written in Python. The repository has 3,735 stars, indicating high community interest."
  },
  {
    "input": "What category is the Finetuned Language Models Are Zero-Shot Learners paper in?",
    "output": "Finetuned Language Models Are Zero-Shot Learners is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the facebookresearch/Detectron implementation of Feature Pyramid Networks for Object Detection.",
    "output": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was Deep Graph Library: A Graph-Centric, Highly-Performant Package released?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package paper was published in 2020."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,341 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What's the implementation repository for Semi-Supervised Classification with Graph Convolutional Networks?",
    "output": "The code for Semi-Supervised Classification with Graph Convolutional Networks is available at https://github.com/tkipf/gcn (tkipf/gcn)."
  },
  {
    "input": "What language is the mit-han-lab/smoothquant repository written in?",
    "output": "The SmoothQuant: Accurate and Efficient Post-Training Quantization implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1409.0473?",
    "output": "Sequence to Sequence Learning with Neural Networks"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 is available at https://github.com/google-deepmind/tracr (google-deepmind/tracr)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2302.08582?",
    "output": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10"
  },
  {
    "input": "How popular is the MMLU: Measuring Massive Multitask Language Understanding implementation?",
    "output": "The hendrycks/test repository has 1,528 stars and 112 forks on GitHub."
  },
  {
    "input": "Explain the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper.",
    "output": "PyTorch: An Imperative Style, High-Performance Deep Learning Library is a 2019 paper in Machine Learning. The implementation is available at pytorch/pytorch which tensors and dynamic neural networks in python with strong gpu acceleration It is written in Python. The repository has 95,727 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2306.00978?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10"
  },
  {
    "input": "What is the GitHub repository for BLIP: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/BLIP (salesforce/BLIP)."
  },
  {
    "input": "What does the hendrycks/test repository do?",
    "output": "Measuring Massive Multitask Language Understanding | ICLR 2021"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the SQuAD: 100,000+ Questions for Machine Comprehension of Text implementation?",
    "output": "The rajpurkar/SQuAD-explorer repository has 569 stars and 121 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "What does the danijar/dreamer repository do?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at openai/frontier-evals which openai frontier evals It is written in Python. The repository has 958 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper was published in 2022."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the FoundationAgents/MetaGPT repository that implements MetaGPT: Meta Programming for Multi-Agent Systems.",
    "output": "\ud83c\udf1f The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming"
  },
  {
    "input": "What topics does the XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation cover?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Explain the ShishirPatil/gorilla implementation of Gorilla: Large Language Model Connected with Massive APIs.",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What language is the jasonwei20/eda_nlp repository written in?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation is written in Python."
  },
  {
    "input": "How popular is the Self-RAG: Learning to Retrieve, Generate, and Critique implementation?",
    "output": "The AkariAsai/self-rag repository has 2,262 stars and 214 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at salesforce/booksum It is written in Python. The repository has 195 stars, indicating high community interest."
  },
  {
    "input": "Explain the google-research/google-research implementation of Synthetic Data Generation with Large Language Models.",
    "output": "Google Research"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is available at https://github.com/TsinghuaAI/CPM-1-Generate (TsinghuaAI/CPM-1-Generate)."
  },
  {
    "input": "Explain the XiangLi1999/PrefixTuning implementation of Prefix-Tuning: Optimizing Continuous Prompts for Generation.",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at linkedin/gdmix which a deep ranking personalization framework It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of DeBERTa: Decoding-enhanced BERT with Disentangled Attention?",
    "output": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1506.02640?",
    "output": "Faster R-CNN: Towards Real-Time Object Detection"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is a 2024 paper in Computation and Language (Natural Language Processing). The implementation is available at deepseek-ai/DeepSeek-Coder-V2 which deepseek-coder-v2: breaking the barrier of closed-source models in code intelligence The repository has 6,281 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.03409?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding"
  },
  {
    "input": "What does the GanjinZero/RRHF repository do?",
    "output": "[NIPS2023] RRHF & Wombat"
  },
  {
    "input": "What is FLAN: Finetuned Language Models Are Zero-Shot Learners about?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/FLAN It is written in Python. The repository has 1,556 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was FlashAttention: Fast and Memory-Efficient Exact Attention released?",
    "output": "The FlashAttention: Fast and Memory-Efficient Exact Attention paper was published in 2022."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning?",
    "output": "The code for AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is available at https://github.com/QingruZhang/AdaLoRA (QingruZhang/AdaLoRA)."
  },
  {
    "input": "Which field does Dream to Control: Learning Behaviors by Latent Imagination belong to?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the google-research/prompt-tuning repository do?",
    "output": "Original Implementation of Prompt Tuning from Lester, et al, 2021"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2309.16588?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "In what year did P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning come out?",
    "output": "The P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning paper was published in 2021."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 implementation?",
    "output": "The HazyResearch/safari repository has 906 stars and 70 forks on GitHub."
  },
  {
    "input": "What is Whisper: Robust Speech Recognition via Large-Scale Weak Supervision about?",
    "output": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/whisper which robust speech recognition via large-scale weak supervision It is written in Python. The repository has 91,793 stars, indicating high community interest."
  },
  {
    "input": "What language is the salesforce/BLIP repository written in?",
    "output": "The BLIP: Bootstrapping Language-Image Pre-training implementation is written in Jupyter Notebook."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "In what year did Semi-Supervised Classification with Graph Convolutional Networks come out?",
    "output": "The Semi-Supervised Classification with Graph Convolutional Networks paper was published in 2016."
  },
  {
    "input": "Describe the huggingface/transformers repository that implements DistilBERT: a distilled version of BERT.",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "What does the openai/evals repository do?",
    "output": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10 is available at https://github.com/shizhediao/active-prompt (shizhediao/active-prompt)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10 repository have?",
    "output": "The haoliuhl/language-quantized-autoencoders repository has 111 stars and 5 forks on GitHub."
  },
  {
    "input": "In what year did PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel come out?",
    "output": "The PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel paper was published in 2022."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1906.08237?",
    "output": "XLNet: Generalized Autoregressive Pretraining"
  },
  {
    "input": "How many stars does the Grounding DINO: Marrying DINO with Grounded Pre-Training repository have?",
    "output": "The IDEA-Research/GroundingDINO repository has 9,401 stars and 976 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "Explain the HazyResearch/safari implementation of arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10.",
    "output": "Convolutions for Sequence Modeling"
  },
  {
    "input": "Give me an overview of GPT-3: Language Models are Few-Shot Learners.",
    "output": "GPT-3: Language Models are Few-Shot Learners is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/gpt-3 which gpt-3: language models are few-shot learners The repository has 15,774 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2209.14375?",
    "output": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Online-RLHF which a recipe for online rlhf and online iterative dpo. It is written in Python. The repository has 537 stars, indicating high community interest."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "How popular is the SimCSE: Simple Contrastive Learning of Sentence Embeddings implementation?",
    "output": "The princeton-nlp/SimCSE repository has 3,623 stars and 533 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is available at https://github.com/deepseek-ai/DeepSeek-Coder-V2 (deepseek-ai/DeepSeek-Coder-V2)."
  },
  {
    "input": "Where can I find the code for Wide & Deep Learning for Recommender Systems?",
    "output": "The code for Wide & Deep Learning for Recommender Systems is available at https://github.com/tensorflow/models (tensorflow/models)."
  },
  {
    "input": "What category is the PaLM: Scaling Language Modeling with Pathways paper in?",
    "output": "PaLM: Scaling Language Modeling with Pathways is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the bowang-lab/MedSAM repository do?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "How many stars does the PyTorch Geometric: Library for Geometric Deep Learning repository have?",
    "output": "The pyg-team/pytorch_geometric repository has 23,255 stars and 3,928 forks on GitHub."
  },
  {
    "input": "What programming language is Neural Collaborative Filtering implemented in?",
    "output": "The Neural Collaborative Filtering implementation is written in Python."
  },
  {
    "input": "Explain the meta-llama/llama implementation of Llama 2: Open Foundation and Fine-Tuned Chat Models.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What does the hpcaitech/Open-Sora repository do?",
    "output": "Open-Sora: Democratizing Efficient Video Production for All"
  },
  {
    "input": "Where can I find the code for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at turboderp-org/exllamav2 which a fast inference library for running llms locally on modern consumer-class gpus It is written in Python. The repository has 4,380 stars, indicating high community interest."
  },
  {
    "input": "When was the PyTorch Geometric: Library for Geometric Deep Learning paper published?",
    "output": "The PyTorch Geometric: Library for Geometric Deep Learning paper was published in 2019."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What does the GAIR-NLP/LIMO repository do?",
    "output": "[COLM 2025] LIMO: Less is More for Reasoning"
  },
  {
    "input": "Give me an overview of SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding.",
    "output": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at nyu-mll/jiant which jiant is an nlp toolkit It is written in Python. The repository has 1,674 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Mamba: Linear-Time Sequence Modeling with Selective State Spaces?",
    "output": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for InPars: Data Augmentation for Information Retrieval?",
    "output": "The code for InPars: Data Augmentation for Information Retrieval is available at https://github.com/zetaalphavector/InPars (zetaalphavector/InPars)."
  },
  {
    "input": "In what year did Analyzing the Structure of Attention in a Transformer Language Model come out?",
    "output": "The Analyzing the Structure of Attention in a Transformer Language Model paper was published in 2020."
  },
  {
    "input": "Describe the volcengine/verl repository that implements arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10.",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Which field does DreamBooth: Fine Tuning Text-to-Image Diffusion Models belong to?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 implementation?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 266 stars and 28 forks on GitHub."
  },
  {
    "input": "How many stars does the PyTorch: An Imperative Style, High-Performance Deep Learning Library repository have?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 1810.04805.",
    "output": "BERT: Pre-training of Deep Bidirectional Transformers"
  },
  {
    "input": "Explain the Graph Attention Networks paper.",
    "output": "Graph Attention Networks is a 2017 paper in Machine Learning. The implementation is available at PetarV-/GAT which graph attention networks (https://arxiv.org/abs/1710.10903) It is written in Python. The repository has 3,469 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at AlibabaResearch/DAMO-ConvAI which damo-convai: the official repository which contains the codebase for alibaba damo conversational ai. It is written in Python. The repository has 1,508 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "When was the RAG: Retrieval-Augmented Generation paper published?",
    "output": "The RAG: Retrieval-Augmented Generation paper was published in 2020."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation cover?",
    "output": "The plurai-ai/intellagent repository covers: agent, evaluation, llmops, simulator, synthetic-data."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the meta-llama/llama repository that implements LIMA: Less Is More for Alignment.",
    "output": "Inference code for Llama models"
  },
  {
    "input": "What category is the T5: Exploring the Limits of Transfer Learning paper in?",
    "output": "T5: Exploring the Limits of Transfer Learning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Prefix-Tuning: Optimizing Continuous Prompts for Generation implemented in?",
    "output": "The Prefix-Tuning: Optimizing Continuous Prompts for Generation implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Diffusion Models Beat GANs on Image Synthesis?",
    "output": "The code for Diffusion Models Beat GANs on Image Synthesis is available at https://github.com/openai/guided-diffusion (openai/guided-diffusion)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Mistral 7B implementation?",
    "output": "The mistralai/mistral-inference repository has 10,577 stars and 994 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 repository have?",
    "output": "The ZHZisZZ/modpo repository has 93 stars and 7 forks on GitHub."
  },
  {
    "input": "What topics does the AudioPaLM: A Large Language Model for Speech Understanding implementation cover?",
    "output": "The lucidrains/audiolm-pytorch repository covers: artificial-intelligence, attention-mechanisms, audio-synthesis, deep-learning, transformers."
  },
  {
    "input": "Which field does MiniGPT-4: Enhancing Vision-Language Understanding belong to?",
    "output": "MiniGPT-4: Enhancing Vision-Language Understanding is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2309.16588.",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is available at https://github.com/MAXNORM8650/MedAgentSim (MAXNORM8650/MedAgentSim)."
  },
  {
    "input": "What paper has ArXiv ID 2201.12086?",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "What paper has ArXiv ID 2302.14838?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10"
  },
  {
    "input": "Which field does The Flan Collection: Designing Data and Methods for Effective Instruction Tuning belong to?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does Universal Language Model Fine-tuning for Text Classification belong to?",
    "output": "Universal Language Model Fine-tuning for Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2106.05091.",
    "output": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is TensorFlow: A System for Large-Scale Machine Learning about?",
    "output": "TensorFlow: A System for Large-Scale Machine Learning is a 2016 paper in Distributed, Parallel, and Cluster Computing. The implementation is available at tensorflow/tensorflow which an open source machine learning framework for everyone It is written in C++. The repository has 192,725 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for T5: Exploring the Limits of Transfer Learning?",
    "output": "The code for T5: Exploring the Limits of Transfer Learning is available at https://github.com/google-research/text-to-text-transfer-transformer (google-research/text-to-text-transfer-transformer)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the BERT Rediscovers the Classical NLP Pipeline paper.",
    "output": "BERT Rediscovers the Classical NLP Pipeline is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "What does the ShishirPatil/gorilla repository do?",
    "output": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)"
  },
  {
    "input": "Describe the JohnNay/llm-lobbyist repository that implements arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10.",
    "output": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at RLHFlow/Directional-Preference-Alignment which directional preference alignment The repository has 58 stars, indicating high community interest."
  },
  {
    "input": "What is Synthetic Data Generation with Large Language Models about?",
    "output": "Synthetic Data Generation with Large Language Models is a 2023 paper in Machine Learning. The implementation is available at google-research/google-research which google research It is written in Jupyter Notebook. The repository has 36,869 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/ConstitutionalHarmlessnessPaper The repository has 249 stars, indicating high community interest."
  },
  {
    "input": "When was the S4: Efficiently Modeling Long Sequences paper published?",
    "output": "The S4: Efficiently Modeling Long Sequences paper was published in 2021."
  },
  {
    "input": "Where can I find the code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows?",
    "output": "The code for Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is available at https://github.com/microsoft/Swin-Transformer (microsoft/Swin-Transformer)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 is available at https://github.com/GanjinZero/RRHF (GanjinZero/RRHF)."
  },
  {
    "input": "What is the ArXiv category of Contriever: Unsupervised Dense Information Retrieval?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Which field does Animate Anyone: Consistent and Controllable Image-to-Video Synthesis belong to?",
    "output": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "In what year did Vicuna: An Open-Source Chatbot come out?",
    "output": "The Vicuna: An Open-Source Chatbot paper was published in 2023."
  },
  {
    "input": "What language is the HazyResearch/safari repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10 implementation is written in Assembly."
  },
  {
    "input": "In what year did CLIP: Learning Transferable Visual Models From Natural Language come out?",
    "output": "The CLIP: Learning Transferable Visual Models From Natural Language paper was published in 2021."
  },
  {
    "input": "What is EfficientDet: Scalable and Efficient Object Detection about?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google/automl which google brain automl It is written in Jupyter Notebook. The repository has 6,430 stars, indicating high community interest."
  },
  {
    "input": "Which field does Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech belong to?",
    "output": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the 0nutation/SpeechGPT implementation of SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities.",
    "output": "SpeechGPT Series: Speech Large Language Models"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the Swin Transformer: Hierarchical Vision Transformer using Shifted Windows paper.",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,517 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 is available at https://github.com/facebookresearch/fairseq (facebookresearch/fairseq)."
  },
  {
    "input": "In what year did LLaVA-1.5: Improved Baselines with Visual Instruction Tuning come out?",
    "output": "The LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper was published in 2023."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10 implementation cover?",
    "output": "The AlibabaResearch/DAMO-ConvAI repository covers: conversational-ai, deep-learning, natural-language-processing, dialog."
  },
  {
    "input": "How many stars does the XLNet: Generalized Autoregressive Pretraining repository have?",
    "output": "The zihangdai/xlnet repository has 6,181 stars and 1,165 forks on GitHub."
  },
  {
    "input": "How many stars does the Decoupled Weight Decay Regularization repository have?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at chenfei-wu/TaskMatrix It is written in Python. The repository has 34,341 stars, indicating high community interest."
  },
  {
    "input": "Summarize the LLaVA-1.5: Improved Baselines with Visual Instruction Tuning paper.",
    "output": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is a 2023 paper in Computer Vision and Pattern Recognition. The implementation is available at haotian-liu/LLaVA which [neurips'23 oral] visual instruction tuning (llava) built towards gpt-4v level capabilities and beyond. It is written in Python. The repository has 24,140 stars, indicating high community interest."
  },
  {
    "input": "What category is the Code Llama: Open Foundation Models for Code paper in?",
    "output": "Code Llama: Open Foundation Models for Code is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2109.10862.",
    "output": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at HazyResearch/safari which convolutions for sequence modeling It is written in Assembly. The repository has 906 stars, indicating high community interest."
  },
  {
    "input": "What programming language is FlashAttention-2: Faster Attention with Better Parallelism implemented in?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism implementation is written in Python."
  },
  {
    "input": "What does the XiangLi1999/PrefixTuning repository do?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2004.04906.",
    "output": "Dense Passage Retrieval for Open-Domain Question Answering"
  },
  {
    "input": "What's the implementation repository for Self-RAG: Learning to Retrieve, Generate, and Critique?",
    "output": "The code for Self-RAG: Learning to Retrieve, Generate, and Critique is available at https://github.com/AkariAsai/self-rag (AkariAsai/self-rag)."
  },
  {
    "input": "When was the RoFormer: Enhanced Transformer with Rotary Position Embedding paper published?",
    "output": "The RoFormer: Enhanced Transformer with Rotary Position Embedding paper was published in 2021."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "Where can I find the code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning?",
    "output": "The code for LLaVA-1.5: Improved Baselines with Visual Instruction Tuning is available at https://github.com/haotian-liu/LLaVA (haotian-liu/LLaVA)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10 implementation?",
    "output": "The arcee-ai/mergekit repository has 6,564 stars and 644 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2204.14198?",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "Describe the ruiqi-zhong/D5 repository that implements arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "How popular is the Finetuned Language Models Are Zero-Shot Learners implementation?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning belong to?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Synthetic Data Generation with Large Language Models implemented in?",
    "output": "The Synthetic Data Generation with Large Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What does the lamini-ai/Lamini-Memory-Tuning repository do?",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "What's the implementation repository for ELECTRA: Pre-training Text Encoders as Discriminators?",
    "output": "The code for ELECTRA: Pre-training Text Encoders as Discriminators is available at https://github.com/google-research/electra (google-research/electra)."
  },
  {
    "input": "In what year did Linformer: Self-Attention with Linear Complexity come out?",
    "output": "The Linformer: Self-Attention with Linear Complexity paper was published in 2021."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Describe the Dao-AILab/flash-attention repository that implements FlashAttention-2: Faster Attention with Better Parallelism.",
    "output": "Fast and memory-efficient exact attention"
  },
  {
    "input": "What does the OpenRLHF/OpenRLHF repository do?",
    "output": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agentic RL)"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What language is the JohnNay/llm-lobbyist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "Give me an overview of GPT-4 Technical Report.",
    "output": "GPT-4 Technical Report is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at openai/evals which evals is a framework for evaluating llms and llm systems, and an open-source registry of benchmarks. It is written in Python. The repository has 17,406 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Deep Graph Library: A Graph-Centric, Highly-Performant Package implemented in?",
    "output": "The Deep Graph Library: A Graph-Centric, Highly-Performant Package implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation?",
    "output": "The allenai/codescientist repository has 303 stars and 40 forks on GitHub."
  },
  {
    "input": "What is EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification about?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at jasonwei20/eda_nlp which data augmentation for nlp, presented at emnlp 2019 It is written in Python. The repository has 1,649 stars, indicating high community interest."
  },
  {
    "input": "What is Contriever: Unsupervised Dense Information Retrieval about?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 766 stars, indicating high community interest."
  },
  {
    "input": "Describe the rail-berkeley/softlearning repository that implements Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning.",
    "output": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 is available at https://github.com/turboderp-org/exllamav2 (turboderp-org/exllamav2)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10 is available at https://github.com/NVlabs/prismer (NVlabs/prismer)."
  },
  {
    "input": "When was the FlashAttention-2: Faster Attention with Better Parallelism paper published?",
    "output": "The FlashAttention-2: Faster Attention with Better Parallelism paper was published in 2023."
  },
  {
    "input": "Which field does HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction belong to?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for Proximal Policy Optimization Algorithms?",
    "output": "The code for Proximal Policy Optimization Algorithms is available at https://github.com/openai/baselines (openai/baselines)."
  },
  {
    "input": "What does the LTH14/fractalgen repository do?",
    "output": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437"
  },
  {
    "input": "What is the GitHub repository for FLAN: Finetuned Language Models Are Zero-Shot Learners?",
    "output": "The code for FLAN: Finetuned Language Models Are Zero-Shot Learners is available at https://github.com/google-research/FLAN (google-research/FLAN)."
  },
  {
    "input": "What year was Inductive Representation Learning on Large Graphs released?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "Tell me the title of ArXiv paper 2204.14198.",
    "output": "Flamingo: A Visual Language Model for Few-Shot Learning"
  },
  {
    "input": "What category is the Graph Attention Networks paper in?",
    "output": "Graph Attention Networks is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the BEiT: BERT Pre-Training of Image Transformers implementation?",
    "output": "The microsoft/unilm repository has 21,873 stars and 2,681 forks on GitHub."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "Describe the Open-Reasoner-Zero/Open-Reasoner-Zero repository that implements arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What is XLNet: Generalized Autoregressive Pretraining about?",
    "output": "XLNet: Generalized Autoregressive Pretraining is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at zihangdai/xlnet which xlnet: generalized autoregressive pretraining for language understanding It is written in Python. The repository has 6,181 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning.",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,069 stars, indicating high community interest."
  },
  {
    "input": "What programming language is Mamba: Linear-Time Sequence Modeling with Selective State Spaces implemented in?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation is written in Python."
  },
  {
    "input": "Summarize the Know What You Don't Know: Unanswerable Questions for SQuAD paper.",
    "output": "Know What You Don't Know: Unanswerable Questions for SQuAD is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at rajpurkar/SQuAD-explorer which visually explore the stanford question answering dataset It is written in JavaScript. The repository has 569 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at ruixin31/Spurious_Rewards It is written in Python. The repository has 345 stars, indicating high community interest."
  },
  {
    "input": "Explain the ViT: An Image is Worth 16x16 Words paper.",
    "output": "ViT: An Image is Worth 16x16 Words is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of ALBERT: A Lite BERT?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Describe the ContextualAI/HALOs repository that implements arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10.",
    "output": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs)."
  },
  {
    "input": "What paper has ArXiv ID 2405.07863?",
    "output": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10 implementation?",
    "output": "The PKU-Alignment/align-anything repository has 4,601 stars and 506 forks on GitHub."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 implementation?",
    "output": "The hpcaitech/Open-Sora repository has 28,081 stars and 2,804 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2104.08691?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MAXNORM8650/MedAgentSim which medagentsim: self-evolving multi-agent simulations for realistic clinical interactions, miccai 2025 (oral and early accepted) It is written in Python. The repository has 101 stars, indicating high community interest."
  },
  {
    "input": "What is FlashAttention-2: Faster Attention with Better Parallelism about?",
    "output": "FlashAttention-2: Faster Attention with Better Parallelism is a 2023 paper in Machine Learning. The implementation is available at Dao-AILab/flash-attention which fast and memory-efficient exact attention It is written in Python. The repository has 20,993 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1907.11692?",
    "output": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
  },
  {
    "input": "How many stars does the Generating Wikipedia by Summarizing Long Sequences repository have?",
    "output": "The tensorflow/tensor2tensor repository has 16,801 stars and 3,698 forks on GitHub."
  },
  {
    "input": "What does the turboderp-org/exllamav2 repository do?",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "Tell me the title of ArXiv paper 2201.12086.",
    "output": "BLIP: Bootstrapping Language-Image Pre-training"
  },
  {
    "input": "How many stars does the DistilBERT: a distilled version of BERT repository have?",
    "output": "The huggingface/transformers repository has 153,664 stars and 31,358 forks on GitHub."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the GitHub repository for Question and Answer Test-Train Overlap in Open-Domain QA Datasets?",
    "output": "The code for Question and Answer Test-Train Overlap in Open-Domain QA Datasets is available at https://github.com/google-research-datasets/natural-questions (google-research-datasets/natural-questions)."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at allenai/FineGrainedRLHF It is written in Python. The repository has 281 stars, indicating high community interest."
  },
  {
    "input": "Explain the bitsandbytes-foundation/bitsandbytes implementation of LLM.int8(): 8-bit Matrix Multiplication for Transformers.",
    "output": "Accessible large language models via k-bit quantization for PyTorch."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is available at https://github.com/tgxs002/align_sd (tgxs002/align_sd)."
  },
  {
    "input": "What is the ArXiv category of RAG: Retrieval-Augmented Generation?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2406.09308?",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of Segment Anything?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Which field does Neural Machine Translation by Jointly Learning to Align and Translate belong to?",
    "output": "Neural Machine Translation by Jointly Learning to Align and Translate is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2212.03533?",
    "output": "Text Embeddings by Weakly-Supervised Contrastive Pre-training"
  },
  {
    "input": "In what year did SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities come out?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities paper was published in 2023."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Med-PaLM 2: Towards Expert-Level Medical Question Answering?",
    "output": "The code for Med-PaLM 2: Towards Expert-Level Medical Question Answering is available at https://github.com/google-research/google-research (google-research/google-research)."
  },
  {
    "input": "Give me an overview of LIMA: Less Is More for Alignment.",
    "output": "LIMA: Less Is More for Alignment is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What year was The Flan Collection: Designing Data and Methods for Effective Instruction Tuning released?",
    "output": "The The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper was published in 2023."
  },
  {
    "input": "Tell me the title of ArXiv paper 2006.11239.",
    "output": "Denoising Diffusion Probabilistic Models"
  },
  {
    "input": "Explain the RAG: Retrieval-Augmented Generation paper.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at haoliuhl/language-quantized-autoencoders which language quantized autoencoders It is written in Python. The repository has 111 stars, indicating high community interest."
  },
  {
    "input": "What paper has ArXiv ID 1607.01759?",
    "output": "Enriching Word Vectors with Subword Information"
  },
  {
    "input": "What is the GitHub repository for TensorFlow: A System for Large-Scale Machine Learning?",
    "output": "The code for TensorFlow: A System for Large-Scale Machine Learning is available at https://github.com/tensorflow/tensorflow (tensorflow/tensorflow)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does Deep Residual Learning for Image Recognition belong to?",
    "output": "Deep Residual Learning for Image Recognition is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.01848?",
    "output": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10"
  },
  {
    "input": "What programming language is SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implemented in?",
    "output": "The SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities implementation is written in Python."
  },
  {
    "input": "What language is the FoundationAgents/MetaGPT repository written in?",
    "output": "The MetaGPT: Meta Programming for Multi-Agent Systems implementation is written in Python."
  },
  {
    "input": "When was the Very Deep Convolutional Networks for Large-Scale Image Recognition paper published?",
    "output": "The Very Deep Convolutional Networks for Large-Scale Image Recognition paper was published in 2014."
  },
  {
    "input": "When was the PyTorch: An Imperative Style, High-Performance Deep Learning Library paper published?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library paper was published in 2019."
  },
  {
    "input": "Tell me the title of ArXiv paper 2406.09308.",
    "output": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 is available at https://github.com/linkedin/gdmix (linkedin/gdmix)."
  },
  {
    "input": "What programming language is XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implemented in?",
    "output": "The XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Playing Atari with Deep Reinforcement Learning?",
    "output": "The code for Playing Atari with Deep Reinforcement Learning is available at https://github.com/google-deepmind/dqn (google-deepmind/dqn)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2307.09288.",
    "output": "Llama 2: Open Foundation and Fine-Tuned Chat Models"
  },
  {
    "input": "What is Dream to Control: Learning Behaviors by Latent Imagination about?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination is a 2019 paper in Machine Learning. The implementation is available at danijar/dreamer which dream to control: learning behaviors by latent imagination It is written in Python. The repository has 570 stars, indicating high community interest."
  },
  {
    "input": "Explain the bigcode-project/starcoder implementation of StarCoder: May the source be with you!.",
    "output": "Home of StarCoder: fine-tuning & inference!"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10 repository have?",
    "output": "The ibm-granite/granite-3.0-language-models repository has 266 stars and 28 forks on GitHub."
  },
  {
    "input": "When was the Mistral 7B paper published?",
    "output": "The Mistral 7B paper was published in 2023."
  },
  {
    "input": "Where can I find the code for BLIP-2: Bootstrapping Language-Image Pre-training?",
    "output": "The code for BLIP-2: Bootstrapping Language-Image Pre-training is available at https://github.com/salesforce/LAVIS (salesforce/LAVIS)."
  },
  {
    "input": "How popular is the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis implementation?",
    "output": "The HumanAIGC/AnimateAnyone repository has 14,794 stars and 1,008 forks on GitHub."
  },
  {
    "input": "Explain the lucidrains/flamingo-pytorch implementation of Flamingo: A Visual Language Model for Few-Shot Learning.",
    "output": "Implementation of \ud83e\udda9 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "Explain the facebookresearch/fairseq implementation of RoBERTa: A Robustly Optimized BERT Pretraining Approach.",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Kwai-YuanQi/MM-RLHF which the next step forward in multimodal llm alignment It is written in Python. The repository has 189 stars, indicating high community interest."
  },
  {
    "input": "What programming language is LLaMA-Adapter: Efficient Fine-tuning of Language Models implemented in?",
    "output": "The LLaMA-Adapter: Efficient Fine-tuning of Language Models implementation is written in Python."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Mistral 7B?",
    "output": "The code for Mistral 7B is available at https://github.com/mistralai/mistral-inference (mistralai/mistral-inference)."
  },
  {
    "input": "What language is the turboderp-org/exllamav2 repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was LLM.int8(): 8-bit Matrix Multiplication for Transformers released?",
    "output": "The LLM.int8(): 8-bit Matrix Multiplication for Transformers paper was published in 2022."
  },
  {
    "input": "Explain the priba/nmp_qc implementation of Neural Message Passing for Quantum Chemistry.",
    "output": "Our own implementation of Neural Message Passing for Computer Vision paper"
  },
  {
    "input": "What is the ArXiv category of Horovod: fast and easy distributed deep learning in TensorFlow?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What paper has ArXiv ID 2302.05206?",
    "output": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 is available at https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero (Open-Reasoner-Zero/Open-Reasoner-Zero)."
  },
  {
    "input": "What year was HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction released?",
    "output": "The HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction paper was published in 2021."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10 is available at https://github.com/HazyResearch/safari (HazyResearch/safari)."
  },
  {
    "input": "What paper has ArXiv ID 2104.00355?",
    "output": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2303.02913?",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "Explain the frankxu2004/knnlm-why implementation of arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10.",
    "output": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\""
  },
  {
    "input": "What is the GitHub repository for S4: Efficiently Modeling Long Sequences?",
    "output": "The code for S4: Efficiently Modeling Long Sequences is available at https://github.com/state-spaces/s4 (state-spaces/s4)."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implemented in?",
    "output": "The EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10 implementation?",
    "output": "The amazon-science/mm-cot repository has 3,985 stars and 333 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for Generating Wikipedia by Summarizing Long Sequences?",
    "output": "The code for Generating Wikipedia by Summarizing Long Sequences is available at https://github.com/tensorflow/tensor2tensor (tensorflow/tensor2tensor)."
  },
  {
    "input": "Explain the lucidrains/audiolm-pytorch implementation of AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "What paper has ArXiv ID 1706.02216?",
    "output": "Inductive Representation Learning on Large Graphs"
  },
  {
    "input": "What are the key topics in the jasonwei20/eda_nlp repository?",
    "output": "The jasonwei20/eda_nlp repository covers: nlp, data-augmentation, text-classification, synonyms, embeddings, sentence, classification, rnn, cnn, swap, position."
  },
  {
    "input": "Explain the facebookresearch/ConvNeXt-V2 implementation of arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10.",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What does the plurai-ai/intellagent repository do?",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "Explain the artidoro/qlora implementation of QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "Explain the volcengine/verl implementation of arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10.",
    "output": "verl: Volcano Engine Reinforcement Learning for LLMs"
  },
  {
    "input": "Describe the turboderp-org/exllamav2 repository that implements arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10.",
    "output": "A fast inference library for running LLMs locally on modern consumer-class GPUs"
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00774.",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "Explain the arcee-ai/mergekit implementation of arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10.",
    "output": "Tools for merging pretrained large language models."
  },
  {
    "input": "What category is the RAG: Retrieval-Augmented Generation paper in?",
    "output": "RAG: Retrieval-Augmented Generation is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "When was the Competition-Level Code Generation with AlphaCode paper published?",
    "output": "The Competition-Level Code Generation with AlphaCode paper was published in 2022."
  },
  {
    "input": "Summarize the Masked Autoencoders Are Scalable Vision Learners paper.",
    "output": "Masked Autoencoders Are Scalable Vision Learners is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at facebookresearch/mae which pytorch implementation of mae https//arxiv.org/abs/2111.06377 It is written in Python. The repository has 8,127 stars, indicating high community interest."
  },
  {
    "input": "What programming language is ELECTRA: Pre-training Text Encoders as Discriminators implemented in?",
    "output": "The ELECTRA: Pre-training Text Encoders as Discriminators implementation is written in Python."
  },
  {
    "input": "What paper has ArXiv ID 2505.22101?",
    "output": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10 is available at https://github.com/ruiqi-zhong/D5 (ruiqi-zhong/D5)."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 implementation?",
    "output": "The plurai-ai/intellagent repository has 1,154 stars and 140 forks on GitHub."
  },
  {
    "input": "Tell me the title of ArXiv paper 2108.12409.",
    "output": "Linformer: Self-Attention with Linear Complexity"
  },
  {
    "input": "What are the key topics in the haoliuhl/language-quantized-autoencoders repository?",
    "output": "The haoliuhl/language-quantized-autoencoders repository covers: autoencoders, bert, large-language-models, multimodal, roberta, vq, vqvae."
  },
  {
    "input": "What paper has ArXiv ID 2301.01947?",
    "output": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10"
  },
  {
    "input": "Explain the ruiqi-zhong/D5 implementation of arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10.",
    "output": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the ArXiv category of Generating Wikipedia by Summarizing Long Sequences?",
    "output": "Generating Wikipedia by Summarizing Long Sequences is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What does the facebookresearch/fairseq repository do?",
    "output": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python."
  },
  {
    "input": "When was the Grounding DINO: Marrying DINO with Grounded Pre-Training paper published?",
    "output": "The Grounding DINO: Marrying DINO with Grounded Pre-Training paper was published in 2023."
  },
  {
    "input": "In what year did Inductive Representation Learning on Large Graphs come out?",
    "output": "The Inductive Representation Learning on Large Graphs paper was published in 2017."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at autonomousvision/stylegan-t which [icml'23] stylegan-t: unlocking the power of gans for fast large-scale text-to-image synthesis It is written in Python. The repository has 1,198 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What programming language is GPT-4 Technical Report implemented in?",
    "output": "The GPT-4 Technical Report implementation is written in Python."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the tkipf/gcn repository that implements Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "Describe the noahshinn/reflexion repository that implements Reflexion: Language Agents with Verbal Reinforcement Learning.",
    "output": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What programming language is Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implemented in?",
    "output": "The Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks implementation is written in Python."
  },
  {
    "input": "Which field does AudioPaLM: A Large Language Model for Speech Understanding belong to?",
    "output": "AudioPaLM: A Large Language Model for Speech Understanding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the google/BIG-bench repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How popular is the FLAN: Finetuned Language Models Are Zero-Shot Learners implementation?",
    "output": "The google-research/FLAN repository has 1,556 stars and 160 forks on GitHub."
  },
  {
    "input": "Where can I find the code for LoRA: Low-Rank Adaptation of Large Language Models?",
    "output": "The code for LoRA: Low-Rank Adaptation of Large Language Models is available at https://github.com/microsoft/LoRA (microsoft/LoRA)."
  },
  {
    "input": "Where can I find the code for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "Which field does High-Resolution Image Synthesis with Latent Diffusion Models belong to?",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.00704.",
    "output": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10"
  },
  {
    "input": "What is the ArXiv category of XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale?",
    "output": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2002.04745?",
    "output": "Lookahead Optimizer: k steps forward, 1 step back"
  },
  {
    "input": "Tell me the title of ArXiv paper 1503.02531.",
    "output": "Distilling the Knowledge in a Neural Network"
  },
  {
    "input": "Summarize the RoFormer: Enhanced Transformer with Rotary Position Embedding paper.",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at ZhuiyiTechnology/roformer which rotary transformer It is written in Python. The repository has 1,059 stars, indicating high community interest."
  },
  {
    "input": "What year was RetNet: Retentive Network: A Successor to Transformer released?",
    "output": "The RetNet: Retentive Network: A Successor to Transformer paper was published in 2022."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ContextualAI/HALOs which a library with extensible implementations of dpo, kto, ppo, orpo, and other human-aware loss functions (halos). It is written in Python. The repository has 894 stars, indicating high community interest."
  },
  {
    "input": "What language is the uber/orbit repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What language is the hpcaitech/Open-Sora repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the LIMA: Less Is More for Alignment repository have?",
    "output": "The meta-llama/llama repository has 58,976 stars and 9,812 forks on GitHub."
  },
  {
    "input": "Explain the langchain-ai/langchain implementation of MRKL Systems: A modular approach to AGI.",
    "output": "\ud83e\udd9c\ud83d\udd17 The platform for reliable agents."
  },
  {
    "input": "Tell me the title of ArXiv paper 2303.02913.",
    "output": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the Shark-NLP/OpenICL repository?",
    "output": "The Shark-NLP/OpenICL repository covers: in-context-learning, nlp, language-model."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Open-Reasoner-Zero/Open-Reasoner-Zero which official repo for open-reasoner-zero It is written in Python. The repository has 2,074 stars, indicating high community interest."
  },
  {
    "input": "What is Swin Transformer: Hierarchical Vision Transformer using Shifted Windows about?",
    "output": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at microsoft/Swin-Transformer which this is an official implementation for \"swin transformer: hierarchical vision transformer using shifted windows\". It is written in Python. The repository has 15,517 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Deep Graph Library: A Graph-Centric, Highly-Performant Package?",
    "output": "The code for Deep Graph Library: A Graph-Centric, Highly-Performant Package is available at https://github.com/dmlc/dgl (dmlc/dgl)."
  },
  {
    "input": "Summarize the HuggingGPT: Solving AI Tasks with ChatGPT paper.",
    "output": "HuggingGPT: Solving AI Tasks with ChatGPT is a 2023 paper in Artificial Intelligence. The implementation is available at microsoft/JARVIS which jarvis, a system to connect llms with ml community. paper: https://arxiv.org/pdf/2303.17580.pdf It is written in Python. The repository has 24,488 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10 is available at https://github.com/plurai-ai/intellagent (plurai-ai/intellagent)."
  },
  {
    "input": "What are the key topics in the openai/CLIP repository?",
    "output": "The openai/CLIP repository covers: deep-learning, machine-learning."
  },
  {
    "input": "When was the RoBERTa: A Robustly Optimized BERT Pretraining Approach paper published?",
    "output": "The RoBERTa: A Robustly Optimized BERT Pretraining Approach paper was published in 2019."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is available at https://github.com/GAIR-NLP/LIMO (GAIR-NLP/LIMO)."
  },
  {
    "input": "What paper has ArXiv ID 2312.02120?",
    "output": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10"
  },
  {
    "input": "What paper has ArXiv ID 2205.12755?",
    "output": "Beir: A Heterogeneous Benchmark for IR"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2112.11446?",
    "output": "FLAN: Finetuned Language Models Are Zero-Shot Learners"
  },
  {
    "input": "Summarize the Beir: A Heterogeneous Benchmark for IR paper.",
    "output": "Beir: A Heterogeneous Benchmark for IR is a 2022 paper in Information Retrieval. The implementation is available at beir-cellar/beir which a heterogeneous benchmark for information retrieval. easy to use, evaluate your models across 15+ diverse ir datasets. It is written in Python. The repository has 2,014 stars, indicating high community interest."
  },
  {
    "input": "Tell me the title of ArXiv paper 2005.11401.",
    "output": "RAG: Retrieval-Augmented Generation"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is MetaGPT: Meta Programming for Multi-Agent Systems about?",
    "output": "MetaGPT: Meta Programming for Multi-Agent Systems is a 2023 paper in Artificial Intelligence. The implementation is available at FoundationAgents/MetaGPT which \ud83c\udf1f the multi-agent framework: first ai software company, towards natural language programming It is written in Python. The repository has 60,328 stars, indicating high community interest."
  },
  {
    "input": "What does the MoonshotAI/Kimi-Audio repository do?",
    "output": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-code-models which granite code models: a family of open foundation models for code intelligence The repository has 1,245 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "What is Sequence to Sequence Learning with Neural Networks about?",
    "output": "Sequence to Sequence Learning with Neural Networks is a 2014 paper in Computation and Language (Natural Language Processing). The implementation is available at farizrahman4u/seq2seq which sequence to sequence learning with keras It is written in Python. The repository has 3,175 stars, indicating high community interest."
  },
  {
    "input": "Describe the tmikolov/word2vec repository that implements Efficient Estimation of Word Representations in Vector Space.",
    "output": "Automatically exported from code.google.com/p/word2vec"
  },
  {
    "input": "What paper has ArXiv ID 2205.01068?",
    "output": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "Explain the Open-Reasoner-Zero/Open-Reasoner-Zero implementation of arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10.",
    "output": "Official Repo for Open-Reasoner-Zero"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10 is available at https://github.com/LTH14/fractalgen (LTH14/fractalgen)."
  },
  {
    "input": "What are the key topics in the facebookresearch/CovidPrognosis repository?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at MoonshotAI/Kimi-Audio which kimi-audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation It is written in Python. The repository has 4,372 stars, indicating high community interest."
  },
  {
    "input": "What does the TsinghuaAI/CPM-1-Generate repository do?",
    "output": "Chinese Pre-Trained Language Models (CPM-LM) Version-I"
  },
  {
    "input": "Tell me the title of ArXiv paper 1910.01108.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding"
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2209.07858?",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10"
  },
  {
    "input": "What does the lucidrains/muse-maskgit-pytorch repository do?",
    "output": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10 is a 2022 paper in Computation and Language (Natural Language Processing). The implementation is available at google/BIG-bench which beyond the imitation game collaborative benchmark for measuring and extrapolating the capabilities of language models It is written in Python. The repository has 3,167 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10 is available at https://github.com/amazon-science/mm-cot (amazon-science/mm-cot)."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "What language is the volcengine/verl repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of StarCoder: May the source be with you!?",
    "output": "StarCoder: May the source be with you! is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2105.05233.",
    "output": "Diffusion Models Beat GANs on Image Synthesis"
  },
  {
    "input": "Describe the lucidrains/audiolm-pytorch repository that implements AudioPaLM: A Large Language Model for Speech Understanding.",
    "output": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "What category is the Contriever: Unsupervised Dense Information Retrieval paper in?",
    "output": "Contriever: Unsupervised Dense Information Retrieval is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at google-deepmind/tracr It is written in Python. The repository has 548 stars, indicating high community interest."
  },
  {
    "input": "Give me an overview of Neural Collaborative Filtering.",
    "output": "Neural Collaborative Filtering is a 2017 paper in Information Retrieval. The implementation is available at hexiangnan/neural_collaborative_filtering which neural collaborative filtering It is written in Python. The repository has 1,865 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What's the implementation repository for Masked Autoencoders Are Scalable Vision Learners?",
    "output": "The code for Masked Autoencoders Are Scalable Vision Learners is available at https://github.com/facebookresearch/mae (facebookresearch/mae)."
  },
  {
    "input": "Which field does ControlNet: Adding Conditional Control to Text-to-Image Diffusion belong to?",
    "output": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10 implementation cover?",
    "output": "The ContextualAI/HALOs repository covers: alignment, dpo, halos, kto, ppo, rlhf."
  },
  {
    "input": "What is the ArXiv category of Grounding DINO: Marrying DINO with Grounded Pre-Training?",
    "output": "Grounding DINO: Marrying DINO with Grounded Pre-Training is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2301.12652?",
    "output": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What does the facebookresearch/ConvNeXt-V2 repository do?",
    "output": "Code release for ConvNeXt V2 model"
  },
  {
    "input": "What category is the Feature Pyramid Networks for Object Detection paper in?",
    "output": "Feature Pyramid Networks for Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the artidoro/qlora repository that implements QLoRA: Efficient Finetuning of Quantized LLMs.",
    "output": "QLoRA: Efficient Finetuning of Quantized LLMs"
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 implementation cover?",
    "output": "The zai-org/ImageReward repository covers: diffusion-models, generative-model, rlhf, human-preferences."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ZHZisZZ/modpo which [acl'24] beyond one-preference-fits-all alignment: multi-objective direct preference optimization It is written in Python. The repository has 93 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1708.05031?",
    "output": "Neural Collaborative Filtering"
  },
  {
    "input": "What does the microsoft/DeBERTa repository do?",
    "output": "The implementation of DeBERTa"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the DreamFusion: Text-to-3D using 2D Diffusion paper.",
    "output": "DreamFusion: Text-to-3D using 2D Diffusion is a 2022 paper in Computer Vision and Pattern Recognition. The implementation is available at ashawkey/stable-dreamfusion which text-to-3d & image-to-3d & mesh exportation with nerf + diffusion. It is written in Python. The repository has 8,785 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Linformer: Self-Attention with Linear Complexity?",
    "output": "The code for Linformer: Self-Attention with Linear Complexity is available at https://github.com/tatp22/linformer-pytorch (tatp22/linformer-pytorch)."
  },
  {
    "input": "Which field does LIMA: Less Is More for Alignment belong to?",
    "output": "LIMA: Less Is More for Alignment is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What paper has ArXiv ID 2204.05862?",
    "output": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10"
  },
  {
    "input": "What are the key topics in the facebookresearch/fairseq repository?",
    "output": "The facebookresearch/fairseq repository covers: python, pytorch, artificial-intelligence."
  },
  {
    "input": "Explain the Universal Language Model Fine-tuning for Text Classification paper.",
    "output": "Universal Language Model Fine-tuning for Text Classification is a 2018 paper in Computation and Language (Natural Language Processing). The implementation is available at fastai/fastai which the fastai deep learning library It is written in Jupyter Notebook. The repository has 27,653 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What category is the Segment Anything paper in?",
    "output": "Segment Anything is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "How popular is the LLM.int8(): 8-bit Matrix Multiplication for Transformers implementation?",
    "output": "The bitsandbytes-foundation/bitsandbytes repository has 7,804 stars and 798 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "Explain the google/BIG-bench implementation of arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10.",
    "output": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models"
  },
  {
    "input": "Describe the lamini-ai/Lamini-Memory-Tuning repository that implements arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10.",
    "output": "Banishing LLM Hallucinations Requires Rethinking Generalization"
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "Explain the lm-sys/FastChat implementation of Vicuna: An Open-Source Chatbot.",
    "output": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena."
  },
  {
    "input": "Where can I find the code for CLIP: Learning Transferable Visual Models From Natural Language?",
    "output": "The code for CLIP: Learning Transferable Visual Models From Natural Language is available at https://github.com/openai/CLIP (openai/CLIP)."
  },
  {
    "input": "When was the Finetuned Language Models Are Zero-Shot Learners paper published?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners paper was published in 2021."
  },
  {
    "input": "Explain the ColBERT: Efficient Passage Search via Contextualized Late Interaction paper.",
    "output": "ColBERT: Efficient Passage Search via Contextualized Late Interaction is a 2021 paper in Information Retrieval. The implementation is available at stanford-futuredata/ColBERT which colbert: state-of-the-art neural search (sigir'20, tacl'21, neurips'21, naacl'22, cikm'22, acl'23, emnlp'23) It is written in Python. The repository has 3,724 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel?",
    "output": "The code for PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel is available at https://github.com/pytorch/pytorch (pytorch/pytorch)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What year was Longformer: The Long-Document Transformer released?",
    "output": "The Longformer: The Long-Document Transformer paper was published in 2020."
  },
  {
    "input": "Which field does Proximal Policy Optimization Algorithms belong to?",
    "output": "Proximal Policy Optimization Algorithms is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the TinyBERT: Distilling BERT for Natural Language Understanding paper.",
    "output": "TinyBERT: Distilling BERT for Natural Language Understanding is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at huawei-noah/Pretrained-Language-Model which pretrained language model and its related optimization techniques developed by huawei noah's ark lab. It is written in Python. The repository has 3,150 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10 is available at https://github.com/oughtinc/ice (oughtinc/ice)."
  },
  {
    "input": "What paper has ArXiv ID 1711.00489?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "Which field does BEiT: BERT Pre-Training of Image Transformers belong to?",
    "output": "BEiT: BERT Pre-Training of Image Transformers is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2504.20734?",
    "output": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10"
  },
  {
    "input": "Summarize the Longformer: The Long-Document Transformer paper.",
    "output": "Longformer: The Long-Document Transformer is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at allenai/longformer which longformer: the long-document transformer It is written in Python. The repository has 2,176 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 is available at https://github.com/facebookresearch/CovidPrognosis (facebookresearch/CovidPrognosis)."
  },
  {
    "input": "Explain the huggingface/transformers implementation of DistilBERT: a distilled version of BERT.",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "Tell me the title of ArXiv paper 2305.06161.",
    "output": "StarCoder: May the source be with you!"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is available at https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat (jacopotagliabue/you-dont-need-a-bigger-boat)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2304.06767?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10"
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at google-research/pegasus It is written in Python. The repository has 1,645 stars, indicating high community interest."
  },
  {
    "input": "What language is the tomekkorbak/pretraining-with-human-feedback repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.09515?",
    "output": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10"
  },
  {
    "input": "Explain the InPars: Data Augmentation for Information Retrieval paper.",
    "output": "InPars: Data Augmentation for Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at zetaalphavector/InPars which inquisitive parrots for search It is written in Python. The repository has 198 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is available at https://github.com/mit-han-lab/offsite-tuning (mit-han-lab/offsite-tuning)."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10 is available at https://github.com/RLHFlow/Online-RLHF (RLHFlow/Online-RLHF)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1711.00489?",
    "output": "Horovod: fast and easy distributed deep learning in TensorFlow"
  },
  {
    "input": "What language is the allenai/codescientist repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is available at https://github.com/Linear95/APO (Linear95/APO)."
  },
  {
    "input": "What category is the Wide & Deep Learning for Recommender Systems paper in?",
    "output": "Wide & Deep Learning for Recommender Systems is in the Information Retrieval (cs.IR) category."
  },
  {
    "input": "What language is the facebookresearch/faiss repository written in?",
    "output": "The Billion-scale similarity search with GPUs implementation is written in C++."
  },
  {
    "input": "What paper has ArXiv ID 2307.03109?",
    "output": "AlpacaEval: An Automatic Evaluator for Instruction-following"
  },
  {
    "input": "What paper has ArXiv ID 2305.15334?",
    "output": "Gorilla: Large Language Model Connected with Massive APIs"
  },
  {
    "input": "What is the ArXiv category of EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at oughtinc/ice which interactive composition explorer: a debugger for compositional language model programs It is written in Python. The repository has 563 stars, indicating high community interest."
  },
  {
    "input": "Describe the microsoft/LoRA repository that implements LoRA: Low-Rank Adaptation of Large Language Models.",
    "output": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\""
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at jacopotagliabue/you-dont-need-a-bigger-boat which an end-to-end implementation of intent prediction with metaflow and other cool tools It is written in Python. The repository has 873 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10 implementation?",
    "output": "The oughtinc/ice repository has 563 stars and 70 forks on GitHub."
  },
  {
    "input": "Describe the RLHFlow/Directional-Preference-Alignment repository that implements arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10.",
    "output": "Directional Preference Alignment"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10 implementation?",
    "output": "The facebookresearch/fairseq repository has 32,022 stars and 6,633 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for Codex: Evaluating Large Language Models Trained on Code?",
    "output": "The code for Codex: Evaluating Large Language Models Trained on Code is available at https://github.com/openai/human-eval (openai/human-eval)."
  },
  {
    "input": "Which field does LLaMA: Open and Efficient Foundation Language Models belong to?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Explain the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 362 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2201.08239?",
    "output": "Contriever: Unsupervised Dense Information Retrieval"
  },
  {
    "input": "What year was Animate Anyone: Consistent and Controllable Image-to-Video Synthesis released?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "How many stars does the Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena repository have?",
    "output": "The lm-sys/FastChat repository has 39,298 stars and 4,778 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at facebookresearch/fairseq which facebook ai research sequence-to-sequence toolkit written in python. It is written in Python. The repository has 32,022 stars, indicating high community interest."
  },
  {
    "input": "What language is the huggingface/alignment-handbook repository written in?",
    "output": "The Zephyr: Direct Distillation of LM Alignment implementation is written in Python."
  },
  {
    "input": "Tell me the title of ArXiv paper 2112.10752.",
    "output": "High-Resolution Image Synthesis with Latent Diffusion Models"
  },
  {
    "input": "Describe the haoliuhl/language-quantized-autoencoders repository that implements arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10.",
    "output": "Language Quantized AutoEncoders"
  },
  {
    "input": "What is the ArXiv category of LLaMA: Open and Efficient Foundation Language Models?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is The Power of Scale for Parameter-Efficient Prompt Tuning implemented in?",
    "output": "The The Power of Scale for Parameter-Efficient Prompt Tuning implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Neural Machine Translation by Jointly Learning to Align and Translate?",
    "output": "The code for Neural Machine Translation by Jointly Learning to Align and Translate is available at https://github.com/lisa-groundhog/GroundHog (lisa-groundhog/GroundHog)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at anthropics/hh-rlhf which human preference data for \"training a helpful and harmless assistant with reinforcement learning from human feedback\" The repository has 1,801 stars, indicating high community interest."
  },
  {
    "input": "Where can I find the code for Llama 2: Open Foundation and Fine-Tuned Chat Models?",
    "output": "The code for Llama 2: Open Foundation and Fine-Tuned Chat Models is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10 is available at https://github.com/ibm-granite/granite-code-models (ibm-granite/granite-code-models)."
  },
  {
    "input": "Summarize the CLIP: Learning Transferable Visual Models From Natural Language paper.",
    "output": "CLIP: Learning Transferable Visual Models From Natural Language is a 2021 paper in Computer Vision and Pattern Recognition. The implementation is available at openai/CLIP which clip (contrastive language-image pretraining),  predict the most relevant text snippet given an image It is written in Jupyter Notebook. The repository has 31,905 stars, indicating high community interest."
  },
  {
    "input": "What language is the microsoft/unilm repository written in?",
    "output": "The Text Embeddings by Weakly-Supervised Contrastive Pre-training implementation is written in Python."
  },
  {
    "input": "What language is the MoonshotAI/Kimi-Audio repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is Finetuned Language Models Are Zero-Shot Learners implemented in?",
    "output": "The Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 1910.01741?",
    "output": "Dream to Control: Learning Behaviors by Latent Imagination"
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation?",
    "output": "The GAIR-NLP/LIMO repository has 1,054 stars and 52 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10 is available at https://github.com/anthropics/hh-rlhf (anthropics/hh-rlhf)."
  },
  {
    "input": "Describe the tensorflow/tensor2tensor repository that implements Generating Wikipedia by Summarizing Long Sequences.",
    "output": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research."
  },
  {
    "input": "Which field does EfficientDet: Scalable and Efficient Object Detection belong to?",
    "output": "EfficientDet: Scalable and Efficient Object Detection is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "What paper has ArXiv ID 2208.01618?",
    "output": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models"
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2503.01141?",
    "output": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10"
  },
  {
    "input": "What programming language is ReAct: Synergizing Reasoning and Acting in Language Models implemented in?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models implementation is written in Jupyter Notebook."
  },
  {
    "input": "What programming language is PyTorch: An Imperative Style, High-Performance Deep Learning Library implemented in?",
    "output": "The PyTorch: An Imperative Style, High-Performance Deep Learning Library implementation is written in Python."
  },
  {
    "input": "Where can I find the code for Code Llama: Open Foundation Models for Code?",
    "output": "The code for Code Llama: Open Foundation Models for Code is available at https://github.com/meta-llama/codellama (meta-llama/codellama)."
  },
  {
    "input": "Give me an overview of An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.",
    "output": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale is a 2020 paper in Computer Vision and Pattern Recognition. The implementation is available at google-research/vision_transformer It is written in Jupyter Notebook. The repository has 12,102 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10 is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at TsinghuaAI/CPM-1-Generate which chinese pre-trained language models (cpm-lm) version-i It is written in Python. The repository has 1,582 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "The code for LLM.int8(): 8-bit Matrix Multiplication for Transformers is available at https://github.com/bitsandbytes-foundation/bitsandbytes (bitsandbytes-foundation/bitsandbytes)."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What paper has ArXiv ID 2301.01751?",
    "output": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10"
  },
  {
    "input": "What programming language is Inductive Representation Learning on Large Graphs implemented in?",
    "output": "The Inductive Representation Learning on Large Graphs implementation is written in Python."
  },
  {
    "input": "What programming language is Masked Autoencoders Are Scalable Vision Learners implemented in?",
    "output": "The Masked Autoencoders Are Scalable Vision Learners implementation is written in Python."
  },
  {
    "input": "Give me an overview of Codex: Evaluating Large Language Models Trained on Code.",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,048 stars, indicating high community interest."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Tell me the title of ArXiv paper 2301.13688.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10"
  },
  {
    "input": "What language is the tensorflow/tensor2tensor repository written in?",
    "output": "The Attention Is All You Need implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10 is a 2022 paper in Machine Learning. The implementation is available at openai/following-instructions-human-feedback The repository has 1,251 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of LLM.int8(): 8-bit Matrix Multiplication for Transformers?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for Text Embeddings by Weakly-Supervised Contrastive Pre-training?",
    "output": "The code for Text Embeddings by Weakly-Supervised Contrastive Pre-training is available at https://github.com/microsoft/unilm (microsoft/unilm)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2412.15838.",
    "output": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10"
  },
  {
    "input": "Explain the allenai/codescientist implementation of arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10.",
    "output": "CodeScientist: An automated scientific discovery system for code-based experiments"
  },
  {
    "input": "How popular is the HuggingGPT: Solving AI Tasks with ChatGPT implementation?",
    "output": "The microsoft/JARVIS repository has 24,488 stars and 2,059 forks on GitHub."
  },
  {
    "input": "What does the huggingface/transformers repository do?",
    "output": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. "
  },
  {
    "input": "What language is the OpenRLHF/OpenRLHF repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What paper has ArXiv ID 1612.03144?",
    "output": "Feature Pyramid Networks for Object Detection"
  },
  {
    "input": "How many stars does the MMLU: Measuring Massive Multitask Language Understanding repository have?",
    "output": "The hendrycks/test repository has 1,528 stars and 112 forks on GitHub."
  },
  {
    "input": "What is the GitHub repository for MMLU: Measuring Massive Multitask Language Understanding?",
    "output": "The code for MMLU: Measuring Massive Multitask Language Understanding is available at https://github.com/hendrycks/test (hendrycks/test)."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10 repository have?",
    "output": "The GXimingLu/Quark repository has 75 stars and 10 forks on GitHub."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10 is available at https://github.com/facebookresearch/TransCoder (facebookresearch/TransCoder)."
  },
  {
    "input": "How popular is the Adam: A Method for Stochastic Optimization implementation?",
    "output": "The pytorch/pytorch repository has 95,727 stars and 26,159 forks on GitHub."
  },
  {
    "input": "Where can I find the code for LIMA: Less Is More for Alignment?",
    "output": "The code for LIMA: Less Is More for Alignment is available at https://github.com/meta-llama/llama (meta-llama/llama)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "When was the ReAct: Synergizing Reasoning and Acting in Language Models paper published?",
    "output": "The ReAct: Synergizing Reasoning and Acting in Language Models paper was published in 2022."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Which field does Question and Answer Test-Train Overlap in Open-Domain QA Datasets belong to?",
    "output": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is the GitHub repository for Dense Passage Retrieval for Open-Domain Question Answering?",
    "output": "The code for Dense Passage Retrieval for Open-Domain Question Answering is available at https://github.com/facebookresearch/DPR (facebookresearch/DPR)."
  },
  {
    "input": "What paper has ArXiv ID 2303.06865?",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10"
  },
  {
    "input": "Where can I find the code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 is available at https://github.com/JohnNay/llm-lobbyist (JohnNay/llm-lobbyist)."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the tkipf/gcn implementation of Semi-Supervised Classification with Graph Convolutional Networks.",
    "output": "Implementation of Graph Convolutional Networks in TensorFlow"
  },
  {
    "input": "What language is the Crista23/goal_directedness_llms repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at amazon-science/mm-cot which official implementation for \"multimodal chain-of-thought reasoning in language models\" (stay tuned and more will be updated) It is written in Python. The repository has 3,985 stars, indicating high community interest."
  },
  {
    "input": "What is Analyzing the Structure of Attention in a Transformer Language Model about?",
    "output": "Analyzing the Structure of Attention in a Transformer Language Model is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at clarkkev/attention-analysis It is written in Jupyter Notebook. The repository has 469 stars, indicating high community interest."
  },
  {
    "input": "Describe the MAXNORM8650/MedAgentSim repository that implements arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10.",
    "output": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)"
  },
  {
    "input": "Describe the sentient-agi/OpenDeepSearch repository that implements arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10.",
    "output": "SOTA search powered LLM"
  },
  {
    "input": "What category is the The Flan Collection: Designing Data and Methods for Effective Instruction Tuning paper in?",
    "output": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What does the huggingface/alignment-handbook repository do?",
    "output": "Robust recipes to align language models with human and AI preferences"
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10 is available at https://github.com/frankxu2004/knnlm-why (frankxu2004/knnlm-why)."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 implementation?",
    "output": "The LTH14/fractalgen repository has 1,206 stars and 66 forks on GitHub."
  },
  {
    "input": "What does the horovod/horovod repository do?",
    "output": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "Summarize the RAG: Retrieval-Augmented Generation paper.",
    "output": "RAG: Retrieval-Augmented Generation is a 2020 paper in Computation and Language (Natural Language Processing). The implementation is available at facebookresearch/DPR which dense passage retriever - is a set of tools and models for open domain q&a task. It is written in Python. The repository has 1,851 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at ibm-granite/granite-3.0-language-models The repository has 266 stars, indicating high community interest."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What is Reformer: The Efficient Transformer about?",
    "output": "Reformer: The Efficient Transformer is a 2020 paper in Machine Learning. The implementation is available at google/trax which trax \u2014 deep learning with clear code and speed It is written in Python. The repository has 8,294 stars, indicating high community interest."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10 paper was published in 2021."
  },
  {
    "input": "How popular is the Enriching Word Vectors with Subword Information implementation?",
    "output": "The facebookresearch/fastText repository has 26,445 stars and 4,815 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2306.17492?",
    "output": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10"
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper was published in 2024."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2104.08691?",
    "output": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at uclaml/SPIN which the official implementation of self-play fine-tuning (spin) It is written in Python. The repository has 1,226 stars, indicating high community interest."
  },
  {
    "input": "Which field does LLM.int8(): 8-bit Matrix Multiplication for Transformers belong to?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What year was arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 released?",
    "output": "The arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification?",
    "output": "The code for EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification is available at https://github.com/jasonwei20/eda_nlp (jasonwei20/eda_nlp)."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at mit-han-lab/offsite-tuning which offsite-tuning: transfer learning without full model It is written in Python. The repository has 383 stars, indicating high community interest."
  },
  {
    "input": "What is P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning about?",
    "output": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at THUDM/P-tuning-v2 which an optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks It is written in Python. The repository has 2,069 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "Explain the huggingface/sentence-transformers implementation of Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.",
    "output": "State-of-the-Art Text Embeddings"
  },
  {
    "input": "Tell me the title of ArXiv paper 2104.05556.",
    "output": "Multi-Task Deep Neural Networks for Natural Language Understanding"
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10 is a 2024 paper in Machine Learning. The implementation is available at hpcaitech/Open-Sora which open-sora: democratizing efficient video production for all It is written in Python. The repository has 28,081 stars, indicating high community interest."
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "When was the Universal Language Model Fine-tuning for Text Classification paper published?",
    "output": "The Universal Language Model Fine-tuning for Text Classification paper was published in 2018."
  },
  {
    "input": "What category is the ReAct: Synergizing Reasoning and Acting in Language Models paper in?",
    "output": "ReAct: Synergizing Reasoning and Acting in Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What is Codex: Evaluating Large Language Models Trained on Code about?",
    "output": "Codex: Evaluating Large Language Models Trained on Code is a 2021 paper in Machine Learning. The implementation is available at openai/human-eval which code for the paper \"evaluating large language models trained on code\" It is written in Python. The repository has 3,048 stars, indicating high community interest."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10 implementation?",
    "output": "The openai/summarize-from-feedback repository has 1,056 stars and 151 forks on GitHub."
  },
  {
    "input": "In what year did arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 come out?",
    "output": "The arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 paper was published in 2025."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2106.07447?",
    "output": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction"
  },
  {
    "input": "When was the Graph Attention Networks paper published?",
    "output": "The Graph Attention Networks paper was published in 2017."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "Which field does LLaMA-Adapter: Efficient Fine-tuning of Language Models belong to?",
    "output": "LLaMA-Adapter: Efficient Fine-tuning of Language Models is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Where can I find the code for JAX: Composable transformations of Python+NumPy programs?",
    "output": "The code for JAX: Composable transformations of Python+NumPy programs is available at https://github.com/jax-ml/jax (jax-ml/jax)."
  },
  {
    "input": "Which field does ALBERT: A Lite BERT belong to?",
    "output": "ALBERT: A Lite BERT is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the liuzhuang13/DenseNet repository written in?",
    "output": "The Densely Connected Convolutional Networks implementation is written in Lua."
  },
  {
    "input": "When was the Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper published?",
    "output": "The Question and Answer Test-Train Overlap in Open-Domain QA Datasets paper was published in 2020."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is available at https://github.com/Linear95/APO (Linear95/APO)."
  },
  {
    "input": "Tell me the title of ArXiv paper 2111.03026.",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10 repository have?",
    "output": "The allenai/codescientist repository has 303 stars and 40 forks on GitHub."
  },
  {
    "input": "Give me an overview of Wide & Deep Learning for Recommender Systems.",
    "output": "Wide & Deep Learning for Recommender Systems is a 2016 paper in Information Retrieval. The implementation is available at tensorflow/models which models and examples built with tensorflow It is written in Python. The repository has 77,680 stars, indicating high community interest."
  },
  {
    "input": "What is LLaMA: Open and Efficient Foundation Language Models about?",
    "output": "LLaMA: Open and Efficient Foundation Language Models is a 2023 paper in Computation and Language (Natural Language Processing). The implementation is available at meta-llama/llama which inference code for llama models It is written in Python. The repository has 58,976 stars, indicating high community interest."
  },
  {
    "input": "What is arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 about?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at facebookresearch/CovidPrognosis which covid deterioration prediction based on chest x-ray radiographs via moco-trained image representations It is written in Python. The repository has 158 stars, indicating high community interest."
  },
  {
    "input": "What language is the google-deepmind/tracr repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Describe the zetaalphavector/InPars repository that implements InPars: Data Augmentation for Information Retrieval.",
    "output": "Inquisitive Parrots for Search"
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Where can I find the code for WizardCoder: Empowering Code LLMs with Evol-Instruct?",
    "output": "The code for WizardCoder: Empowering Code LLMs with Evol-Instruct is available at https://github.com/nlpxucan/WizardLM (nlpxucan/WizardLM)."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2203.05556?",
    "output": "FlashAttention: Fast and Memory-Efficient Exact Attention"
  },
  {
    "input": "In what year did Animate Anyone: Consistent and Controllable Image-to-Video Synthesis come out?",
    "output": "The Animate Anyone: Consistent and Controllable Image-to-Video Synthesis paper was published in 2023."
  },
  {
    "input": "Summarize the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context paper.",
    "output": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at kimiyoung/transformer-xl It is written in Python. The repository has 3,680 stars, indicating high community interest."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is a 2021 paper in Machine Learning. The implementation is available at rll-research/BPref which official codebase for \"b-pref: benchmarking preference-basedreinforcement learning\" contains scripts to reproduce experiments. It is written in Python. The repository has 133 stars, indicating high community interest."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10 is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What language is the zai-org/ImageReward repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "Which field does arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 belong to?",
    "output": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Explain the stanford-futuredata/ColBERT implementation of ColBERT: Efficient Passage Search via Contextualized Late Interaction.",
    "output": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)"
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10 repository have?",
    "output": "The anthropics/evals repository has 315 stars and 33 forks on GitHub."
  },
  {
    "input": "How many stars does the MetaGPT: Meta Programming for Multi-Agent Systems repository have?",
    "output": "The FoundationAgents/MetaGPT repository has 60,328 stars and 7,416 forks on GitHub."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10 is a 2025 paper in Machine Learning. The implementation is available at Crista23/goal_directedness_llms It is written in Python. The repository has 12 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for Segment Anything in Medical Images?",
    "output": "The code for Segment Anything in Medical Images is available at https://github.com/bowang-lab/MedSAM (bowang-lab/MedSAM)."
  },
  {
    "input": "What does the openai/whisper repository do?",
    "output": "Robust Speech Recognition via Large-Scale Weak Supervision"
  },
  {
    "input": "What is the ArXiv category of arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10?",
    "output": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "How popular is the Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context implementation?",
    "output": "The kimiyoung/transformer-xl repository has 3,680 stars and 765 forks on GitHub."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10 is available at https://github.com/simplescaling/s1 (simplescaling/s1)."
  },
  {
    "input": "What category is the RoFormer: Enhanced Transformer with Rotary Position Embedding paper in?",
    "output": "RoFormer: Enhanced Transformer with Rotary Position Embedding is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "What programming language is AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implemented in?",
    "output": "The AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning implementation is written in Python."
  },
  {
    "input": "What language is the state-spaces/mamba repository written in?",
    "output": "The Mamba: Linear-Time Sequence Modeling with Selective State Spaces implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10 is available at https://github.com/ZHZisZZ/modpo (ZHZisZZ/modpo)."
  },
  {
    "input": "Give me an overview of ELECTRA: Pre-training Text Encoders as Discriminators.",
    "output": "ELECTRA: Pre-training Text Encoders as Discriminators is a 2019 paper in Computation and Language (Natural Language Processing). The implementation is available at google-research/electra which electra: pre-training text encoders as discriminators rather than generators It is written in Python. The repository has 2,365 stars, indicating high community interest."
  },
  {
    "input": "How popular is the XLNet: Generalized Autoregressive Pretraining implementation?",
    "output": "The zihangdai/xlnet repository has 6,181 stars and 1,165 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10 is available at https://github.com/autonomousvision/stylegan-t (autonomousvision/stylegan-t)."
  },
  {
    "input": "What year was ZeRO: Memory Optimizations Toward Training Trillion Parameter Models released?",
    "output": "The ZeRO: Memory Optimizations Toward Training Trillion Parameter Models paper was published in 2019."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2309.16588?",
    "output": "Segment Anything in Medical Images"
  },
  {
    "input": "What language is the sylinrl/TruthfulQA repository written in?",
    "output": "The arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2301.00774?",
    "output": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10"
  },
  {
    "input": "What's the implementation repository for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10?",
    "output": "The code for arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10 is available at https://github.com/Shark-NLP/OpenICL (Shark-NLP/OpenICL)."
  },
  {
    "input": "When was the EfficientDet: Scalable and Efficient Object Detection paper published?",
    "output": "The EfficientDet: Scalable and Efficient Object Detection paper was published in 2020."
  },
  {
    "input": "What paper has ArXiv ID 2410.18050?",
    "output": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10"
  },
  {
    "input": "How many stars does the Animate Anyone: Consistent and Controllable Image-to-Video Synthesis repository have?",
    "output": "The HumanAIGC/AnimateAnyone repository has 14,794 stars and 1,008 forks on GitHub."
  },
  {
    "input": "Explain the llava-rlhf/LLaVA-RLHF implementation of arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10.",
    "output": "Aligning LMMs with Factually Augmented RLHF"
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at Linear95/APO which code for acl2024 paper - adversarial preference optimization (apo). It is written in Python. The repository has 56 stars, indicating high community interest."
  },
  {
    "input": "What is the title of the paper with ArXiv ID 2012.06009?",
    "output": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10"
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10 paper was published in 2023."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is the GitHub repository for Efficient Estimation of Word Representations in Vector Space?",
    "output": "The code for Efficient Estimation of Word Representations in Vector Space is available at https://github.com/tmikolov/word2vec (tmikolov/word2vec)."
  },
  {
    "input": "Explain the plurai-ai/intellagent implementation of arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10.",
    "output": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions"
  },
  {
    "input": "What language is the mistralai/mistral-inference repository written in?",
    "output": "The Mistral 7B implementation is written in Jupyter Notebook."
  },
  {
    "input": "How many stars does the arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10 repository have?",
    "output": "The facebookresearch/CovidPrognosis repository has 158 stars and 38 forks on GitHub."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10 implementation is written in Jupyter Notebook."
  },
  {
    "input": "What's the implementation repository for Inductive Representation Learning on Large Graphs?",
    "output": "The code for Inductive Representation Learning on Large Graphs is available at https://github.com/williamleif/GraphSAGE (williamleif/GraphSAGE)."
  },
  {
    "input": "Summarize the Contriever: Unsupervised Dense Information Retrieval paper.",
    "output": "Contriever: Unsupervised Dense Information Retrieval is a 2022 paper in Information Retrieval. The implementation is available at facebookresearch/contriever which contriever: unsupervised dense information retrieval with contrastive learning It is written in Python. The repository has 766 stars, indicating high community interest."
  },
  {
    "input": "How popular is the Denoising Diffusion Probabilistic Models implementation?",
    "output": "The hojonathanho/diffusion repository has 4,906 stars and 461 forks on GitHub."
  },
  {
    "input": "What topics does the arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10 implementation cover?",
    "output": "The facebookresearch/CovidPrognosis repository covers: medical-imaging, deep-learning, radiography, x-ray, pytorch, covid-19, medical-image-analysis."
  },
  {
    "input": "Which field does Sequence to Sequence Learning with Neural Networks belong to?",
    "output": "Sequence to Sequence Learning with Neural Networks is in the Computation and Language (Natural Language Processing) (cs.CL) category."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10 is a 2020 paper in Machine Learning. The implementation is available at uber/orbit which a python package for bayesian forecasting with object-oriented design and probabilistic models under the hood. It is written in Python. The repository has 2,021 stars, indicating high community interest."
  },
  {
    "input": "What programming language is arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implemented in?",
    "output": "The arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10 implementation is written in Python."
  },
  {
    "input": "What is LoRA: Low-Rank Adaptation of Large Language Models about?",
    "output": "LoRA: Low-Rank Adaptation of Large Language Models is a 2021 paper in Computation and Language (Natural Language Processing). The implementation is available at microsoft/LoRA which code for loralib, an implementation of \"lora: low-rank adaptation of large language models\" It is written in Python. The repository has 13,038 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the Dense Passage Retrieval for Open-Domain Question Answering repository have?",
    "output": "The facebookresearch/DPR repository has 1,851 stars and 316 forks on GitHub."
  },
  {
    "input": "Give me an overview of arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10.",
    "output": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at ruiqi-zhong/D5 which the github repo for goal driven discovery of distributional differences via language descriptions It is written in Python. The repository has 71 stars, indicating high community interest."
  },
  {
    "input": "How many stars does the PaLM: Scaling Language Modeling with Pathways repository have?",
    "output": "The lucidrains/PaLM-pytorch repository has 827 stars and 82 forks on GitHub."
  },
  {
    "input": "What paper has ArXiv ID 2206.01861?",
    "output": "LLM.int8(): 8-bit Matrix Multiplication for Transformers"
  },
  {
    "input": "Tell me the title of ArXiv paper 1906.04341.",
    "output": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
  },
  {
    "input": "What category is the LLaVA: Large Language and Vision Assistant paper in?",
    "output": "LLaVA: Large Language and Vision Assistant is in the Computer Vision and Pattern Recognition (cs.CV) category."
  },
  {
    "input": "Where can I find the code for SimCSE: Simple Contrastive Learning of Sentence Embeddings?",
    "output": "The code for SimCSE: Simple Contrastive Learning of Sentence Embeddings is available at https://github.com/princeton-nlp/SimCSE (princeton-nlp/SimCSE)."
  },
  {
    "input": "What language is the tatsu-lab/alpaca_eval repository written in?",
    "output": "The AlpacaEval: An Automatic Evaluator for Instruction-following implementation is written in Jupyter Notebook."
  },
  {
    "input": "What language is the google-research/FLAN repository written in?",
    "output": "The FLAN: Finetuned Language Models Are Zero-Shot Learners implementation is written in Python."
  },
  {
    "input": "How popular is the arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10 implementation?",
    "output": "The volcengine/verl repository has 17,344 stars and 2,782 forks on GitHub."
  },
  {
    "input": "What programming language is Competition-Level Code Generation with AlphaCode implemented in?",
    "output": "The Competition-Level Code Generation with AlphaCode implementation is written in C++."
  },
  {
    "input": "Explain the RetNet: Retentive Network: A Successor to Transformer paper.",
    "output": "RetNet: Retentive Network: A Successor to Transformer is a 2022 paper in Machine Learning. The implementation is available at microsoft/torchscale which foundation architecture for (m)llms It is written in Python. The repository has 3,124 stars, indicating high community interest."
  },
  {
    "input": "What is the GitHub repository for DreamFusion: Text-to-3D using 2D Diffusion?",
    "output": "The code for DreamFusion: Text-to-3D using 2D Diffusion is available at https://github.com/ashawkey/stable-dreamfusion (ashawkey/stable-dreamfusion)."
  },
  {
    "input": "What's the implementation repository for Billion-scale similarity search with GPUs?",
    "output": "The code for Billion-scale similarity search with GPUs is available at https://github.com/facebookresearch/faiss (facebookresearch/faiss)."
  },
  {
    "input": "Summarize the AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning paper.",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is a 2022 paper in Machine Learning. The implementation is available at QingruZhang/AdaLoRA which adalora: adaptive budget allocation for parameter-efficient fine-tuning (iclr 2023).  It is written in Python. The repository has 362 stars, indicating high community interest."
  },
  {
    "input": "Summarize the arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at NVlabs/prismer which the implementation of \"prismer: a vision-language model with multi-task experts\". It is written in Python. The repository has 1,307 stars, indicating high community interest."
  },
  {
    "input": "What's the implementation repository for Prefix-Tuning: Optimizing Continuous Prompts for Generation?",
    "output": "The code for Prefix-Tuning: Optimizing Continuous Prompts for Generation is available at https://github.com/XiangLi1999/PrefixTuning (XiangLi1999/PrefixTuning)."
  },
  {
    "input": "Explain the arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 paper.",
    "output": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10 is a 2023 paper in Machine Learning. The implementation is available at FMInference/FlexLLMGen which running large language models on a single gpu for throughput-oriented scenarios. It is written in Python. The repository has 9,381 stars, indicating high community interest."
  },
  {
    "input": "When was the arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper published?",
    "output": "The arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10 paper was published in 2020."
  },
  {
    "input": "How many stars does the Longformer: The Long-Document Transformer repository have?",
    "output": "The allenai/longformer repository has 2,176 stars and 288 forks on GitHub."
  },
  {
    "input": "What category is the arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 paper in?",
    "output": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10 is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "Which field does AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning belong to?",
    "output": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning is in the Machine Learning (cs.LG) category."
  },
  {
    "input": "What programming language is BEiT: BERT Pre-Training of Image Transformers implemented in?",
    "output": "The BEiT: BERT Pre-Training of Image Transformers implementation is written in Python."
  },
  {
    "input": "Describe the bowang-lab/MedSAM repository that implements Segment Anything in Medical Images.",
    "output": "Segment Anything in Medical Images"
  }
]