[
  {
    "paper": {
      "arxiv_id": "2505.22954",
      "title": "arXiv Query: search_query=&id_list=2505.22954&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2505.22954",
      "abstract": "Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The Gödel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin Gödel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation."
    },
    "repositories": [
      {
        "url": "https://github.com/ruixin31/Spurious_Rewards",
        "name": "ruixin31/Spurious_Rewards",
        "description": null,
        "stars": 345,
        "forks": 20,
        "language": "Python",
        "created_at": "2025-05-27T09:50:22+00:00",
        "updated_at": "2025-12-07T10:36:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2505.22101",
      "title": "arXiv Query: search_query=&id_list=2505.22101&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2505.22101",
      "abstract": "Large Language Models (LLMs) have emerged as foundational infrastructure in the pursuit of Artificial General Intelligence (AGI). Despite their remarkable capabilities in language perception and generation, current LLMs fundamentally lack a unified and structured architecture for handling memory. They primarily rely on parametric memory (knowledge encoded in model weights) and ephemeral activation memory (context-limited runtime states). While emerging methods like Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack lifecycle management and multi-modal integration, limiting their capacity for long-term knowledge evolution. To address this, we introduce MemOS, a memory operating system designed for LLMs that, for the first time, elevates memory to a first-class operational resource. It builds unified mechanisms for representation, organization, and governance across three core memory types: parametric, activation, and plaintext. At its core is the MemCube, a standardized memory abstraction that enables tracking, fusion, and migration of heterogeneous memory, while offering structured, traceable access across tasks and contexts. MemOS establishes a memory-centric execution framework with strong controllability, adaptability, and evolvability. It fills a critical gap in current LLM infrastructure and lays the groundwork for continual adaptation, personalized intelligence, and cross-platform coordination in next-generation intelligent systems."
    },
    "repositories": [
      {
        "url": "https://github.com/ruixin31/Spurious_Rewards",
        "name": "ruixin31/Spurious_Rewards",
        "description": null,
        "stars": 345,
        "forks": 20,
        "language": "Python",
        "created_at": "2025-05-27T09:50:22+00:00",
        "updated_at": "2025-12-07T10:36:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2505.23006",
      "title": "arXiv Query: search_query=&id_list=2505.23006&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2505.23006",
      "abstract": "The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents."
    },
    "repositories": [
      {
        "url": "https://github.com/ruixin31/Spurious_Rewards",
        "name": "ruixin31/Spurious_Rewards",
        "description": null,
        "stars": 345,
        "forks": 20,
        "language": "Python",
        "created_at": "2025-05-27T09:50:22+00:00",
        "updated_at": "2025-12-07T10:36:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.19413",
      "title": "arXiv Query: search_query=&id_list=2504.19413&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.19413",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents."
    },
    "repositories": [
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.20734",
      "title": "arXiv Query: search_query=&id_list=2504.20734&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.20734",
      "abstract": "Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single aggregated corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over various modality-specific and unified baselines."
    },
    "repositories": [
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/XiaomiMiMo/MiMo",
        "name": "XiaomiMiMo/MiMo",
        "description": "MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining",
        "stars": 1637,
        "forks": 68,
        "language": "Python",
        "created_at": "2025-04-26T09:31:17+00:00",
        "updated_at": "2025-12-07T09:36:24+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.21801",
      "title": "arXiv Query: search_query=&id_list=2504.21801&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.21801",
      "abstract": "We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing."
    },
    "repositories": [
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MoonshotAI/Kimi-Audio",
        "name": "MoonshotAI/Kimi-Audio",
        "description": "Kimi-Audio, an open-source audio foundation model excelling in audio understanding, generation, and conversation",
        "stars": 4372,
        "forks": 319,
        "language": "Python",
        "created_at": "2025-04-25T10:00:18+00:00",
        "updated_at": "2025-12-09T18:21:35+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/XiaomiMiMo/MiMo",
        "name": "XiaomiMiMo/MiMo",
        "description": "MiMo: Unlocking the Reasoning Potential of Language Model – From Pretraining to Posttraining",
        "stars": 1637,
        "forks": 68,
        "language": "Python",
        "created_at": "2025-04-26T09:31:17+00:00",
        "updated_at": "2025-12-07T09:36:24+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.16084",
      "title": "arXiv Query: search_query=&id_list=2504.16084&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.16084",
      "abstract": "This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 211% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the maj@n metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model maj@n, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
    },
    "repositories": [
      {
        "url": "https://github.com/Crista23/goal_directedness_llms",
        "name": "Crista23/goal_directedness_llms",
        "description": null,
        "stars": 12,
        "forks": 2,
        "language": "Python",
        "created_at": "2025-03-20T22:49:04+00:00",
        "updated_at": "2025-08-20T23:00:58+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/TIGER-AI-Lab/General-Reasoner",
        "name": "TIGER-AI-Lab/General-Reasoner",
        "description": "General Reasoner: Advancing LLM Reasoning Across All Domains [NeurIPS25]",
        "stars": 209,
        "forks": 13,
        "language": "Python",
        "created_at": "2025-04-10T03:28:44+00:00",
        "updated_at": "2025-12-01T06:21:24+00:00",
        "topics": [
          "llm",
          "reasoning",
          "rl"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.11844",
      "title": "arXiv Query: search_query=&id_list=2504.11844&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.11844",
      "abstract": "To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs."
    },
    "repositories": [
      {
        "url": "https://github.com/Crista23/goal_directedness_llms",
        "name": "Crista23/goal_directedness_llms",
        "description": null,
        "stars": 12,
        "forks": 2,
        "language": "Python",
        "created_at": "2025-03-20T22:49:04+00:00",
        "updated_at": "2025-08-20T23:00:58+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/TIGER-AI-Lab/General-Reasoner",
        "name": "TIGER-AI-Lab/General-Reasoner",
        "description": "General Reasoner: Advancing LLM Reasoning Across All Domains [NeurIPS25]",
        "stars": 209,
        "forks": 13,
        "language": "Python",
        "created_at": "2025-04-10T03:28:44+00:00",
        "updated_at": "2025-12-01T06:21:24+00:00",
        "topics": [
          "llm",
          "reasoning",
          "rl"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.01848",
      "title": "arXiv Query: search_query=&id_list=2504.01848&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.01848",
      "abstract": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We open-source our code (https://github.com/openai/preparedness) to facilitate future research in understanding the AI engineering capabilities of AI agents."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/frontier-evals",
        "name": "openai/frontier-evals",
        "description": "OpenAI Frontier Evals",
        "stars": 958,
        "forks": 111,
        "language": "Python",
        "created_at": "2025-03-28T15:45:01+00:00",
        "updated_at": "2025-12-09T10:19:37+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/allenai/codescientist",
        "name": "allenai/codescientist",
        "description": "CodeScientist: An automated scientific discovery system for code-based experiments",
        "stars": 303,
        "forks": 40,
        "language": "Python",
        "created_at": "2025-03-10T22:34:12+00:00",
        "updated_at": "2025-12-04T18:06:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.00698",
      "title": "arXiv Query: search_query=&id_list=2504.00698&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.00698",
      "abstract": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency."
    },
    "repositories": [
      {
        "url": "https://github.com/allenai/codescientist",
        "name": "allenai/codescientist",
        "description": "CodeScientist: An automated scientific discovery system for code-based experiments",
        "stars": 303,
        "forks": 40,
        "language": "Python",
        "created_at": "2025-03-10T22:34:12+00:00",
        "updated_at": "2025-12-04T18:06:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.22708",
      "title": "arXiv Query: search_query=&id_list=2503.22708&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.22708",
      "abstract": "Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries."
    },
    "repositories": [
      {
        "url": "https://github.com/allenai/codescientist",
        "name": "allenai/codescientist",
        "description": "CodeScientist: An automated scientific discovery system for code-based experiments",
        "stars": 303,
        "forks": 40,
        "language": "Python",
        "created_at": "2025-03-10T22:34:12+00:00",
        "updated_at": "2025-12-04T18:06:31+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MAXNORM8650/MedAgentSim",
        "name": "MAXNORM8650/MedAgentSim",
        "description": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)",
        "stars": 101,
        "forks": 12,
        "language": "Python",
        "created_at": "2025-01-18T06:39:48+00:00",
        "updated_at": "2025-12-09T19:28:58+00:00",
        "topics": [
          "multi-agent-simulations"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.23513",
      "title": "arXiv Query: search_query=&id_list=2503.23513&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.23513",
      "abstract": "Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts with masked losses, RARE transforms learning objectives from rote memorization to contextualized reasoning. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Extensive experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\\% accuracy. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence."
    },
    "repositories": [
      {
        "url": "https://github.com/MAXNORM8650/MedAgentSim",
        "name": "MAXNORM8650/MedAgentSim",
        "description": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)",
        "stars": 101,
        "forks": 12,
        "language": "Python",
        "created_at": "2025-01-18T06:39:48+00:00",
        "updated_at": "2025-12-09T19:28:58+00:00",
        "topics": [
          "multi-agent-simulations"
        ]
      },
      {
        "url": "https://github.com/sentient-agi/OpenDeepSearch",
        "name": "sentient-agi/OpenDeepSearch",
        "description": "SOTA search powered LLM",
        "stars": 3735,
        "forks": 343,
        "language": "Python",
        "created_at": "2025-03-20T14:29:49+00:00",
        "updated_at": "2025-12-09T15:26:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2504.02732",
      "title": "arXiv Query: search_query=&id_list=2504.02732&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2504.02732",
      "abstract": "Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training."
    },
    "repositories": [
      {
        "url": "https://github.com/MAXNORM8650/MedAgentSim",
        "name": "MAXNORM8650/MedAgentSim",
        "description": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)",
        "stars": 101,
        "forks": 12,
        "language": "Python",
        "created_at": "2025-01-18T06:39:48+00:00",
        "updated_at": "2025-12-09T19:28:58+00:00",
        "topics": [
          "multi-agent-simulations"
        ]
      },
      {
        "url": "https://github.com/sentient-agi/OpenDeepSearch",
        "name": "sentient-agi/OpenDeepSearch",
        "description": "SOTA search powered LLM",
        "stars": 3735,
        "forks": 343,
        "language": "Python",
        "created_at": "2025-03-20T14:29:49+00:00",
        "updated_at": "2025-12-09T15:26:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.22678",
      "title": "arXiv Query: search_query=&id_list=2503.22678&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.22678",
      "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM's ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \\href{https://medagentsim.netlify.app/}."
    },
    "repositories": [
      {
        "url": "https://github.com/MAXNORM8650/MedAgentSim",
        "name": "MAXNORM8650/MedAgentSim",
        "description": "MedAgentSim: Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions, MICCAI 2025 (oral and early accepted)",
        "stars": 101,
        "forks": 12,
        "language": "Python",
        "created_at": "2025-01-18T06:39:48+00:00",
        "updated_at": "2025-12-09T19:28:58+00:00",
        "topics": [
          "multi-agent-simulations"
        ]
      },
      {
        "url": "https://github.com/sentient-agi/OpenDeepSearch",
        "name": "sentient-agi/OpenDeepSearch",
        "description": "SOTA search powered LLM",
        "stars": 3735,
        "forks": 343,
        "language": "Python",
        "created_at": "2025-03-20T14:29:49+00:00",
        "updated_at": "2025-12-09T15:26:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.20201",
      "title": "arXiv Query: search_query=&id_list=2503.20201&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.20201",
      "abstract": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and OpenAI's GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES."
    },
    "repositories": [
      {
        "url": "https://github.com/sentient-agi/OpenDeepSearch",
        "name": "sentient-agi/OpenDeepSearch",
        "description": "SOTA search powered LLM",
        "stars": 3735,
        "forks": 343,
        "language": "Python",
        "created_at": "2025-03-20T14:29:49+00:00",
        "updated_at": "2025-12-09T15:26:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.01141",
      "title": "arXiv Query: search_query=&id_list=2503.01141&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.01141",
      "abstract": "Chain-of-thought prompting has emerged as a powerful technique for enabling large language models (LLMs) to solve complex reasoning tasks. However, these reasoning chains can be verbose, raising concerns about efficiency. In response, recent works have sought to decrease response lengths through simple prompting strategies (e.g. 'be concise'). In this work, we conduct the first systematic study of the relationship between reasoning length and model performance across a diverse range of compression instructions (e.g. 'use 10 words or less' or 'remove all punctuation'). In doing so, we discover a universal tradeoff between reasoning length and accuracy that persists across even very distinct reasoning chains. We demonstrate that this tradeoff emerges from a sharp threshold behavior at the question level: each task has an intrinsic 'token complexity' - a minimal number of tokens required for successful problem-solving. We show how token complexity enables us to compute information-theoretic limits on the accuracy-compression tradeoff, and find that prompt-based compression strategies operate far from these theoretical limits. This suggests there may be significant room for improvement and our framework provides a benchmark to help researchers evaluate progress in reasoning efficiency. Our work also highlights the importance of adaptive compression -- giving shorter responses for easier questions -- and we show that token complexity is a useful tool for measuring this capability."
    },
    "repositories": [
      {
        "url": "https://github.com/LTH14/fractalgen",
        "name": "LTH14/fractalgen",
        "description": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437",
        "stars": 1206,
        "forks": 66,
        "language": "Python",
        "created_at": "2025-02-18T15:00:28+00:00",
        "updated_at": "2025-12-08T12:54:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2503.00735",
      "title": "arXiv Query: search_query=&id_list=2503.00735&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2503.00735",
      "abstract": "We introduce LADDER (Learning through Autonomous Difficulty-Driven Example Recursion), a framework which enables Large Language Models to autonomously improve their problem-solving capabilities through self-guided learning by recursively generating and solving progressively simpler variants of complex problems. Unlike prior approaches that require curated datasets or human feedback, LADDER leverages a model's own capabilities to generate easier question variants. We demonstrate LADDER's effectiveness in the subject of mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to achieve 73% on the MIT Integration Bee qualifying examination. We also introduce TTRL (Test-Time Reinforcement Learning), where we perform reinforcement learning on variants of test problems at inference time. TTRL enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of 90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's performance. These results show how self-directed strategic learning can achieve significant capability improvements without relying on architectural scaling or human supervision."
    },
    "repositories": [
      {
        "url": "https://github.com/LTH14/fractalgen",
        "name": "LTH14/fractalgen",
        "description": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437",
        "stars": 1206,
        "forks": 66,
        "language": "Python",
        "created_at": "2025-02-18T15:00:28+00:00",
        "updated_at": "2025-12-08T12:54:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.19328",
      "title": "arXiv Query: search_query=&id_list=2502.19328&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.19328",
      "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling)."
    },
    "repositories": [
      {
        "url": "https://github.com/LTH14/fractalgen",
        "name": "LTH14/fractalgen",
        "description": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437",
        "stars": 1206,
        "forks": 66,
        "language": "Python",
        "created_at": "2025-02-18T15:00:28+00:00",
        "updated_at": "2025-12-08T12:54:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.17437",
      "title": "arXiv Query: search_query=&id_list=2502.17437&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.17437",
      "abstract": "Modularization is a cornerstone of computer science, abstracting complex functions into atomic building blocks. In this paper, we introduce a new level of modularization by abstracting generative models into atomic generative modules. Analogous to fractals in mathematics, our method constructs a new type of generative model by recursively invoking atomic generative modules, resulting in self-similar fractal architectures that we call fractal generative models. As a running example, we instantiate our fractal framework using autoregressive models as the atomic generative modules and examine it on the challenging task of pixel-by-pixel image generation, demonstrating strong performance in both likelihood estimation and generation quality. We hope this work could open a new paradigm in generative modeling and provide a fertile ground for future research. Code is available at https://github.com/LTH14/fractalgen."
    },
    "repositories": [
      {
        "url": "https://github.com/LTH14/fractalgen",
        "name": "LTH14/fractalgen",
        "description": "PyTorch implementation of FractalGen https://arxiv.org/abs/2502.17437",
        "stars": 1206,
        "forks": 66,
        "language": "Python",
        "created_at": "2025-02-18T15:00:28+00:00",
        "updated_at": "2025-12-08T12:54:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.09992",
      "title": "arXiv Query: search_query=&id_list=2502.09992&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.09992",
      "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong scalability and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/."
    },
    "repositories": [
      {
        "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
        "name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
        "description": "Official Repo for Open-Reasoner-Zero",
        "stars": 2074,
        "forks": 118,
        "language": "Python",
        "created_at": "2025-02-19T17:28:25+00:00",
        "updated_at": "2025-12-05T07:58:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.12115",
      "title": "arXiv Query: search_query=&id_list=2502.12115&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.12115",
      "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development."
    },
    "repositories": [
      {
        "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
        "name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
        "description": "Official Repo for Open-Reasoner-Zero",
        "stars": 2074,
        "forks": 118,
        "language": "Python",
        "created_at": "2025-02-19T17:28:25+00:00",
        "updated_at": "2025-12-05T07:58:31+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MoonshotAI/MoBA",
        "name": "MoonshotAI/MoBA",
        "description": "MoBA: Mixture of Block Attention for Long-Context LLMs",
        "stars": 2012,
        "forks": 127,
        "language": "Python",
        "created_at": "2025-02-17T13:27:30+00:00",
        "updated_at": "2025-12-09T16:54:14+00:00",
        "topics": [
          "flash-attention",
          "llm",
          "llm-serving",
          "llm-training",
          "moe",
          "pytorch",
          "transformer"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.14815",
      "title": "arXiv Query: search_query=&id_list=2502.14815&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.14815",
      "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules."
    },
    "repositories": [
      {
        "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
        "name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
        "description": "Official Repo for Open-Reasoner-Zero",
        "stars": 2074,
        "forks": 118,
        "language": "Python",
        "created_at": "2025-02-19T17:28:25+00:00",
        "updated_at": "2025-12-05T07:58:31+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/MoonshotAI/MoBA",
        "name": "MoonshotAI/MoBA",
        "description": "MoBA: Mixture of Block Attention for Long-Context LLMs",
        "stars": 2012,
        "forks": 127,
        "language": "Python",
        "created_at": "2025-02-17T13:27:30+00:00",
        "updated_at": "2025-12-09T16:54:14+00:00",
        "topics": [
          "flash-attention",
          "llm",
          "llm-serving",
          "llm-training",
          "moe",
          "pytorch",
          "transformer"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.19393",
      "title": "arXiv Query: search_query=&id_list=2501.19393&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.19393",
      "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending \"Wait\" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1"
    },
    "repositories": [
      {
        "url": "https://github.com/simplescaling/s1",
        "name": "simplescaling/s1",
        "description": "s1: Simple test-time scaling",
        "stars": 6611,
        "forks": 762,
        "language": "Python",
        "created_at": "2025-02-01T02:38:16+00:00",
        "updated_at": "2025-12-10T03:16:36+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/GAIR-NLP/LIMO",
        "name": "GAIR-NLP/LIMO",
        "description": "[COLM 2025] LIMO: Less is More for Reasoning",
        "stars": 1054,
        "forks": 52,
        "language": "Python",
        "created_at": "2025-02-04T08:37:43+00:00",
        "updated_at": "2025-12-03T21:11:22+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.01061",
      "title": "arXiv Query: search_query=&id_list=2502.01061&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.01061",
      "abstract": "End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io)"
    },
    "repositories": [
      {
        "url": "https://github.com/GAIR-NLP/LIMO",
        "name": "GAIR-NLP/LIMO",
        "description": "[COLM 2025] LIMO: Less is More for Reasoning",
        "stars": 1054,
        "forks": 52,
        "language": "Python",
        "created_at": "2025-02-04T08:37:43+00:00",
        "updated_at": "2025-12-03T21:11:22+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/kmswin1/Syntriever",
        "name": "kmswin1/Syntriever",
        "description": "\"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs\" the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), Findings, Accepted",
        "stars": 29,
        "forks": 0,
        "language": "Python",
        "created_at": "2025-01-22T23:25:57+00:00",
        "updated_at": "2025-11-17T07:37:46+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.03387",
      "title": "arXiv Query: search_query=&id_list=2502.03387&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.03387",
      "abstract": "We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\\% accuracy on AIME24 and 95.6\\% on MATH500, surpassing previous fine-tuned models (6.5\\% on AIME24, 59.2\\% on MATH500) while using only 1\\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as \"cognitive templates\" that guide reasoning."
    },
    "repositories": [
      {
        "url": "https://github.com/GAIR-NLP/LIMO",
        "name": "GAIR-NLP/LIMO",
        "description": "[COLM 2025] LIMO: Less is More for Reasoning",
        "stars": 1054,
        "forks": 52,
        "language": "Python",
        "created_at": "2025-02-04T08:37:43+00:00",
        "updated_at": "2025-12-03T21:11:22+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/kmswin1/Syntriever",
        "name": "kmswin1/Syntriever",
        "description": "\"Syntriever: How to Train Your Retriever with Synthetic Data from LLMs\" the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL), Findings, Accepted",
        "stars": 29,
        "forks": 0,
        "language": "Python",
        "created_at": "2025-01-22T23:25:57+00:00",
        "updated_at": "2025-11-17T07:37:46+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.13545",
      "title": "arXiv Query: search_query=&id_list=2501.13545&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.13545",
      "abstract": "Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements to Algorithm-of-Thoughts (AoT), which we dub AoT+, help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously."
    },
    "repositories": [
      {
        "url": "https://github.com/plurai-ai/intellagent",
        "name": "plurai-ai/intellagent",
        "description": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions",
        "stars": 1154,
        "forks": 140,
        "language": "Python",
        "created_at": "2024-10-28T20:53:08+00:00",
        "updated_at": "2025-12-06T19:16:19+00:00",
        "topics": [
          "agent",
          "evaluation",
          "llmops",
          "simulator",
          "synthetic-data"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.13824",
      "title": "arXiv Query: search_query=&id_list=2501.13824&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.13824",
      "abstract": "Hallucinations in large language models (LLMs), plausible but factually inaccurate text, are often viewed as undesirable. However, recent work suggests that such outputs may hold creative potential. In this paper, we investigate whether hallucinations can improve LLMs on molecule property prediction, a key task in early-stage drug discovery. We prompt LLMs to generate natural language descriptions from molecular SMILES strings and incorporate these often hallucinated descriptions into downstream classification tasks. Evaluating seven instruction-tuned LLMs across five datasets, we find that hallucinations significantly improve predictive accuracy for some models. Notably, Falcon3-Mamba-7B outperforms all baselines when hallucinated text is included, while hallucinations generated by GPT-4o consistently yield the greatest gains between models. We further identify and categorize over 18,000 beneficial hallucinations, with structural misdescriptions emerging as the most impactful type, suggesting that hallucinated statements about molecular structure may increase model confidence. Ablation studies show that larger models benefit more from hallucinations, while temperature has a limited effect. Our findings challenge conventional views of hallucination as purely problematic and suggest new directions for leveraging hallucinations as a useful signal in scientific modeling tasks like drug discovery."
    },
    "repositories": [
      {
        "url": "https://github.com/plurai-ai/intellagent",
        "name": "plurai-ai/intellagent",
        "description": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions",
        "stars": 1154,
        "forks": 140,
        "language": "Python",
        "created_at": "2024-10-28T20:53:08+00:00",
        "updated_at": "2025-12-06T19:16:19+00:00",
        "topics": [
          "agent",
          "evaluation",
          "llmops",
          "simulator",
          "synthetic-data"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.11067",
      "title": "arXiv Query: search_query=&id_list=2501.11067&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.11067",
      "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent"
    },
    "repositories": [
      {
        "url": "https://github.com/plurai-ai/intellagent",
        "name": "plurai-ai/intellagent",
        "description": "A framework for comprehensive diagnosis and optimization of agents using simulated, realistic synthetic interactions",
        "stars": 1154,
        "forks": 140,
        "language": "Python",
        "created_at": "2024-10-28T20:53:08+00:00",
        "updated_at": "2025-12-06T19:16:19+00:00",
        "topics": [
          "agent",
          "evaluation",
          "llmops",
          "simulator",
          "synthetic-data"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2410.12896",
      "title": "arXiv Query: search_query=&id_list=2410.12896&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2410.12896",
      "abstract": "The success of Large Language Models (LLMs) is inherently linked to the availability of vast, diverse, and high-quality data for training and evaluation. However, the growth rate of high-quality data is significantly outpaced by the expansion of training datasets, leading to a looming data exhaustion crisis. This underscores the urgent need to enhance data efficiency and explore new data sources. In this context, synthetic data has emerged as a promising solution. Currently, data generation primarily consists of two major approaches: data augmentation and synthesis. This paper comprehensively reviews and summarizes data generation techniques throughout the lifecycle of LLMs, including data preparation, pre-training, fine-tuning, instruction-tuning, preference alignment, and applications. Furthermore, We discuss the current constraints faced by these methods and investigate potential pathways for future development and research. Our aspiration is to equip researchers with a clear understanding of these methodologies, enabling them to swiftly identify appropriate data generation strategies in the construction of LLMs, while providing valuable insights for future exploration."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-3.0-language-models",
        "name": "ibm-granite/granite-3.0-language-models",
        "description": null,
        "stars": 266,
        "forks": 28,
        "language": null,
        "created_at": "2024-10-18T20:01:15+00:00",
        "updated_at": "2025-11-26T20:19:38+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2410.18050",
      "title": "arXiv Query: search_query=&id_list=2410.18050&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2410.18050",
      "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \"lost in the middle\" issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG's understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system's components and fine-tuning strategies. Data and code are available at https://github.com/QingFei1/LongRAG."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-3.0-language-models",
        "name": "ibm-granite/granite-3.0-language-models",
        "description": null,
        "stars": 266,
        "forks": 28,
        "language": null,
        "created_at": "2024-10-18T20:01:15+00:00",
        "updated_at": "2025-11-26T20:19:38+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.10209",
      "title": "arXiv Query: search_query=&id_list=2406.10209&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.10209",
      "abstract": "Large language models can memorize and repeat their training data, causing privacy and copyright risks. To mitigate memorization, we introduce a subtle modification to the next-token training objective that we call the goldfish loss. During training, randomly sampled subsets of tokens are excluded from the loss computation. These dropped tokens are not memorized by the model, which prevents verbatim reproduction of a complete chain of tokens from the training set. We run extensive experiments training billion-scale Llama-2 models, both pre-trained and trained from scratch, and demonstrate significant reductions in extractable memorization with little to no impact on downstream benchmarks."
    },
    "repositories": [
      {
        "url": "https://github.com/hpcaitech/Open-Sora",
        "name": "hpcaitech/Open-Sora",
        "description": "Open-Sora: Democratizing Efficient Video Production for All",
        "stars": 28081,
        "forks": 2804,
        "language": "Python",
        "created_at": "2024-02-20T03:01:34+00:00",
        "updated_at": "2025-12-10T01:15:01+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.07394",
      "title": "arXiv Query: search_query=&id_list=2406.07394&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.07394",
      "abstract": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications."
    },
    "repositories": [
      {
        "url": "https://github.com/hpcaitech/Open-Sora",
        "name": "hpcaitech/Open-Sora",
        "description": "Open-Sora: Democratizing Efficient Video Production for All",
        "stars": 28081,
        "forks": 2804,
        "language": "Python",
        "created_at": "2024-02-20T03:01:34+00:00",
        "updated_at": "2025-12-10T01:15:01+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.12824",
      "title": "arXiv Query: search_query=&id_list=2406.12824&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.12824",
      "abstract": "Retrieval Augmented Generation (RAG) enriches the ability of language models to reason using external context to augment responses for a given user prompt. This approach has risen in popularity due to practical applications in various applications of language models in search, question/answering, and chat-bots. However, the exact nature of how this approach works isn't clearly understood. In this paper, we mechanistically examine the RAG pipeline to highlight that language models take shortcut and have a strong bias towards utilizing only the context information to answer the question, while relying minimally on their parametric memory. We probe this mechanistic behavior in language models with: (i) Causal Mediation Analysis to show that the parametric memory is minimally utilized when answering a question and (ii) Attention Contributions and Knockouts to show that the last token residual stream do not get enriched from the subject token in the question, but gets enriched from other informative tokens in the context. We find this pronounced shortcut behaviour true across both LLaMa and Phi family of models."
    },
    "repositories": [
      {
        "url": "https://github.com/hpcaitech/Open-Sora",
        "name": "hpcaitech/Open-Sora",
        "description": "Open-Sora: Democratizing Efficient Video Production for All",
        "stars": 28081,
        "forks": 2804,
        "language": "Python",
        "created_at": "2024-02-20T03:01:34+00:00",
        "updated_at": "2025-12-10T01:15:01+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.09308",
      "title": "arXiv Query: search_query=&id_list=2406.09308&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.09308",
      "abstract": "Transformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution."
    },
    "repositories": [
      {
        "url": "https://github.com/lamini-ai/Lamini-Memory-Tuning",
        "name": "lamini-ai/Lamini-Memory-Tuning",
        "description": "Banishing LLM Hallucinations Requires Rethinking Generalization",
        "stars": 275,
        "forks": 12,
        "language": null,
        "created_at": "2024-06-13T01:52:35+00:00",
        "updated_at": "2025-10-01T01:09:50+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.06326",
      "title": "arXiv Query: search_query=&id_list=2406.06326&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.06326",
      "abstract": "Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge."
    },
    "repositories": [
      {
        "url": "https://github.com/lamini-ai/Lamini-Memory-Tuning",
        "name": "lamini-ai/Lamini-Memory-Tuning",
        "description": "Banishing LLM Hallucinations Requires Rethinking Generalization",
        "stars": 275,
        "forks": 12,
        "language": null,
        "created_at": "2024-06-13T01:52:35+00:00",
        "updated_at": "2025-10-01T01:09:50+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2406.09403",
      "title": "arXiv Query: search_query=&id_list=2406.09403&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2406.09403",
      "abstract": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment with a wide range of math tasks (including geometry, functions, graphs, and chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in https://visualsketchpad.github.io/."
    },
    "repositories": [
      {
        "url": "https://github.com/lamini-ai/Lamini-Memory-Tuning",
        "name": "lamini-ai/Lamini-Memory-Tuning",
        "description": "Banishing LLM Hallucinations Requires Rethinking Generalization",
        "stars": 275,
        "forks": 12,
        "language": null,
        "created_at": "2024-06-13T01:52:35+00:00",
        "updated_at": "2025-10-01T01:09:50+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.02803",
      "title": "arXiv Query: search_query=&id_list=2405.02803&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2405.02803",
      "abstract": "Training large-scale machine learning models poses distinct system challenges, given both the size and complexity of today's workloads. Recently, many organizations training state-of-the-art Generative AI models have reported cases of instability during training, often taking the form of loss spikes. Numeric deviation has emerged as a potential cause of this training instability, although quantifying this is especially challenging given the costly nature of training runs. In this work, we develop a principled approach to understanding the effects of numeric deviation, and construct proxies to put observations into context when downstream effects are difficult to quantify. As a case study, we apply this framework to analyze the widely-adopted Flash Attention optimization. We find that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. We then use a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-code-models",
        "name": "ibm-granite/granite-code-models",
        "description": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "stars": 1245,
        "forks": 87,
        "language": null,
        "created_at": "2024-04-23T19:23:54+00:00",
        "updated_at": "2025-12-09T03:16:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.03520",
      "title": "arXiv Query: search_query=&id_list=2405.03520&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2405.03520",
      "abstract": "General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the cornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora model has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of physical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis navigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs facilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world models, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the intricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling intelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and discuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and inspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-code-models",
        "name": "ibm-granite/granite-code-models",
        "description": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "stars": 1245,
        "forks": 87,
        "language": null,
        "created_at": "2024-04-23T19:23:54+00:00",
        "updated_at": "2025-12-09T03:16:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.03548",
      "title": "arXiv Query: search_query=&id_list=2405.03548&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2405.03548",
      "abstract": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-code-models",
        "name": "ibm-granite/granite-code-models",
        "description": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "stars": 1245,
        "forks": 87,
        "language": null,
        "created_at": "2024-04-23T19:23:54+00:00",
        "updated_at": "2025-12-09T03:16:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.04324",
      "title": "arXiv Query: search_query=&id_list=2405.04324&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2405.04324",
      "abstract": "Large Language Models (LLMs) trained on code are revolutionizing the software development process. Increasingly, code LLMs are being integrated into software development environments to improve the productivity of human programmers, and LLM-based agents are beginning to show promise for handling complex tasks autonomously. Realizing the full potential of code LLMs requires a wide range of capabilities, including code generation, fixing bugs, explaining and documenting code, maintaining repositories, and more. In this work, we introduce the Granite series of decoder-only code models for code generative tasks, trained with code written in 116 programming languages. The Granite Code models family consists of models ranging in size from 3 to 34 billion parameters, suitable for applications ranging from complex application modernization tasks to on-device memory-constrained use cases. Evaluation on a comprehensive set of tasks demonstrates that Granite Code models consistently reaches state-of-the-art performance among available open-source code LLMs. The Granite Code model family was optimized for enterprise software development workflows and performs well across a range of coding tasks (e.g. code generation, fixing and explanation), making it a versatile all around code model. We release all our Granite Code models under an Apache 2.0 license for both research and commercial use."
    },
    "repositories": [
      {
        "url": "https://github.com/ibm-granite/granite-code-models",
        "name": "ibm-granite/granite-code-models",
        "description": "Granite Code Models: A Family of Open Foundation Models for Code Intelligence",
        "stars": 1245,
        "forks": 87,
        "language": null,
        "created_at": "2024-04-23T19:23:54+00:00",
        "updated_at": "2025-12-09T03:16:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2403.17887",
      "title": "arXiv Query: search_query=&id_list=2403.17887&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2403.17887",
      "abstract": "How is knowledge stored in an LLM's weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find these unnecessary parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. Surprisingly, with this method we find minimal degradation of performance until after a large fraction (up to half) of the layers are removed for some common open-weight models. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. For our study, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single 40GB A100 GPU."
    },
    "repositories": [
      {
        "url": "https://github.com/OpenBMB/Eurus",
        "name": "OpenBMB/Eurus",
        "description": null,
        "stars": 320,
        "forks": 14,
        "language": "Python",
        "created_at": "2024-03-30T10:10:12+00:00",
        "updated_at": "2025-10-23T00:35:13+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2404.03592",
      "title": "arXiv Query: search_query=&id_list=2404.03592&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2404.03592",
      "abstract": "Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft."
    },
    "repositories": [
      {
        "url": "https://github.com/OpenBMB/Eurus",
        "name": "OpenBMB/Eurus",
        "description": null,
        "stars": 320,
        "forks": 14,
        "language": "Python",
        "created_at": "2024-03-30T10:10:12+00:00",
        "updated_at": "2025-10-23T00:35:13+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2312.02120",
      "title": "arXiv Query: search_query=&id_list=2312.02120&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2312.02120",
      "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references."
    },
    "repositories": [
      {
        "url": "https://github.com/ContextualAI/HALOs",
        "name": "ContextualAI/HALOs",
        "description": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs).",
        "stars": 894,
        "forks": 50,
        "language": "Python",
        "created_at": "2023-12-03T07:53:36+00:00",
        "updated_at": "2025-11-26T13:36:49+00:00",
        "topics": [
          "alignment",
          "dpo",
          "halos",
          "kto",
          "ppo",
          "rlhf"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2312.02783",
      "title": "arXiv Query: search_query=&id_list=2312.02783&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2312.02783",
      "abstract": "Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs."
    },
    "repositories": [
      {
        "url": "https://github.com/ContextualAI/HALOs",
        "name": "ContextualAI/HALOs",
        "description": "A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs).",
        "stars": 894,
        "forks": 50,
        "language": "Python",
        "created_at": "2023-12-03T07:53:36+00:00",
        "updated_at": "2025-11-26T13:36:49+00:00",
        "topics": [
          "alignment",
          "dpo",
          "halos",
          "kto",
          "ppo",
          "rlhf"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.08518",
      "title": "arXiv Query: search_query=&id_list=2303.08518&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.08518",
      "abstract": "Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs. Our model and code are available at https://github.com/microsoft/LMOps."
    },
    "repositories": [
      {
        "url": "https://github.com/FMInference/FlexLLMGen",
        "name": "FMInference/FlexLLMGen",
        "description": "Running large language models on a single GPU for throughput-oriented scenarios.",
        "stars": 9381,
        "forks": 588,
        "language": "Python",
        "created_at": "2023-02-15T21:18:53+00:00",
        "updated_at": "2025-12-08T20:07:56+00:00",
        "topics": [
          "deep-learning",
          "gpt-3",
          "high-throughput",
          "large-language-models",
          "machine-learning",
          "offloading",
          "opt"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.09431",
      "title": "arXiv Query: search_query=&id_list=2303.09431&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.09431",
      "abstract": "With the introduction of Neural Radiance Fields (NeRFs), novel view synthesis has recently made a big leap forward. At the core, NeRF proposes that each 3D point can emit radiance, allowing to conduct view synthesis using differentiable volumetric rendering. While neural radiance fields can accurately represent 3D scenes for computing the image rendering, 3D meshes are still the main scene representation supported by most computer graphics and simulation pipelines, enabling tasks such as real time rendering and physics-based simulations. Obtaining 3D meshes from neural radiance fields still remains an open challenge since NeRFs are optimized for view synthesis, not enforcing an accurate underlying geometry on the radiance field. We thus propose a novel compact and flexible architecture that enables easy 3D surface reconstruction from any NeRF-driven approach. Upon having trained the radiance field, we distill the volumetric 3D representation into a Signed Surface Approximation Network, allowing easy extraction of the 3D mesh and appearance. Our final 3D mesh is physically accurate and can be rendered in real time on an array of devices."
    },
    "repositories": [
      {
        "url": "https://github.com/FMInference/FlexLLMGen",
        "name": "FMInference/FlexLLMGen",
        "description": "Running large language models on a single GPU for throughput-oriented scenarios.",
        "stars": 9381,
        "forks": 588,
        "language": "Python",
        "created_at": "2023-02-15T21:18:53+00:00",
        "updated_at": "2025-12-08T20:07:56+00:00",
        "topics": [
          "deep-learning",
          "gpt-3",
          "high-throughput",
          "large-language-models",
          "machine-learning",
          "offloading",
          "opt"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.06865",
      "title": "arXiv Query: search_query=&id_list=2303.06865&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.06865",
      "abstract": "The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen"
    },
    "repositories": [
      {
        "url": "https://github.com/FMInference/FlexLLMGen",
        "name": "FMInference/FlexLLMGen",
        "description": "Running large language models on a single GPU for throughput-oriented scenarios.",
        "stars": 9381,
        "forks": 588,
        "language": "Python",
        "created_at": "2023-02-15T21:18:53+00:00",
        "updated_at": "2025-12-08T20:07:56+00:00",
        "topics": [
          "deep-learning",
          "gpt-3",
          "high-throughput",
          "large-language-models",
          "machine-learning",
          "offloading",
          "opt"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.03378",
      "title": "arXiv Query: search_query=&id_list=2303.03378&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.03378",
      "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale."
    },
    "repositories": [
      {
        "url": "https://github.com/NVlabs/prismer",
        "name": "NVlabs/prismer",
        "description": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\".",
        "stars": 1307,
        "forks": 74,
        "language": "Python",
        "created_at": "2023-03-02T00:20:34+00:00",
        "updated_at": "2025-11-19T11:31:51+00:00",
        "topics": [
          "image-captioning",
          "language-model",
          "multi-modal-learning",
          "multi-task-learning",
          "vision-language-model",
          "vision-and-language",
          "vqa"
        ]
      },
      {
        "url": "https://github.com/chenfei-wu/TaskMatrix",
        "name": "chenfei-wu/TaskMatrix",
        "description": null,
        "stars": 34341,
        "forks": 3268,
        "language": "Python",
        "created_at": "2023-03-02T09:04:28+00:00",
        "updated_at": "2025-12-09T06:48:48+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.02506",
      "title": "arXiv Query: search_query=&id_list=2303.02506&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.02506",
      "abstract": "Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of task-specific experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from multiple readily-available, pre-trained experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-arts, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer."
    },
    "repositories": [
      {
        "url": "https://github.com/NVlabs/prismer",
        "name": "NVlabs/prismer",
        "description": "The implementation of \"Prismer: A Vision-Language Model with Multi-Task Experts\".",
        "stars": 1307,
        "forks": 74,
        "language": "Python",
        "created_at": "2023-03-02T00:20:34+00:00",
        "updated_at": "2025-11-19T11:31:51+00:00",
        "topics": [
          "image-captioning",
          "language-model",
          "multi-modal-learning",
          "multi-task-learning",
          "vision-language-model",
          "vision-and-language",
          "vqa"
        ]
      },
      {
        "url": "https://github.com/chenfei-wu/TaskMatrix",
        "name": "chenfei-wu/TaskMatrix",
        "description": null,
        "stars": 34341,
        "forks": 3268,
        "language": "Python",
        "created_at": "2023-03-02T09:04:28+00:00",
        "updated_at": "2025-12-09T06:48:48+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.04671",
      "title": "arXiv Query: search_query=&id_list=2303.04671&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.04671",
      "abstract": "ChatGPT is attracting a cross-field interest as it provides a language interface with remarkable conversational competency and reasoning capabilities across many domains. However, since ChatGPT is trained with languages, it is currently not capable of processing or generating images from the visual world. At the same time, Visual Foundation Models, such as Visual Transformers or Stable Diffusion, although showing great visual understanding and generation capabilities, they are only experts on specific tasks with one-round fixed inputs and outputs. To this end, We build a system called \\textbf{Visual ChatGPT}, incorporating different Visual Foundation Models, to enable the user to interact with ChatGPT by 1) sending and receiving not only languages but also images 2) providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps. 3) providing feedback and asking for corrected results. We design a series of prompts to inject the visual model information into ChatGPT, considering models of multiple inputs/outputs and models that require visual feedback. Experiments show that Visual ChatGPT opens the door to investigating the visual roles of ChatGPT with the help of Visual Foundation Models. Our system is publicly available at \\url{https://github.com/microsoft/visual-chatgpt}."
    },
    "repositories": [
      {
        "url": "https://github.com/chenfei-wu/TaskMatrix",
        "name": "chenfei-wu/TaskMatrix",
        "description": null,
        "stars": 34341,
        "forks": 3268,
        "language": "Python",
        "created_at": "2023-03-02T09:04:28+00:00",
        "updated_at": "2025-12-09T06:48:48+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.04226",
      "title": "arXiv Query: search_query=&id_list=2303.04226&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.04226",
      "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC."
    },
    "repositories": [
      {
        "url": "https://github.com/HazyResearch/safari",
        "name": "HazyResearch/safari",
        "description": "Convolutions for Sequence Modeling",
        "stars": 906,
        "forks": 70,
        "language": "Assembly",
        "created_at": "2023-02-14T08:15:25+00:00",
        "updated_at": "2025-12-05T01:02:58+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.03846",
      "title": "arXiv Query: search_query=&id_list=2303.03846&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.03846",
      "abstract": "We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former."
    },
    "repositories": [
      {
        "url": "https://github.com/HazyResearch/safari",
        "name": "HazyResearch/safari",
        "description": "Convolutions for Sequence Modeling",
        "stars": 906,
        "forks": 70,
        "language": "Assembly",
        "created_at": "2023-02-14T08:15:25+00:00",
        "updated_at": "2025-12-05T01:02:58+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/Shark-NLP/OpenICL",
        "name": "Shark-NLP/OpenICL",
        "description": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning.",
        "stars": 582,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-25T03:32:01+00:00",
        "updated_at": "2025-12-05T13:30:56+00:00",
        "topics": [
          "in-context-learning",
          "nlp",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.04129",
      "title": "arXiv Query: search_query=&id_list=2303.04129&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.04129",
      "abstract": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field."
    },
    "repositories": [
      {
        "url": "https://github.com/HazyResearch/safari",
        "name": "HazyResearch/safari",
        "description": "Convolutions for Sequence Modeling",
        "stars": 906,
        "forks": 70,
        "language": "Assembly",
        "created_at": "2023-02-14T08:15:25+00:00",
        "updated_at": "2025-12-05T01:02:58+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/Shark-NLP/OpenICL",
        "name": "Shark-NLP/OpenICL",
        "description": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning.",
        "stars": 582,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-25T03:32:01+00:00",
        "updated_at": "2025-12-05T13:30:56+00:00",
        "topics": [
          "in-context-learning",
          "nlp",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.10866",
      "title": "arXiv Query: search_query=&id_list=2302.10866&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.10866",
      "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K."
    },
    "repositories": [
      {
        "url": "https://github.com/HazyResearch/safari",
        "name": "HazyResearch/safari",
        "description": "Convolutions for Sequence Modeling",
        "stars": 906,
        "forks": 70,
        "language": "Assembly",
        "created_at": "2023-02-14T08:15:25+00:00",
        "updated_at": "2025-12-05T01:02:58+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/Shark-NLP/OpenICL",
        "name": "Shark-NLP/OpenICL",
        "description": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning.",
        "stars": 582,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-25T03:32:01+00:00",
        "updated_at": "2025-12-05T13:30:56+00:00",
        "topics": [
          "in-context-learning",
          "nlp",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.02913",
      "title": "arXiv Query: search_query=&id_list=2303.02913&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.02913",
      "abstract": "In recent years, In-context Learning (ICL) has gained increasing attention and emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates. However, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components. To facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL is research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs. It also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research. The effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL"
    },
    "repositories": [
      {
        "url": "https://github.com/Shark-NLP/OpenICL",
        "name": "Shark-NLP/OpenICL",
        "description": "OpenICL is an open-source framework to facilitate research, development, and prototyping of in-context learning.",
        "stars": 582,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-25T03:32:01+00:00",
        "updated_at": "2025-12-05T13:30:56+00:00",
        "topics": [
          "in-context-learning",
          "nlp",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.14838",
      "title": "arXiv Query: search_query=&id_list=2302.14838&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.14838",
      "abstract": "Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design."
    },
    "repositories": [
      {
        "url": "https://github.com/ruiqi-zhong/D5",
        "name": "ruiqi-zhong/D5",
        "description": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions",
        "stars": 71,
        "forks": 13,
        "language": "Python",
        "created_at": "2023-02-24T15:39:42+00:00",
        "updated_at": "2025-09-29T23:10:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.01469",
      "title": "arXiv Query: search_query=&id_list=2303.01469&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.01469",
      "abstract": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256."
    },
    "repositories": [
      {
        "url": "https://github.com/ruiqi-zhong/D5",
        "name": "ruiqi-zhong/D5",
        "description": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions",
        "stars": 71,
        "forks": 13,
        "language": "Python",
        "created_at": "2023-02-24T15:39:42+00:00",
        "updated_at": "2025-09-29T23:10:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.14233",
      "title": "arXiv Query: search_query=&id_list=2302.14233&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.14233",
      "abstract": "Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal \"$\\textit{comparing the side effects of drug A and drug B}$\" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A \"$\\textit{mention feelings of paranoia}$\" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goals to propose more relevant, novel, and significant candidate discoveries. Finally, our system produces discoveries previously unknown to the authors on a wide range of applications in OpenD5, including temporal and demographic differences in discussion topics, political stances and stereotypes in speech, insights in commercial reviews, and error patterns in NLP models."
    },
    "repositories": [
      {
        "url": "https://github.com/ruiqi-zhong/D5",
        "name": "ruiqi-zhong/D5",
        "description": "The GitHub repo for Goal Driven Discovery of Distributional Differences via Language Descriptions",
        "stars": 71,
        "forks": 13,
        "language": "Python",
        "created_at": "2023-02-24T15:39:42+00:00",
        "updated_at": "2025-09-29T23:10:14+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/siddk/voltron-robotics",
        "name": "siddk/voltron-robotics",
        "description": "Voltron: Language-Driven Representation Learning for Robotics",
        "stars": 233,
        "forks": 24,
        "language": "Python",
        "created_at": "2023-02-25T21:34:56+00:00",
        "updated_at": "2025-12-05T06:55:48+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/siddk/voltron-evaluation",
        "name": "siddk/voltron-evaluation",
        "description": "Voltron Evaluation: Diverse Evaluation Tasks for Robotic Representation Learning",
        "stars": 37,
        "forks": 9,
        "language": "Python",
        "created_at": "2023-02-25T21:38:32+00:00",
        "updated_at": "2025-11-15T17:35:19+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.12766",
      "title": "arXiv Query: search_query=&id_list=2302.12766&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.12766",
      "abstract": "Recent work in visual representation learning for robotics demonstrates the viability of learning from large video datasets of humans performing everyday tasks. Leveraging methods such as masked autoencoding and contrastive learning, these representations exhibit strong transfer to policy learning for visuomotor control. But, robot learning encompasses a diverse set of problems beyond control including grasp affordance prediction, language-conditioned imitation learning, and intent scoring for human-robot collaboration, amongst others. First, we demonstrate that existing representations yield inconsistent results across these tasks: masked autoencoding approaches pick up on low-level spatial features at the cost of high-level semantics, while contrastive learning approaches capture the opposite. We then introduce Voltron, a framework for language-driven representation learning from human videos and associated captions. Voltron trades off language-conditioned visual reconstruction to learn low-level visual patterns, and visually-grounded language generation to encode high-level semantics. We also construct a new evaluation suite spanning five distinct robot learning problems $\\unicode{x2013}$ a unified platform for holistically evaluating visual representations for robotics. Through comprehensive, controlled experiments across all five problems, we find that Voltron's language-driven representations outperform the prior state-of-the-art, especially on targeted problems requiring higher-level features."
    },
    "repositories": [
      {
        "url": "https://github.com/siddk/voltron-robotics",
        "name": "siddk/voltron-robotics",
        "description": "Voltron: Language-Driven Representation Learning for Robotics",
        "stars": 233,
        "forks": 24,
        "language": "Python",
        "created_at": "2023-02-25T21:34:56+00:00",
        "updated_at": "2025-12-05T06:55:48+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/siddk/voltron-evaluation",
        "name": "siddk/voltron-evaluation",
        "description": "Voltron Evaluation: Diverse Evaluation Tasks for Robotic Representation Learning",
        "stars": 37,
        "forks": 9,
        "language": "Python",
        "created_at": "2023-02-25T21:38:32+00:00",
        "updated_at": "2025-11-15T17:35:19+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.09778",
      "title": "arXiv Query: search_query=&id_list=2302.09778&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.09778",
      "abstract": "Recent large-scale generative models learned on big data are capable of synthesizing incredible images yet suffer from limited controllability. This work offers a new generation paradigm that allows flexible control of the output image, such as spatial layout and palette, while maintaining the synthesis quality and model creativity. With compositionality as the core idea, we first decompose an image into representative factors, and then train a diffusion model with all these factors as the conditions to recompose the input. At the inference stage, the rich intermediate representations work as composable elements, leading to a huge design space (i.e., exponentially proportional to the number of decomposed factors) for customizable content creation. It is noteworthy that our approach, which we call Composer, supports various levels of conditions, such as text description as the global information, depth map and sketch as the local guidance, color histogram for low-level details, etc. Besides improving controllability, we confirm that Composer serves as a general framework and facilitates a wide range of classical generative tasks without retraining. Code and models will be made available."
    },
    "repositories": [
      {
        "url": "https://github.com/ali-vilab/composer",
        "name": "ali-vilab/composer",
        "description": "Official implementation of \"Composer: Creative and Controllable Image Synthesis with Composable Conditions\"",
        "stars": 1560,
        "forks": 49,
        "language": null,
        "created_at": "2023-02-22T01:03:54+00:00",
        "updated_at": "2025-11-19T11:31:38+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/tianjunz/HIR",
        "name": "tianjunz/HIR",
        "description": null,
        "stars": 159,
        "forks": 11,
        "language": "Python",
        "created_at": "2023-02-09T23:51:26+00:00",
        "updated_at": "2025-03-12T06:27:15+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/shizhediao/active-prompt",
        "name": "shizhediao/active-prompt",
        "description": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\"",
        "stars": 248,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-23T12:41:31+00:00",
        "updated_at": "2025-12-03T16:32:16+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.05206",
      "title": "arXiv Query: search_query=&id_list=2302.05206&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.05206",
      "abstract": "Reinforcement learning has seen wide success in finetuning large language models to better align with instructions via human feedback. The so-called algorithm, Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive performance on the GPT series models. However, the underlying Reinforcement Learning (RL) algorithm is complex and requires an additional training pipeline for reward and value networks. In this paper, we consider an alternative approach: converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner. Such an algorithm doesn't require any additional parameters except for the original language model and maximally reuses the pretraining pipeline. To achieve this, we formulate instruction alignment problem for language models as a goal-reaching problem in decision making. We propose Hindsight Instruction Relabeling (HIR), a novel algorithm for aligning language models with instructions. The resulting two-stage algorithm shed light to a family of reward-free approaches that utilize the hindsightly relabeled instructions based on feedback. We evaluate the performance of HIR extensively on 12 challenging BigBench reasoning tasks and show that HIR outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning."
    },
    "repositories": [
      {
        "url": "https://github.com/tianjunz/HIR",
        "name": "tianjunz/HIR",
        "description": null,
        "stars": 159,
        "forks": 11,
        "language": "Python",
        "created_at": "2023-02-09T23:51:26+00:00",
        "updated_at": "2025-03-12T06:27:15+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/shizhediao/active-prompt",
        "name": "shizhediao/active-prompt",
        "description": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\"",
        "stars": 248,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-23T12:41:31+00:00",
        "updated_at": "2025-12-03T16:32:16+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.12246",
      "title": "arXiv Query: search_query=&id_list=2302.12246&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.12246",
      "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt."
    },
    "repositories": [
      {
        "url": "https://github.com/shizhediao/active-prompt",
        "name": "shizhediao/active-prompt",
        "description": "Source code for the paper \"Active Prompting with Chain-of-Thought for Large Language Models\"",
        "stars": 248,
        "forks": 30,
        "language": "Python",
        "created_at": "2023-02-23T12:41:31+00:00",
        "updated_at": "2025-12-03T16:32:16+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.07459",
      "title": "arXiv Query: search_query=&id_list=2302.07459&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.07459",
      "abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to \"morally self-correct\" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles."
    },
    "repositories": [
      {
        "url": "https://github.com/haoliuhl/language-quantized-autoencoders",
        "name": "haoliuhl/language-quantized-autoencoders",
        "description": "Language Quantized AutoEncoders",
        "stars": 111,
        "forks": 5,
        "language": "Python",
        "created_at": "2023-02-03T05:11:48+00:00",
        "updated_at": "2025-11-17T20:27:36+00:00",
        "topics": [
          "autoencoders",
          "bert",
          "large-language-models",
          "multimodal",
          "roberta",
          "vq",
          "vqvae"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.08242",
      "title": "arXiv Query: search_query=&id_list=2302.08242&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.08242",
      "abstract": "Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks."
    },
    "repositories": [
      {
        "url": "https://github.com/haoliuhl/language-quantized-autoencoders",
        "name": "haoliuhl/language-quantized-autoencoders",
        "description": "Language Quantized AutoEncoders",
        "stars": 111,
        "forks": 5,
        "language": "Python",
        "created_at": "2023-02-03T05:11:48+00:00",
        "updated_at": "2025-11-17T20:27:36+00:00",
        "topics": [
          "autoencoders",
          "bert",
          "large-language-models",
          "multimodal",
          "roberta",
          "vq",
          "vqvae"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.00902",
      "title": "arXiv Query: search_query=&id_list=2302.00902&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.00902",
      "abstract": "Recent progress in scaling up large language models has shown impressive capabilities in performing few-shot learning across a wide range of text-based tasks. However, a key limitation is that these language models fundamentally lack visual perception - a crucial attribute needed to extend these models to be able to interact with the real world and solve vision tasks, such as in visual-question answering and robotics. Prior works have largely connected image to text through pretraining and/or fine-tuning on curated image-text datasets, which can be a costly and expensive process. In order to resolve this limitation, we propose a simple yet effective approach called Language-Quantized AutoEncoder (LQAE), a modification of VQ-VAE that learns to align text-image data in an unsupervised manner by leveraging pretrained language models (e.g., BERT, RoBERTa). Our main idea is to encode image as sequences of text tokens by directly quantizing image embeddings using a pretrained language codebook. We then apply random masking followed by a BERT model, and have the decoder reconstruct the original image from BERT predicted text token embeddings. By doing so, LQAE learns to represent similar images with similar clusters of text tokens, thereby aligning these two modalities without the use of aligned text-image pairs. This enables few-shot image classification with large language models (e.g., GPT-3) as well as linear classification of images based on BERT text features. To the best of our knowledge, our work is the first work that uses unaligned images for multimodal tasks by leveraging the power of pretrained language models."
    },
    "repositories": [
      {
        "url": "https://github.com/haoliuhl/language-quantized-autoencoders",
        "name": "haoliuhl/language-quantized-autoencoders",
        "description": "Language Quantized AutoEncoders",
        "stars": 111,
        "forks": 5,
        "language": "Python",
        "created_at": "2023-02-03T05:11:48+00:00",
        "updated_at": "2025-11-17T20:27:36+00:00",
        "topics": [
          "autoencoders",
          "bert",
          "large-language-models",
          "multimodal",
          "roberta",
          "vq",
          "vqvae"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.03011",
      "title": "arXiv Query: search_query=&id_list=2302.03011&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.03011",
      "abstract": "Text-guided generative diffusion models unlock powerful image creation and editing tools. While these have been extended to video generation, current approaches that edit the content of existing footage while retaining structure require expensive re-training for every input or rely on error-prone propagation of image edits across frames. In this work, we present a structure and content-guided video diffusion model that edits videos based on visual or textual descriptions of the desired output. Conflicts between user-provided content edits and structure representations occur due to insufficient disentanglement between the two aspects. As a solution, we show that training on monocular depth estimates with varying levels of detail provides control over structure and content fidelity. Our model is trained jointly on images and videos which also exposes explicit control of temporal consistency through a novel guidance method. Our experiments demonstrate a wide variety of successes; fine-grained control over output characteristics, customization based on a few reference images, and a strong user preference towards results by our model."
    },
    "repositories": [
      {
        "url": "https://github.com/mit-han-lab/offsite-tuning",
        "name": "mit-han-lab/offsite-tuning",
        "description": "Offsite-Tuning: Transfer Learning without Full Model",
        "stars": 383,
        "forks": 39,
        "language": "Python",
        "created_at": "2023-02-02T20:21:11+00:00",
        "updated_at": "2025-12-02T08:44:00+00:00",
        "topics": [
          "deep-learning",
          "transfer-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.04023",
      "title": "arXiv Query: search_query=&id_list=2302.04023&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.04023",
      "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn \"prompt engineering\" fashion. We also release codebase for evaluation set extraction."
    },
    "repositories": [
      {
        "url": "https://github.com/mit-han-lab/offsite-tuning",
        "name": "mit-han-lab/offsite-tuning",
        "description": "Offsite-Tuning: Transfer Learning without Full Model",
        "stars": 383,
        "forks": 39,
        "language": "Python",
        "created_at": "2023-02-02T20:21:11+00:00",
        "updated_at": "2025-12-02T08:44:00+00:00",
        "topics": [
          "deep-learning",
          "transfer-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.03917",
      "title": "arXiv Query: search_query=&id_list=2302.03917&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.03917",
      "abstract": "We introduce Noise2Music, where a series of diffusion models is trained to generate high-quality 30-second music clips from text prompts. Two types of diffusion models, a generator model, which generates an intermediate representation conditioned on text, and a cascader model, which generates high-fidelity audio conditioned on the intermediate representation and possibly the text, are trained and utilized in succession to generate high-fidelity music. We explore two options for the intermediate representation, one using a spectrogram and the other using audio with lower fidelity. We find that the generated audio is not only able to faithfully reflect key elements of the text prompt such as genre, tempo, instruments, mood, and era, but goes beyond to ground fine-grained semantics of the prompt. Pretrained large language models play a key role in this story -- they are used to generate paired text for the audio of the training set and to extract embeddings of the text prompts ingested by the diffusion models.\n  Generated examples: https://google-research.github.io/noise2music"
    },
    "repositories": [
      {
        "url": "https://github.com/mit-han-lab/offsite-tuning",
        "name": "mit-han-lab/offsite-tuning",
        "description": "Offsite-Tuning: Transfer Learning without Full Model",
        "stars": 383,
        "forks": 39,
        "language": "Python",
        "created_at": "2023-02-02T20:21:11+00:00",
        "updated_at": "2025-12-02T08:44:00+00:00",
        "topics": [
          "deep-learning",
          "transfer-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.04870",
      "title": "arXiv Query: search_query=&id_list=2302.04870&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.04870",
      "abstract": "Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning."
    },
    "repositories": [
      {
        "url": "https://github.com/mit-han-lab/offsite-tuning",
        "name": "mit-han-lab/offsite-tuning",
        "description": "Offsite-Tuning: Transfer Learning without Full Model",
        "stars": 383,
        "forks": 39,
        "language": "Python",
        "created_at": "2023-02-02T20:21:11+00:00",
        "updated_at": "2025-12-02T08:44:00+00:00",
        "topics": [
          "deep-learning",
          "transfer-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.12652",
      "title": "arXiv Query: search_query=&id_list=2301.12652&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.12652",
      "abstract": "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%."
    },
    "repositories": [
      {
        "url": "https://github.com/amazon-science/mm-cot",
        "name": "amazon-science/mm-cot",
        "description": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)",
        "stars": 3985,
        "forks": 333,
        "language": "Python",
        "created_at": "2023-02-02T06:31:32+00:00",
        "updated_at": "2025-12-09T11:08:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.13188",
      "title": "arXiv Query: search_query=&id_list=2301.13188&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.13188",
      "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
    },
    "repositories": [
      {
        "url": "https://github.com/amazon-science/mm-cot",
        "name": "amazon-science/mm-cot",
        "description": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)",
        "stars": 3985,
        "forks": 333,
        "language": "Python",
        "created_at": "2023-02-02T06:31:32+00:00",
        "updated_at": "2025-12-09T11:08:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.13688",
      "title": "arXiv Query: search_query=&id_list=2301.13688&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.13688",
      "abstract": "We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2."
    },
    "repositories": [
      {
        "url": "https://github.com/amazon-science/mm-cot",
        "name": "amazon-science/mm-cot",
        "description": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)",
        "stars": 3985,
        "forks": 333,
        "language": "Python",
        "created_at": "2023-02-02T06:31:32+00:00",
        "updated_at": "2025-12-09T11:08:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.00923",
      "title": "arXiv Query: search_query=&id_list=2302.00923&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.00923",
      "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot."
    },
    "repositories": [
      {
        "url": "https://github.com/amazon-science/mm-cot",
        "name": "amazon-science/mm-cot",
        "description": "Official implementation for \"Multimodal Chain-of-Thought Reasoning in Language Models\" (stay tuned and more will be updated)",
        "stars": 3985,
        "forks": 333,
        "language": "Python",
        "created_at": "2023-02-02T06:31:32+00:00",
        "updated_at": "2025-12-09T11:08:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.10343",
      "title": "arXiv Query: search_query=&id_list=2301.10343&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.10343",
      "abstract": "Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. ClimaX extends the Transformer architecture with novel encoding and aggregation blocks that allow effective use of available compute while maintaining general utility. ClimaX is pre-trained with a self-supervised learning objective on climate datasets derived from CMIP6. The pre-trained ClimaX can then be fine-tuned to address a breadth of climate and weather tasks, including those that involve atmospheric variables and spatio-temporal scales unseen during pretraining. Compared to existing data-driven baselines, we show that this generality in ClimaX results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets. The source code is available at https://github.com/microsoft/ClimaX."
    },
    "repositories": [
      {
        "url": "https://github.com/autonomousvision/stylegan-t",
        "name": "autonomousvision/stylegan-t",
        "description": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "stars": 1198,
        "forks": 61,
        "language": "Python",
        "created_at": "2023-01-20T15:37:36+00:00",
        "updated_at": "2025-12-03T07:49:58+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.11316",
      "title": "arXiv Query: search_query=&id_list=2301.11316&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.11316",
      "abstract": "This work formulates the machine learning mechanism as a bi-level optimization problem. The inner level optimization loop entails minimizing a properly chosen loss function evaluated on the training data. This is nothing but the well-studied training process in pursuit of optimal model parameters. The outer level optimization loop is less well-studied and involves maximizing a properly chosen performance metric evaluated on the validation data. This is what we call the \"iteration process\", pursuing optimal model hyper-parameters. Among many other degrees of freedom, this process entails model engineering (e.g., neural network architecture design) and management, experiment tracking, dataset versioning and augmentation. The iteration process could be automated via Automatic Machine Learning (AutoML) or left to the intuitions of machine learning students, engineers, and researchers. Regardless of the route we take, there is a need to reduce the computational cost of the iteration step and as a direct consequence reduce the carbon footprint of developing artificial intelligence algorithms. Despite the clean and unified mathematical formulation of the iteration step as a bi-level optimization problem, its solutions are case specific and complex. This work will consider such cases while increasing the level of complexity from supervised learning to semi-supervised, self-supervised, unsupervised, few-shot, federated, reinforcement, and physics-informed learning. As a consequence of this exercise, this proposal surfaces a plethora of open problems in the field, many of which can be addressed in parallel."
    },
    "repositories": [
      {
        "url": "https://github.com/autonomousvision/stylegan-t",
        "name": "autonomousvision/stylegan-t",
        "description": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "stars": 1198,
        "forks": 61,
        "language": "Python",
        "created_at": "2023-01-20T15:37:36+00:00",
        "updated_at": "2025-12-03T07:49:58+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.11305",
      "title": "arXiv Query: search_query=&id_list=2301.11305&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.11305",
      "abstract": "The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information."
    },
    "repositories": [
      {
        "url": "https://github.com/autonomousvision/stylegan-t",
        "name": "autonomousvision/stylegan-t",
        "description": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "stars": 1198,
        "forks": 61,
        "language": "Python",
        "created_at": "2023-01-20T15:37:36+00:00",
        "updated_at": "2025-12-03T07:49:58+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.09515",
      "title": "arXiv Query: search_query=&id_list=2301.09515&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.09515",
      "abstract": "Text-to-image synthesis has recently seen significant progress thanks to large pretrained language models, large-scale training data, and the introduction of scalable model families such as diffusion and autoregressive models. However, the best-performing models require iterative evaluation to generate a single sample. In contrast, generative adversarial networks (GANs) only need a single forward pass. They are thus much faster, but they currently remain far behind the state-of-the-art in large-scale text-to-image synthesis. This paper aims to identify the necessary steps to regain competitiveness. Our proposed model, StyleGAN-T, addresses the specific requirements of large-scale text-to-image synthesis, such as large capacity, stable training on diverse datasets, strong text alignment, and controllable variation vs. text alignment tradeoff. StyleGAN-T significantly improves over previous GANs and outperforms distilled diffusion models - the previous state-of-the-art in fast text-to-image synthesis - in terms of sample quality and speed."
    },
    "repositories": [
      {
        "url": "https://github.com/autonomousvision/stylegan-t",
        "name": "autonomousvision/stylegan-t",
        "description": "[ICML'23] StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis",
        "stars": 1198,
        "forks": 61,
        "language": "Python",
        "created_at": "2023-01-20T15:37:36+00:00",
        "updated_at": "2025-12-03T07:49:58+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.04104",
      "title": "arXiv Query: search_query=&id_list=2301.04104&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.04104",
      "abstract": "Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable."
    },
    "repositories": [
      {
        "url": "https://github.com/google-deepmind/tracr",
        "name": "google-deepmind/tracr",
        "description": null,
        "stars": 548,
        "forks": 47,
        "language": "Python",
        "created_at": "2022-12-01T09:59:18+00:00",
        "updated_at": "2025-11-21T09:43:23+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.05062",
      "title": "arXiv Query: search_query=&id_list=2301.05062&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.05062",
      "abstract": "We show how to \"compile\" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study \"superposition\" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the \"programs\" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/google-deepmind/tracr."
    },
    "repositories": [
      {
        "url": "https://github.com/google-deepmind/tracr",
        "name": "google-deepmind/tracr",
        "description": null,
        "stars": 548,
        "forks": 47,
        "language": "Python",
        "created_at": "2022-12-01T09:59:18+00:00",
        "updated_at": "2025-11-21T09:43:23+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/frankxu2004/knnlm-why",
        "name": "frankxu2004/knnlm-why",
        "description": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\"",
        "stars": 59,
        "forks": 3,
        "language": "Python",
        "created_at": "2021-10-09T02:38:44+00:00",
        "updated_at": "2025-09-11T17:48:59+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.04856",
      "title": "arXiv Query: search_query=&id_list=2301.04856&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.04856",
      "abstract": "This book is the result of a seminar in which we reviewed multimodal approaches and attempted to create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. Further, modeling frameworks are discussed where one modality is transformed into the other, as well as models in which one modality is utilized to enhance representation learning for the other. To conclude the second part, architectures with a focus on handling both modalities simultaneously are introduced. Finally, we also cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. One interesting application (Generative Art) eventually caps off this booklet."
    },
    "repositories": [
      {
        "url": "https://github.com/frankxu2004/knnlm-why",
        "name": "frankxu2004/knnlm-why",
        "description": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\"",
        "stars": 59,
        "forks": 3,
        "language": "Python",
        "created_at": "2021-10-09T02:38:44+00:00",
        "updated_at": "2025-09-11T17:48:59+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.02828",
      "title": "arXiv Query: search_query=&id_list=2301.02828&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.02828",
      "abstract": "Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why."
    },
    "repositories": [
      {
        "url": "https://github.com/frankxu2004/knnlm-why",
        "name": "frankxu2004/knnlm-why",
        "description": "Repo for ICML23 \"Why do Nearest Neighbor Language Models Work?\"",
        "stars": 59,
        "forks": 3,
        "language": "Python",
        "created_at": "2021-10-09T02:38:44+00:00",
        "updated_at": "2025-09-11T17:48:59+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.00704",
      "title": "arXiv Query: search_query=&id_list=2301.00704&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.00704",
      "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io"
    },
    "repositories": [
      {
        "url": "https://github.com/lucidrains/muse-maskgit-pytorch",
        "name": "lucidrains/muse-maskgit-pytorch",
        "description": "Implementation of Muse: Text-to-Image Generation via Masked Generative Transformers, in Pytorch",
        "stars": 919,
        "forks": 89,
        "language": "Python",
        "created_at": "2023-01-03T16:54:57+00:00",
        "updated_at": "2025-12-01T13:21:10+00:00",
        "topics": [
          "artificial-intelligence",
          "deep-learning",
          "text-to-image",
          "attention-mechanisms",
          "transformers"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.00303",
      "title": "arXiv Query: search_query=&id_list=2301.00303&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.00303",
      "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/ConvNeXt-V2",
        "name": "facebookresearch/ConvNeXt-V2",
        "description": "Code release for ConvNeXt V2 model",
        "stars": 1917,
        "forks": 157,
        "language": "Python",
        "created_at": "2022-12-29T23:40:13+00:00",
        "updated_at": "2025-12-09T07:41:03+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/JohnNay/llm-lobbyist",
        "name": "JohnNay/llm-lobbyist",
        "description": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023).",
        "stars": 172,
        "forks": 15,
        "language": "Jupyter Notebook",
        "created_at": "2023-01-03T00:38:02+00:00",
        "updated_at": "2025-11-19T11:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.00774",
      "title": "arXiv Query: search_query=&id_list=2301.00774&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.00774",
      "abstract": "We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/ConvNeXt-V2",
        "name": "facebookresearch/ConvNeXt-V2",
        "description": "Code release for ConvNeXt V2 model",
        "stars": 1917,
        "forks": 157,
        "language": "Python",
        "created_at": "2022-12-29T23:40:13+00:00",
        "updated_at": "2025-12-09T07:41:03+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/JohnNay/llm-lobbyist",
        "name": "JohnNay/llm-lobbyist",
        "description": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023).",
        "stars": 172,
        "forks": 15,
        "language": "Jupyter Notebook",
        "created_at": "2023-01-03T00:38:02+00:00",
        "updated_at": "2025-11-19T11:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.00808",
      "title": "arXiv Query: search_query=&id_list=2301.00808&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.00808",
      "abstract": "Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9% accuracy using only public training data."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/ConvNeXt-V2",
        "name": "facebookresearch/ConvNeXt-V2",
        "description": "Code release for ConvNeXt V2 model",
        "stars": 1917,
        "forks": 157,
        "language": "Python",
        "created_at": "2022-12-29T23:40:13+00:00",
        "updated_at": "2025-12-09T07:41:03+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/JohnNay/llm-lobbyist",
        "name": "JohnNay/llm-lobbyist",
        "description": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023).",
        "stars": 172,
        "forks": 15,
        "language": "Jupyter Notebook",
        "created_at": "2023-01-03T00:38:02+00:00",
        "updated_at": "2025-11-19T11:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.01181",
      "title": "arXiv Query: search_query=&id_list=2301.01181&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.01181",
      "abstract": "We demonstrate a proof-of-concept of a large language model conducting corporate lobbying related activities. An autoregressive large language model (OpenAI's text-davinci-003) determines if proposed U.S. Congressional bills are relevant to specific public companies and provides explanations and confidence levels. For the bills the model deems as relevant, the model drafts a letter to the sponsor of the bill in an attempt to persuade the congressperson to make changes to the proposed legislation. We use hundreds of novel ground-truth labels of the relevance of a bill to a company to benchmark the performance of the model. It outperforms the baseline of predicting the most common outcome of irrelevance. We also benchmark the performance of the previous OpenAI GPT-3 model (text-davinci-002), which was the state-of-the-art model on many academic natural language tasks until text-davinci-003 was recently released. The performance of text-davinci-002 is worse than the simple baseline. Longer-term, if AI begins to influence law in a manner that is not a direct extension of human intentions, this threatens the critical role that law as information could play in aligning AI with humans. Initially, AI is being used to simply augment human lobbyists for a small portion of their daily tasks. However, firms have an incentive to use less and less human oversight over automated assessments of policy ideas and the written communication to regulatory agencies and Congressional staffers. The core question raised is where to draw the line between human-driven and AI-driven policy influence."
    },
    "repositories": [
      {
        "url": "https://github.com/JohnNay/llm-lobbyist",
        "name": "JohnNay/llm-lobbyist",
        "description": "Code for the paper: \"Large Language Models as Corporate Lobbyists\" (2023).",
        "stars": 172,
        "forks": 15,
        "language": "Jupyter Notebook",
        "created_at": "2023-01-03T00:38:02+00:00",
        "updated_at": "2025-11-19T11:30:31+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/oughtinc/ice",
        "name": "oughtinc/ice",
        "description": "Interactive Composition Explorer: a debugger for compositional language model programs",
        "stars": 563,
        "forks": 70,
        "language": "Python",
        "created_at": "2022-09-16T14:11:36+00:00",
        "updated_at": "2025-11-18T15:03:56+00:00",
        "topics": [
          "debugging",
          "gpt-3",
          "python",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.01947",
      "title": "arXiv Query: search_query=&id_list=2301.01947&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.01947",
      "abstract": "We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with accuracy comparable to that of traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm. The code is available at https://github.com/steerapi/stitchnet."
    },
    "repositories": [
      {
        "url": "https://github.com/oughtinc/ice",
        "name": "oughtinc/ice",
        "description": "Interactive Composition Explorer: a debugger for compositional language model programs",
        "stars": 563,
        "forks": 70,
        "language": "Python",
        "created_at": "2022-09-16T14:11:36+00:00",
        "updated_at": "2025-11-18T15:03:56+00:00",
        "topics": [
          "debugging",
          "gpt-3",
          "python",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.01751",
      "title": "arXiv Query: search_query=&id_list=2301.01751&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2301.01751",
      "abstract": "Language models (LMs) can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state. Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive. We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs. We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc. To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs. We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines: describing the placebo used in a randomized controlled trial (25% to 65%), evaluating participant adherence to a medical intervention (53% to 70%), and answering NLP questions on the Qasper dataset (38% to 69%). These applications serve as case studies for a workflow that, if automated, could keep ML systems interpretable and safe even as they scale to increasingly complex tasks."
    },
    "repositories": [
      {
        "url": "https://github.com/oughtinc/ice",
        "name": "oughtinc/ice",
        "description": "Interactive Composition Explorer: a debugger for compositional language model programs",
        "stars": 563,
        "forks": 70,
        "language": "Python",
        "created_at": "2022-09-16T14:11:36+00:00",
        "updated_at": "2025-11-18T15:03:56+00:00",
        "topics": [
          "debugging",
          "gpt-3",
          "python",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2004.08492",
      "title": "arXiv Query: search_query=&id_list=2004.08492&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2004.08492",
      "abstract": "Time series forecasting is an active research topic in academia as well as industry. Although we see an increasing amount of adoptions of machine learning methods in solving some of those forecasting challenges, statistical methods remain powerful while dealing with low granularity data. This paper introduces a refined Bayesian exponential smoothing model with the help of probabilistic programming languages including Stan. Our model refinements include additional global trend, transformation for multiplicative form, noise distribution and choice of priors. A benchmark study is conducted on a rich set of time-series data sets for our models along with other well-known time series models."
    },
    "repositories": [
      {
        "url": "https://github.com/uber/orbit",
        "name": "uber/orbit",
        "description": "A Python package for Bayesian forecasting with object-oriented design and probabilistic models under the hood.",
        "stars": 2021,
        "forks": 141,
        "language": "Python",
        "created_at": "2020-01-07T18:20:37+00:00",
        "updated_at": "2025-12-03T07:00:46+00:00",
        "topics": [
          "python",
          "forecasting",
          "bayesian",
          "exponential-smoothing",
          "pyro",
          "stan",
          "pystan",
          "probabilistic-programming",
          "probabilistic",
          "forecast",
          "orbit",
          "time-series",
          "regression",
          "arima",
          "changepoint",
          "bayesian-methods",
          "bayesian-statistics",
          "machine-learning",
          "regression-models",
          "pytorch"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2007.16122",
      "title": "arXiv Query: search_query=&id_list=2007.16122&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2007.16122",
      "abstract": "Multi-stage cascade architecture exists widely in many industrial systems such as recommender systems and online advertising, which often consists of sequential modules including matching, pre-ranking, ranking, etc. For a long time, it is believed pre-ranking is just a simplified version of the ranking module, considering the larger size of the candidate set to be ranked. Thus, efforts are made mostly on simplifying ranking model to handle the explosion of computing power for online inference. In this paper, we rethink the challenge of the pre-ranking system from an algorithm-system co-design view. Instead of saving computing power with restriction of model architecture which causes loss of model performance, here we design a new pre-ranking system by joint optimization of both the pre-ranking model and the computing power it costs. We name it COLD (Computing power cost-aware Online and Lightweight Deep pre-ranking system). COLD beats SOTA in three folds: (i) an arbitrary deep model with cross features can be applied in COLD under a constraint of controllable computing power cost. (ii) computing power cost is explicitly reduced by applying optimization tricks for inference acceleration. This further brings space for COLD to apply more complex deep models to reach better performance. (iii) COLD model works in an online learning and severing manner, bringing it excellent ability to handle the challenge of the data distribution shift. Meanwhile, the fully online pre-ranking system of COLD provides us with a flexible infrastructure that supports efficient new model developing and online A/B testing.Since 2019, COLD has been deployed in almost all products involving the pre-ranking module in the display advertising system in Alibaba, bringing significant improvements."
    },
    "repositories": [
      {
        "url": "https://github.com/linkedin/gdmix",
        "name": "linkedin/gdmix",
        "description": "A deep ranking personalization framework",
        "stars": 133,
        "forks": 18,
        "language": "Python",
        "created_at": "2020-06-03T16:06:57+00:00",
        "updated_at": "2025-08-09T14:54:54+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2004.13637",
      "title": "arXiv Query: search_query=&id_list=2004.13637&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2004.13637",
      "abstract": "Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent persona. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/TransCoder",
        "name": "facebookresearch/TransCoder",
        "description": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf",
        "stars": 1724,
        "forks": 263,
        "language": "Python",
        "created_at": "2020-07-10T10:58:19+00:00",
        "updated_at": "2025-12-10T00:46:39+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2006.03511",
      "title": "arXiv Query: search_query=&id_list=2006.03511&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2006.03511",
      "abstract": "A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/TransCoder",
        "name": "facebookresearch/TransCoder",
        "description": "Public release of the TransCoder research project https://arxiv.org/pdf/2006.03511.pdf",
        "stars": 1724,
        "forks": 263,
        "language": "Python",
        "created_at": "2020-07-10T10:58:19+00:00",
        "updated_at": "2025-12-10T00:46:39+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2006.16779",
      "title": "arXiv Query: search_query=&id_list=2006.16779&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2006.16779",
      "abstract": "To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generative model augmented with latent variables and an evaluation model are further trained to generate diverse responses and to select the best response, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/pegasus",
        "name": "google-research/pegasus",
        "description": null,
        "stars": 1645,
        "forks": 317,
        "language": "Python",
        "created_at": "2020-03-25T04:18:20+00:00",
        "updated_at": "2025-11-28T14:55:22+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/salesforce/GeDi",
        "name": "salesforce/GeDi",
        "description": "GeDi: Generative Discriminator Guided Sequence Generation",
        "stars": 210,
        "forks": 45,
        "language": "Python",
        "created_at": "2020-09-02T22:47:22+00:00",
        "updated_at": "2025-11-26T02:34:50+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2009.06367",
      "title": "arXiv Query: search_query=&id_list=2009.06367&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2009.06367",
      "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed."
    },
    "repositories": [
      {
        "url": "https://github.com/salesforce/GeDi",
        "name": "salesforce/GeDi",
        "description": "GeDi: Generative Discriminator Guided Sequence Generation",
        "stars": 210,
        "forks": 45,
        "language": "Python",
        "created_at": "2020-09-02T22:47:22+00:00",
        "updated_at": "2025-11-26T02:34:50+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/facebookresearch/dynalab",
        "name": "facebookresearch/dynalab",
        "description": "The Python library with command line tools to interact with Dynabench(https://dynabench.org/), such as uploading models. ",
        "stars": 55,
        "forks": 10,
        "language": "Python",
        "created_at": "2020-11-17T13:43:25+00:00",
        "updated_at": "2024-11-10T04:05:32+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2102.01192",
      "title": "arXiv Query: search_query=&id_list=2102.01192&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2102.01192",
      "abstract": "We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2104.00355",
      "title": "arXiv Query: search_query=&id_list=2104.00355&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2104.00355",
      "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2109.03264",
      "title": "arXiv Query: search_query=&id_list=2109.03264&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2109.03264",
      "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2012.06009",
      "title": "arXiv Query: search_query=&id_list=2012.06009&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2012.06009",
      "abstract": "Different from shopping in physical stores, where people have the opportunity to closely check a product (e.g., touching the surface of a T-shirt or smelling the scent of perfume) before making a purchase decision, online shoppers rely greatly on the uploaded product images to make any purchase decision. The decision-making is challenging when selling or purchasing second-hand items online since estimating the items' prices is not trivial. In this work, we present a vision-based price suggestion system for the online second-hand item shopping platform. The goal of vision-based price suggestion is to help sellers set effective prices for their second-hand listings with the images uploaded to the online platforms.\n  First, we propose to better extract representative visual features from the images with the aid of some other image-based item information (e.g., category, brand). Then, we design a vision-based price suggestion module which takes the extracted visual features along with some statistical item features from the shopping platform as the inputs to determine whether an uploaded item image is qualified for price suggestion by a binary classification model, and provide price suggestions for items with qualified images by a regression model. According to two demands from the platform, two different objective functions are proposed to jointly optimize the classification model and the regression model. For better model training, we also propose a warm-up training strategy for the joint optimization. Extensive experiments on a large real-world dataset demonstrate the effectiveness of our vision-based price prediction system."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/CovidPrognosis",
        "name": "facebookresearch/CovidPrognosis",
        "description": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations",
        "stars": 158,
        "forks": 38,
        "language": "Python",
        "created_at": "2020-12-10T20:04:03+00:00",
        "updated_at": "2025-08-09T14:51:02+00:00",
        "topics": [
          "medical-imaging",
          "deep-learning",
          "radiography",
          "x-ray",
          "pytorch",
          "covid-19",
          "medical-image-analysis"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2101.04909",
      "title": "arXiv Query: search_query=&id_list=2101.04909&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2101.04909",
      "abstract": "The rapid spread of COVID-19 cases in recent months has strained hospital resources, making rapid and accurate triage of patients presenting to emergency departments a necessity. Machine learning techniques using clinical data such as chest X-rays have been used to predict which patients are most at risk of deterioration. We consider the task of predicting two types of patient deterioration based on chest X-rays: adverse event deterioration (i.e., transfer to the intensive care unit, intubation, or mortality) and increased oxygen requirements beyond 6 L per day. Due to the relative scarcity of COVID-19 patient data, existing solutions leverage supervised pretraining on related non-COVID images, but this is limited by the differences between the pretraining data and the target COVID-19 patient data. In this paper, we use self-supervised learning based on the momentum contrast (MoCo) method in the pretraining phase to learn more general image representations to use for downstream tasks. We present three results. The first is deterioration prediction from a single image, where our model achieves an area under receiver operating characteristic curve (AUC) of 0.742 for predicting an adverse event within 96 hours (compared to 0.703 with supervised pretraining) and an AUC of 0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours (compared to 0.749 with supervised pretraining). We then propose a new transformer-based architecture that can process sequences of multiple images for prediction and show that this model can achieve an improved AUC of 0.786 for predicting an adverse event at 96 hours and an AUC of 0.848 for predicting mortalities at 96 hours. A small pilot clinical study suggested that the prediction accuracy of our model is comparable to that of experienced radiologists analyzing the same information."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/CovidPrognosis",
        "name": "facebookresearch/CovidPrognosis",
        "description": "COVID deterioration prediction based on chest X-ray radiographs via MoCo-trained image representations",
        "stars": 158,
        "forks": 38,
        "language": "Python",
        "created_at": "2020-12-10T20:04:03+00:00",
        "updated_at": "2025-08-09T14:51:02+00:00",
        "topics": [
          "medical-imaging",
          "deep-learning",
          "radiography",
          "x-ray",
          "pytorch",
          "covid-19",
          "medical-image-analysis"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2107.07346",
      "title": "arXiv Query: search_query=&id_list=2107.07346&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2107.07346",
      "abstract": "We argue that immature data pipelines are preventing a large portion of industry practitioners from leveraging the latest research on recommender systems. We propose our template data stack for machine learning at \"reasonable scale\", and show how many challenges are solved by embracing a serverless paradigm. Leveraging our experience, we detail how modern open source can provide a pipeline processing terabytes of data with limited infrastructure work."
    },
    "repositories": [
      {
        "url": "https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat",
        "name": "jacopotagliabue/you-dont-need-a-bigger-boat",
        "description": "An end-to-end implementation of intent prediction with Metaflow and other cool tools",
        "stars": 873,
        "forks": 66,
        "language": "Python",
        "created_at": "2021-05-26T14:52:01+00:00",
        "updated_at": "2025-11-23T06:48:43+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.18290",
      "title": "arXiv Query: search_query=&id_list=2305.18290&start=0&max_results=10",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2305.18290",
      "abstract": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train."
    },
    "repositories": [
      {
        "url": "https://github.com/volcengine/verl",
        "name": "volcengine/verl",
        "description": "verl: Volcano Engine Reinforcement Learning for LLMs",
        "stars": 17344,
        "forks": 2782,
        "language": "Python",
        "created_at": "2024-10-31T06:11:15+00:00",
        "updated_at": "2025-12-10T03:19:07+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/OpenRLHF/OpenRLHF",
        "name": "OpenRLHF/OpenRLHF",
        "description": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agentic RL)",
        "stars": 8562,
        "forks": 826,
        "language": "Python",
        "created_at": "2023-07-30T02:20:13+00:00",
        "updated_at": "2025-12-10T03:10:22+00:00",
        "topics": [
          "transformers",
          "vllm",
          "large-language-models",
          "raylib",
          "reinforcement-learning-from-human-feedback",
          "reinforcement-learning",
          "openai-o1",
          "proximal-policy-optimization"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2210.17323",
      "title": "arXiv Query: search_query=&id_list=2210.17323&start=0&max_results=10",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2210.17323",
      "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels. We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq."
    },
    "repositories": [
      {
        "url": "https://github.com/turboderp-org/exllamav2",
        "name": "turboderp-org/exllamav2",
        "description": "A fast inference library for running LLMs locally on modern consumer-class GPUs",
        "stars": 4380,
        "forks": 325,
        "language": "Python",
        "created_at": "2023-08-30T08:54:22+00:00",
        "updated_at": "2025-12-09T20:14:29+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2306.00978",
      "title": "arXiv Query: search_query=&id_list=2306.00978&start=0&max_results=10",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2306.00978",
      "abstract": "Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs."
    },
    "repositories": [
      {
        "url": "https://github.com/turboderp-org/exllamav2",
        "name": "turboderp-org/exllamav2",
        "description": "A fast inference library for running LLMs locally on modern consumer-class GPUs",
        "stars": 4380,
        "forks": 325,
        "language": "Python",
        "created_at": "2023-08-30T08:54:22+00:00",
        "updated_at": "2025-12-09T20:14:29+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2311.03099",
      "title": "arXiv Query: search_query=&id_list=2311.03099&start=0&max_results=10",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2311.03099",
      "abstract": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
    },
    "repositories": [
      {
        "url": "https://github.com/arcee-ai/mergekit",
        "name": "arcee-ai/mergekit",
        "description": "Tools for merging pretrained large language models.",
        "stars": 6564,
        "forks": 644,
        "language": "Python",
        "created_at": "2023-08-21T03:50:04+00:00",
        "updated_at": "2025-12-10T02:26:35+00:00",
        "topics": [
          "llama",
          "llm",
          "model-merging"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.01068",
      "title": "arXiv Query: search_query=&id_list=2205.01068&start=0&max_results=10",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2205.01068",
      "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    "repositories": [
      {
        "url": "https://github.com/google/BIG-bench",
        "name": "google/BIG-bench",
        "description": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models",
        "stars": 3167,
        "forks": 616,
        "language": "Python",
        "created_at": "2021-01-15T23:28:20+00:00",
        "updated_at": "2025-12-08T20:41:21+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.05131",
      "title": "arXiv Query: search_query=&id_list=2205.05131&start=0&max_results=10",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2205.05131",
      "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized & unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 & GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B."
    },
    "repositories": [
      {
        "url": "https://github.com/google/BIG-bench",
        "name": "google/BIG-bench",
        "description": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models",
        "stars": 3167,
        "forks": 616,
        "language": "Python",
        "created_at": "2021-01-15T23:28:20+00:00",
        "updated_at": "2025-12-08T20:41:21+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.04434",
      "title": "arXiv Query: search_query=&id_list=2405.04434&start=0&max_results=10",
      "year": 2024,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2405.04434",
      "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models."
    },
    "repositories": [
      {
        "url": "https://github.com/deepseek-ai/DeepSeek-Coder-V2",
        "name": "deepseek-ai/DeepSeek-Coder-V2",
        "description": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
        "stars": 6281,
        "forks": 1015,
        "language": null,
        "created_at": "2024-06-14T03:39:37+00:00",
        "updated_at": "2025-12-09T16:41:44+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/deepseek-ai/DeepSeek-V3",
        "name": "deepseek-ai/DeepSeek-V3",
        "description": null,
        "stars": 100660,
        "forks": 16401,
        "language": "Python",
        "created_at": "2024-12-26T09:52:40+00:00",
        "updated_at": "2025-12-10T02:43:45+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2012.00413",
      "title": "arXiv Query: search_query=&id_list=2012.00413&start=0&max_results=10",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2012.00413",
      "abstract": "Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3, with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many NLP tasks in the settings of few-shot (even zero-shot) learning. The code and parameters are available at https://github.com/TsinghuaAI/CPM-Generate."
    },
    "repositories": [
      {
        "url": "https://github.com/TsinghuaAI/CPM-1-Generate",
        "name": "TsinghuaAI/CPM-1-Generate",
        "description": "Chinese Pre-Trained Language Models (CPM-LM) Version-I",
        "stars": 1582,
        "forks": 211,
        "language": "Python",
        "created_at": "2020-11-13T10:20:32+00:00",
        "updated_at": "2025-12-01T10:57:23+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/thunlp/OpenCLaP",
        "name": "thunlp/OpenCLaP",
        "description": "Open Chinese Language Pre-trained Model Zoo",
        "stars": 988,
        "forks": 147,
        "language": null,
        "created_at": "2019-07-01T02:23:06+00:00",
        "updated_at": "2025-10-10T07:10:57+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2502.10391",
      "title": "arXiv Query: search_query=&id_list=2502.10391&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2502.10391",
      "abstract": "Despite notable advancements in Multimodal Large Language Models (MLLMs), most state-of-the-art models have not undergone thorough alignment with human preferences. This gap exists because current alignment research has primarily achieved progress in specific areas (e.g., hallucination reduction), while the broader question of whether aligning models with human preferences can systematically enhance MLLM capability remains largely unexplored. To this end, we introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained, human-annotated preference comparison pairs. This dataset represents a substantial advancement over existing resources, offering superior size, diversity, annotation granularity, and quality. Leveraging this dataset, we propose several key innovations to improve both the quality of reward models and the efficiency of alignment algorithms. Notably, we introduce a Critique-Based Reward Model, which generates critiques of model outputs before assigning scores, offering enhanced interpretability and more informative feedback compared to traditional scalar reward mechanisms. Additionally, we propose Dynamic Reward Scaling, a method that adjusts the loss weight of each sample according to the reward signal, thereby optimizing the use of high-quality comparison pairs. Our approach is rigorously evaluated across $\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results demonstrating significant and consistent improvements in model performance. Specifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm leads to a $\\mathbf{19.5}$% increase in conversational abilities and a $\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and evaluation code, as well as reward modeling and safety benchmarks. For more details, please visit our project page: https://mm-rlhf.github.io."
    },
    "repositories": [
      {
        "url": "https://github.com/Kwai-YuanQi/MM-RLHF",
        "name": "Kwai-YuanQi/MM-RLHF",
        "description": "The Next Step Forward in Multimodal LLM Alignment",
        "stars": 189,
        "forks": 8,
        "language": "Python",
        "created_at": "2025-02-16T09:15:02+00:00",
        "updated_at": "2025-12-02T00:21:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.12895",
      "title": "arXiv Query: search_query=&id_list=2501.12895&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.12895",
      "abstract": "Large language models (LLMs) demonstrate impressive performance but lack the flexibility to adapt to human preferences quickly without retraining. In this work, we introduce Test-time Preference Optimization (TPO), a framework that aligns LLM outputs with human preferences during inference, removing the need to update model parameters. Rather than relying on purely numerical rewards, TPO translates reward signals into textual critiques and uses them as textual rewards to iteratively refine its response. Evaluations on benchmarks covering instruction following, preference alignment, safety, and mathematics reveal that TPO progressively improves alignment with human preferences. Notably, after only a few TPO steps, the initially unaligned Llama-3.1-70B-SFT model can surpass the aligned counterpart, Llama-3.1-70B-Instruct. Furthermore, TPO scales efficiently with both the search width and depth during inference. Through case studies, we illustrate how TPO exploits the innate capacity of LLM to interpret and act upon reward signals. Our findings establish TPO as a practical, lightweight alternative for test-time preference optimization, achieving alignment on the fly. Our code is publicly available at https://github.com/yafuly/TPO."
    },
    "repositories": [
      {
        "url": "https://github.com/yafuly/TPO",
        "name": "yafuly/TPO",
        "description": "Test-time preferenece optimization (ICML 2025).",
        "stars": 172,
        "forks": 11,
        "language": "Jupyter Notebook",
        "created_at": "2025-01-14T02:50:41+00:00",
        "updated_at": "2025-12-04T05:28:36+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2501.03262",
      "title": "arXiv Query: search_query=&id_list=2501.03262&start=0&max_results=10",
      "year": 2025,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2501.03262",
      "abstract": "Reinforcement Learning from Human Feedback~(RLHF) plays a crucial role in aligning Large Language Models~(LLMs). The dominant algorithm, Proximal Policy Optimization~(PPO), employs a critic network to estimate advantages, which introduces significant computational and memory overhead. To address this, a family of critic-free algorithms (e.g., GRPO, RLOO) has emerged. However, these methods typically rely on \\textit{prompt-level (local)} advantage normalization, which suffers from inaccurate advantage estimation, a tendency to overfit, and, as we show, is a theoretically biased estimator. To solve these challenges, we introduce REINFORCE++, a critic-free framework centered on \\textbf{Global Advantage Normalization}. By normalizing advantages across the entire global batch rather than small, prompt-specific groups, our method provides a more stable and theoretically sound, \\textit{effectively unbiased} estimate (whose bias vanishes as batch size increases). We introduce two variants: REINFORCE++, a highly efficient and general algorithm ($k \\ge 1$) for general-domain RLHF, and REINFORCE++ /w baseline, a robust group-sampling variant ($k > 1$) for complex reasoning tasks. Our empirical evaluation demonstrates that each variant shows superior stability and performance in its respective domain, outperforming existing methods and even PPO in complex agentic settings."
    },
    "repositories": [
      {
        "url": "https://github.com/OpenRLHF/OpenRLHF",
        "name": "OpenRLHF/OpenRLHF",
        "description": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agentic RL)",
        "stars": 8562,
        "forks": 826,
        "language": "Python",
        "created_at": "2023-07-30T02:20:13+00:00",
        "updated_at": "2025-12-10T03:10:22+00:00",
        "topics": [
          "transformers",
          "vllm",
          "large-language-models",
          "raylib",
          "reinforcement-learning-from-human-feedback",
          "reinforcement-learning",
          "openai-o1",
          "proximal-policy-optimization"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2412.15838",
      "title": "arXiv Query: search_query=&id_list=2412.15838&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2412.15838",
      "abstract": "Reinforcement learning from human feedback (RLHF) has proven effective in enhancing the instruction-following capabilities of large language models; however, it remains underexplored in the cross-modality domain. As the number of modalities increases, aligning all-modality models with human intentions -- such as instruction following -- becomes a pressing challenge. In this work, we make the first attempt to fine-tune all-modality models (i.e. input and output with any modality, also named any-to-any models) using human preference data across all modalities (including text, image, audio, and video), ensuring its behavior aligns with human intentions. This endeavor presents several challenges. First, there is no large-scale all-modality human preference data in existing open-source resources, as most datasets are limited to specific modalities, predominantly text and image. Secondly, the effectiveness of binary preferences in RLHF for post-training alignment in complex all-modality scenarios remains an unexplored area. Finally, there is a lack of a systematic framework to evaluate the capabilities of all-modality models, particularly regarding modality selection and synergy. To address these challenges, we propose the align-anything framework, which includes meticulously annotated 200k all-modality human preference data. Then, we introduce an alignment method that learns from unified language feedback, effectively capturing complex modality-specific human preferences and enhancing the model's instruction-following capabilities. Furthermore, to assess performance improvements in all-modality models after post-training alignment, we construct a challenging all-modality capability evaluation framework -- eval-anything. All data, models, and code frameworks have been open-sourced for the community. For more details, please refer to https://github.com/PKU-Alignment/align-anything."
    },
    "repositories": [
      {
        "url": "https://github.com/PKU-Alignment/align-anything",
        "name": "PKU-Alignment/align-anything",
        "description": "Align Anything: Training All-modality Model with Feedback",
        "stars": 4601,
        "forks": 506,
        "language": "Python",
        "created_at": "2024-07-14T11:05:19+00:00",
        "updated_at": "2025-12-09T13:35:08+00:00",
        "topics": [
          "large-language-models",
          "multimodal",
          "rlhf",
          "chameleon",
          "dpo",
          "vision-language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2409.19256",
      "title": "arXiv Query: search_query=&id_list=2409.19256&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2409.19256",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF dataflow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53$\\times$~20.57$\\times$ throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code will be available at https://github.com/volcengine/verl."
    },
    "repositories": [
      {
        "url": "https://github.com/volcengine/verl",
        "name": "volcengine/verl",
        "description": "verl: Volcano Engine Reinforcement Learning for LLMs",
        "stars": 17344,
        "forks": 2782,
        "language": "Python",
        "created_at": "2024-10-31T06:11:15+00:00",
        "updated_at": "2025-12-10T03:19:07+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2309.14525",
      "title": "arXiv Query: search_query=&id_list=2309.14525&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2309.14525",
      "abstract": "Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in \"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io."
    },
    "repositories": [
      {
        "url": "https://github.com/llava-rlhf/LLaVA-RLHF",
        "name": "llava-rlhf/LLaVA-RLHF",
        "description": "Aligning LMMs with Factually Augmented RLHF",
        "stars": 387,
        "forks": 26,
        "language": "Python",
        "created_at": "2023-09-27T03:01:15+00:00",
        "updated_at": "2025-12-05T08:58:44+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2402.18571",
      "title": "arXiv Query: search_query=&id_list=2402.18571&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2402.18571",
      "abstract": "Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO)."
    },
    "repositories": [
      {
        "url": "https://github.com/RLHFlow/Directional-Preference-Alignment",
        "name": "RLHFlow/Directional-Preference-Alignment",
        "description": "Directional Preference Alignment",
        "stars": 58,
        "forks": 3,
        "language": null,
        "created_at": "2024-02-27T02:51:26+00:00",
        "updated_at": "2025-11-19T11:41:43+00:00",
        "topics": [
          "rlhf",
          "ai-alignment",
          "large-language-models"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2405.07863",
      "title": "arXiv Query: search_query=&id_list=2405.07863&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2405.07863",
      "abstract": "We present the workflow of Online Iterative Reinforcement Learning from Human Feedback (RLHF) in this technical report, which is widely reported to outperform its offline counterpart by a large margin in the recent large language model (LLM) literature. However, existing open-source RLHF projects are still largely confined to the offline learning setting. In this technical report, we aim to fill in this gap and provide a detailed recipe that is easy to reproduce for online iterative RLHF. In particular, since online human feedback is usually infeasible for open-source communities with limited resources, we start by constructing preference models using a diverse set of open-source datasets and use the constructed proxy preference model to approximate human feedback. Then, we discuss the theoretical insights and algorithmic principles behind online iterative RLHF, followed by a detailed practical implementation. Our trained LLM achieves impressive performance on LLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as well as other academic benchmarks such as HumanEval and TruthfulQA. We have shown that supervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performance with fully open-source datasets. Further, we have made our models, curated datasets, and comprehensive step-by-step code guidebooks publicly available. Please refer to https://github.com/RLHFlow/RLHF-Reward-Modeling and https://github.com/RLHFlow/Online-RLHF for more detailed information."
    },
    "repositories": [
      {
        "url": "https://github.com/RLHFlow/Online-RLHF",
        "name": "RLHFlow/Online-RLHF",
        "description": "A recipe for online RLHF and online iterative DPO.",
        "stars": 537,
        "forks": 49,
        "language": "Python",
        "created_at": "2024-05-10T14:33:50+00:00",
        "updated_at": "2025-11-22T23:17:40+00:00",
        "topics": [
          "llm",
          "rlhf",
          "llama3"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2401.01335",
      "title": "arXiv Query: search_query=&id_list=2401.01335&start=0&max_results=10",
      "year": 2024,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2401.01335",
      "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achieved only when the LLM policy aligns with the target data distribution. Empirically, we evaluate our method on several benchmark datasets including the HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our results show that SPIN can significantly improve the LLM's performance across a variety of benchmarks and even outperform models trained through direct preference optimization (DPO) supplemented with extra GPT-4 preference data. This sheds light on the promise of self-play, enabling the achievement of human-level performance in LLMs without the need for expert opponents. Codes are available at https://github.com/uclaml/SPIN."
    },
    "repositories": [
      {
        "url": "https://github.com/uclaml/SPIN",
        "name": "uclaml/SPIN",
        "description": "The official implementation of Self-Play Fine-Tuning (SPIN)",
        "stars": 1226,
        "forks": 104,
        "language": "Python",
        "created_at": "2024-02-04T21:43:19+00:00",
        "updated_at": "2025-12-05T08:29:20+00:00",
        "topics": [
          "deep-learning",
          "fine-tuning",
          "large-language-models",
          "self-play"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2310.03708",
      "title": "arXiv Query: search_query=&id_list=2310.03708&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2310.03708",
      "abstract": "A single language model, even when aligned with labelers through reinforcement learning from human feedback (RLHF), may not suit all human preferences. Recent approaches therefore prefer customization, gathering multi-dimensional feedback, and creating distinct reward models for each dimension. Different language models are then optimized for various preferences using multi-objective RLHF (MORLHF) with varying reward weights. However, RL fine-tuning is unstable and resource-heavy, especially with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free extension of Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO folds language modeling directly into reward modeling, training language models as implicit collective reward models that combine all objectives with specific weights. MODPO theoretically yields the same optimal solutions as MORLHF but is practically more stable and efficient. Empirical results in safety alignment and long-form question answering show that MODPO matches or outperforms existing methods, producing a Pareto front of language models catering to diverse preferences with three times less computational resources compared to MORLHF. Code is available at https://github.com/ZHZisZZ/modpo."
    },
    "repositories": [
      {
        "url": "https://github.com/ZHZisZZ/modpo",
        "name": "ZHZisZZ/modpo",
        "description": "[ACL'24] Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization",
        "stars": 93,
        "forks": 7,
        "language": "Python",
        "created_at": "2024-03-10T14:26:21+00:00",
        "updated_at": "2025-11-28T14:34:19+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2306.01693",
      "title": "arXiv Query: search_query=&id_list=2306.01693&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2306.01693",
      "abstract": "Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io."
    },
    "repositories": [
      {
        "url": "https://github.com/allenai/FineGrainedRLHF",
        "name": "allenai/FineGrainedRLHF",
        "description": null,
        "stars": 281,
        "forks": 22,
        "language": "Python",
        "created_at": "2023-05-26T06:09:40+00:00",
        "updated_at": "2025-12-02T08:03:47+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2311.08045",
      "title": "arXiv Query: search_query=&id_list=2311.08045&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2311.08045",
      "abstract": "Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing alignment methods depend on manually annotated preference data to guide the LLM optimization directions. However, continuously updating LLMs for alignment raises a distribution gap between model-generated samples and human-annotated responses, hindering training effectiveness. To mitigate this issue, previous methods require additional preference annotation on newly generated samples to adapt to the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an Adversarial Preference Optimization (APO) framework, in which the LLM and the reward model update alternatively via a min-max game. Through adversarial training, the reward model can adapt to the shifted generation distribution of the LLM without any additional annotation. With comprehensive experiments, we find the proposed adversarial training framework further enhances existing alignment baselines in terms of LLM helpfulness and harmlessness. The code is at https://github.com/Linear95/APO."
    },
    "repositories": [
      {
        "url": "https://github.com/Linear95/APO",
        "name": "Linear95/APO",
        "description": "Code for ACL2024 paper - Adversarial Preference Optimization (APO).",
        "stars": 56,
        "forks": 3,
        "language": "Python",
        "created_at": "2023-11-20T07:21:03+00:00",
        "updated_at": "2025-11-19T04:15:07+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2306.17492",
      "title": "arXiv Query: search_query=&id_list=2306.17492&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2306.17492",
      "abstract": "Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pair-wise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automatic-based, reward-based, GPT-4, and human evaluations."
    },
    "repositories": [
      {
        "url": "https://github.com/AlibabaResearch/DAMO-ConvAI",
        "name": "AlibabaResearch/DAMO-ConvAI",
        "description": "DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.",
        "stars": 1508,
        "forks": 236,
        "language": "Python",
        "created_at": "2022-07-15T09:48:59+00:00",
        "updated_at": "2025-12-09T07:25:11+00:00",
        "topics": [
          "conversational-ai",
          "deep-learning",
          "natural-language-processing",
          "dialog"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.06767",
      "title": "arXiv Query: search_query=&id_list=2304.06767&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2304.06767",
      "abstract": "Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially serious consequences. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) to address this problem, where generative models are fine-tuned with RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generative models effectively. Utilizing a reward model and a sufficient number of samples, our approach selects the high-quality samples, discarding those that exhibit undesired behavior, and subsequently enhancing the model by fine-tuning on these filtered samples. Our studies show that RAFT can effectively improve the model performance in both reward learning and other automated metrics in both large language models and diffusion models."
    },
    "repositories": [
      {
        "url": "https://github.com/OptimalScale/LMFlow",
        "name": "OptimalScale/LMFlow",
        "description": "An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All.",
        "stars": 8489,
        "forks": 834,
        "language": "Python",
        "created_at": "2023-03-27T13:56:29+00:00",
        "updated_at": "2025-12-08T05:33:36+00:00",
        "topics": [
          "chatgpt",
          "deep-learning",
          "instruction-following",
          "language-model",
          "pretrained-models",
          "pytorch",
          "transformer"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.05302",
      "title": "arXiv Query: search_query=&id_list=2304.05302&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2304.05302",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). However, PPO is sensitive to hyperparameters and requires multiple models in its standard implementation, making it hard to train and scale up to larger parameter counts. In contrast, we propose a novel learning paradigm called RRHF, which scores sampled responses from different sources via a logarithm of conditional probabilities and learns to align these probabilities with human preferences through ranking loss. RRHF can leverage sampled responses from various sources including the model responses from itself, other large language model responses, and human expert responses to learn to rank them. RRHF only needs 1 to 2 models during tuning and can efficiently align language models with human preferences robustly without complex hyperparameter tuning. Additionally, RRHF can be considered an extension of SFT and reward model training while being simpler than PPO in terms of coding, model counts, and hyperparameters. We evaluate RRHF on the Helpful and Harmless dataset, demonstrating comparable alignment performance with PPO by reward model score and human labeling. Extensive experiments show that the performance of RRHF is highly related to sampling quality which suggests RRHF is a best-of-n learner. Codes available at https://github.com/GanjinZero/RRHF."
    },
    "repositories": [
      {
        "url": "https://github.com/GanjinZero/RRHF",
        "name": "GanjinZero/RRHF",
        "description": "[NIPS2023] RRHF & Wombat",
        "stars": 811,
        "forks": 45,
        "language": "Python",
        "created_at": "2023-04-10T08:21:45+00:00",
        "updated_at": "2025-10-04T17:42:52+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.14420",
      "title": "arXiv Query: search_query=&id_list=2303.14420&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2303.14420",
      "abstract": "Recent years have witnessed a rapid growth of deep generative models, with text-to-image models gaining significant attention from the public. However, existing models often generate images that do not align well with human preferences, such as awkward combinations of limbs and facial expressions. To address this issue, we collect a dataset of human choices on generated images from the Stable Foundation Discord channel. Our experiments demonstrate that current evaluation metrics for generative models do not correlate well with human choices. Thus, we train a human preference classifier with the collected dataset and derive a Human Preference Score (HPS) based on the classifier. Using HPS, we propose a simple yet effective method to adapt Stable Diffusion to better align with human preferences. Our experiments show that HPS outperforms CLIP in predicting human choices and has good generalization capability toward images generated from other models. By tuning Stable Diffusion with the guidance of HPS, the adapted model is able to generate images that are more preferred by human users. The project page is available here: https://tgxs002.github.io/align_sd_web/ ."
    },
    "repositories": [
      {
        "url": "https://github.com/tgxs002/align_sd",
        "name": "tgxs002/align_sd",
        "description": "Better Aligning Text-to-Image Models with Human Preference. ICCV 2023",
        "stars": 293,
        "forks": 10,
        "language": "Python",
        "created_at": "2023-03-23T14:09:34+00:00",
        "updated_at": "2025-11-27T08:07:50+00:00",
        "topics": [
          "stable-diffusion"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.05977",
      "title": "arXiv Query: search_query=&id_list=2304.05977&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2304.05977",
      "abstract": "We present a comprehensive solution to learn and improve text-to-image models from human preference feedback. To begin with, we build ImageReward -- the first general-purpose text-to-image human preference reward model -- to effectively encode human preferences. Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date. In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis. On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer. Both automatic and human evaluation support ReFL's advantages over compared methods. All code and datasets are provided at \\url{https://github.com/THUDM/ImageReward}."
    },
    "repositories": [
      {
        "url": "https://github.com/zai-org/ImageReward",
        "name": "zai-org/ImageReward",
        "description": "[NeurIPS 2023] ImageReward: Learning and Evaluating Human Preferences for Text-to-image Generation",
        "stars": 1594,
        "forks": 83,
        "language": "Python",
        "created_at": "2023-04-01T09:04:17+00:00",
        "updated_at": "2025-12-09T08:14:01+00:00",
        "topics": [
          "diffusion-models",
          "generative-model",
          "rlhf",
          "human-preferences"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.08582",
      "title": "arXiv Query: search_query=&id_list=2302.08582&start=0&max_results=10",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.08582",
      "abstract": "Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training."
    },
    "repositories": [
      {
        "url": "https://github.com/tomekkorbak/pretraining-with-human-feedback",
        "name": "tomekkorbak/pretraining-with-human-feedback",
        "description": "Code accompanying the paper Pretraining Language Models with Human Preferences",
        "stars": 180,
        "forks": 13,
        "language": "Python",
        "created_at": "2023-02-20T16:16:20+00:00",
        "updated_at": "2025-10-11T23:25:52+00:00",
        "topics": [
          "ai-alignment",
          "ai-safety",
          "decision-transformers",
          "gpt",
          "language-models",
          "pretraining",
          "reinforcement-learning",
          "rlhf"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2210.01241",
      "title": "arXiv Query: search_query=&id_list=2210.01241&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2210.01241",
      "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations."
    },
    "repositories": [
      {
        "url": "https://github.com/allenai/RL4LMs",
        "name": "allenai/RL4LMs",
        "description": "A modular RL library to fine-tune language models to human preferences",
        "stars": 2372,
        "forks": 203,
        "language": "Python",
        "created_at": "2022-08-18T05:29:16+00:00",
        "updated_at": "2025-12-10T03:04:29+00:00",
        "topics": [
          "language-modeling",
          "nlp",
          "reinforcement-learning",
          "dialogue-generation",
          "machine-translation",
          "natural-language-processing",
          "summarization",
          "table-to-text",
          "text-generation"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2209.14375",
      "title": "arXiv Query: search_query=&id_list=2209.14375&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2209.14375",
      "abstract": "We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases."
    },
    "repositories": [
      {
        "url": "https://github.com/nyu-mll/quality",
        "name": "nyu-mll/quality",
        "description": null,
        "stars": 144,
        "forks": 10,
        "language": "Python",
        "created_at": "2021-12-13T05:31:26+00:00",
        "updated_at": "2025-12-10T02:45:49+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/uclanlp/corefBias",
        "name": "uclanlp/corefBias",
        "description": "To analyze and remove gender bias in coreference resolution systems",
        "stars": 78,
        "forks": 25,
        "language": "CSS",
        "created_at": "2018-03-01T19:57:07+00:00",
        "updated_at": "2025-11-18T19:55:37+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/nyu-mll/BBQ",
        "name": "nyu-mll/BBQ",
        "description": "Repository for the Bias Benchmark for QA dataset.",
        "stars": 133,
        "forks": 33,
        "language": "Python",
        "created_at": "2021-10-14T21:01:14+00:00",
        "updated_at": "2025-12-07T22:51:19+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2209.07858",
      "title": "arXiv Query: search_query=&id_list=2209.07858&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2209.07858",
      "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models."
    },
    "repositories": [
      {
        "url": "https://github.com/anthropics/hh-rlhf",
        "name": "anthropics/hh-rlhf",
        "description": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\"",
        "stars": 1801,
        "forks": 150,
        "language": null,
        "created_at": "2022-04-10T18:14:24+00:00",
        "updated_at": "2025-12-06T00:41:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2208.02294",
      "title": "arXiv Query: search_query=&id_list=2208.02294&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2208.02294",
      "abstract": "Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans \"in the wild\" remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/bert",
        "name": "google-research/bert",
        "description": "TensorFlow code and pre-trained models for BERT",
        "stars": 39724,
        "forks": 9710,
        "language": "Python",
        "created_at": "2018-10-25T22:57:34+00:00",
        "updated_at": "2025-12-09T21:17:31+00:00",
        "topics": [
          "nlp",
          "google",
          "natural-language-processing",
          "natural-language-understanding",
          "tensorflow"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.13636",
      "title": "arXiv Query: search_query=&id_list=2205.13636&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2205.13636",
      "abstract": "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives."
    },
    "repositories": [
      {
        "url": "https://github.com/GXimingLu/Quark",
        "name": "GXimingLu/Quark",
        "description": null,
        "stars": 75,
        "forks": 10,
        "language": "Python",
        "created_at": "2022-10-12T19:17:20+00:00",
        "updated_at": "2025-03-20T03:14:11+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2204.05862",
      "title": "arXiv Query: search_query=&id_list=2204.05862&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2204.05862",
      "abstract": "We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work."
    },
    "repositories": [
      {
        "url": "https://github.com/anthropics/hh-rlhf",
        "name": "anthropics/hh-rlhf",
        "description": "Human preference data for \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\"",
        "stars": 1801,
        "forks": 150,
        "language": null,
        "created_at": "2022-04-10T18:14:24+00:00",
        "updated_at": "2025-12-06T00:41:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2203.11147",
      "title": "arXiv Query: search_query=&id_list=2203.11147&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2203.11147",
      "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train \"open-book\" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true."
    },
    "repositories": [
      {
        "url": "https://github.com/nyu-mll/quality",
        "name": "nyu-mll/quality",
        "description": null,
        "stars": 144,
        "forks": 10,
        "language": "Python",
        "created_at": "2021-12-13T05:31:26+00:00",
        "updated_at": "2025-12-10T02:45:49+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/sylinrl/TruthfulQA",
        "name": "sylinrl/TruthfulQA",
        "description": "TruthfulQA: Measuring How Models Imitate Human Falsehoods",
        "stars": 854,
        "forks": 107,
        "language": "Jupyter Notebook",
        "created_at": "2021-08-24T23:23:02+00:00",
        "updated_at": "2025-12-08T20:20:39+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2203.02155",
      "title": "arXiv Query: search_query=&id_list=2203.02155&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2203.02155",
      "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/following-instructions-human-feedback",
        "name": "openai/following-instructions-human-feedback",
        "description": null,
        "stars": 1251,
        "forks": 146,
        "language": null,
        "created_at": "2022-01-25T01:15:00+00:00",
        "updated_at": "2025-12-09T03:18:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.08073",
      "title": "arXiv Query: search_query=&id_list=2212.08073&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2212.08073",
      "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels."
    },
    "repositories": [
      {
        "url": "https://github.com/anthropics/ConstitutionalHarmlessnessPaper",
        "name": "anthropics/ConstitutionalHarmlessnessPaper",
        "description": null,
        "stars": 249,
        "forks": 24,
        "language": null,
        "created_at": "2022-10-14T02:10:05+00:00",
        "updated_at": "2025-12-09T19:49:20+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.09251",
      "title": "arXiv Query: search_query=&id_list=2212.09251&start=0&max_results=10",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2212.09251",
      "abstract": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer (\"sycophancy\") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors."
    },
    "repositories": [
      {
        "url": "https://github.com/anthropics/evals",
        "name": "anthropics/evals",
        "description": null,
        "stars": 315,
        "forks": 33,
        "language": null,
        "created_at": "2022-12-12T22:41:18+00:00",
        "updated_at": "2025-12-07T18:29:32+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2112.09332",
      "title": "arXiv Query: search_query=&id_list=2112.09332&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2112.09332",
      "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit."
    },
    "repositories": [
      {
        "url": "https://github.com/sylinrl/TruthfulQA",
        "name": "sylinrl/TruthfulQA",
        "description": "TruthfulQA: Measuring How Models Imitate Human Falsehoods",
        "stars": 854,
        "forks": 107,
        "language": "Jupyter Notebook",
        "created_at": "2021-08-24T23:23:02+00:00",
        "updated_at": "2025-12-08T20:20:39+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2109.10862",
      "title": "arXiv Query: search_query=&id_list=2109.10862&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2109.10862",
      "abstract": "A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\\sim5\\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model."
    },
    "repositories": [
      {
        "url": "https://github.com/salesforce/booksum",
        "name": "salesforce/booksum",
        "description": null,
        "stars": 195,
        "forks": 35,
        "language": "Python",
        "created_at": "2021-05-10T21:15:04+00:00",
        "updated_at": "2025-11-21T14:45:11+00:00",
        "topics": []
      },
      {
        "url": "https://github.com/google-deepmind/narrativeqa",
        "name": "google-deepmind/narrativeqa",
        "description": "This repository contains the NarrativeQA dataset. It includes the list of documents with Wikipedia summaries, links to full stories, and questions and answers.",
        "stars": 495,
        "forks": 67,
        "language": "Shell",
        "created_at": "2017-12-20T14:39:57+00:00",
        "updated_at": "2025-12-10T02:45:41+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2106.05091",
      "title": "arXiv Query: search_query=&id_list=2106.05091&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2106.05091",
      "abstract": "Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions."
    },
    "repositories": [
      {
        "url": "https://github.com/rll-research/BPref",
        "name": "rll-research/BPref",
        "description": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments.",
        "stars": 133,
        "forks": 31,
        "language": "Python",
        "created_at": "2021-10-22T17:49:16+00:00",
        "updated_at": "2025-12-08T00:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2111.03026",
      "title": "arXiv Query: search_query=&id_list=2111.03026&start=0&max_results=10",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2111.03026",
      "abstract": "Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref."
    },
    "repositories": [
      {
        "url": "https://github.com/rll-research/BPref",
        "name": "rll-research/BPref",
        "description": "Official codebase for \"B-Pref: Benchmarking Preference-BasedReinforcement Learning\" contains scripts to reproduce experiments.",
        "stars": 133,
        "forks": 31,
        "language": "Python",
        "created_at": "2021-10-22T17:49:16+00:00",
        "updated_at": "2025-12-08T00:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2009.01325",
      "title": "arXiv Query: search_query=&id_list=2009.01325&start=0&max_results=10",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2009.01325",
      "abstract": "As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/summarize-from-feedback",
        "name": "openai/summarize-from-feedback",
        "description": "Code for \"Learning to summarize from human feedback\"",
        "stars": 1056,
        "forks": 151,
        "language": "Python",
        "created_at": "2020-09-02T17:34:05+00:00",
        "updated_at": "2025-12-04T03:47:10+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1706.03762",
      "title": "Attention Is All You Need",
      "year": 2017,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
    },
    "repositories": [
      {
        "url": "https://github.com/tensorflow/tensor2tensor",
        "name": "tensorflow/tensor2tensor",
        "description": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.",
        "stars": 16800,
        "forks": 3698,
        "language": "Python",
        "created_at": "2017-06-15T16:57:39+00:00",
        "updated_at": "2025-12-09T22:58:06+00:00",
        "topics": [
          "machine-learning",
          "machine-translation",
          "deep-learning",
          "reinforcement-learning",
          "tpu"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1810.04805",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers",
      "year": 2018,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/bert",
        "name": "google-research/bert",
        "description": "TensorFlow code and pre-trained models for BERT",
        "stars": 39724,
        "forks": 9710,
        "language": "Python",
        "created_at": "2018-10-25T22:57:34+00:00",
        "updated_at": "2025-12-09T21:17:31+00:00",
        "topics": [
          "nlp",
          "google",
          "natural-language-processing",
          "natural-language-understanding",
          "tensorflow"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1907.11692",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1907.11692",
      "abstract": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1910.13461",
      "title": "ALBERT: A Lite BERT",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1910.13461",
      "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/albert",
        "name": "google-research/albert",
        "description": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
        "stars": 3275,
        "forks": 573,
        "language": "Python",
        "created_at": "2019-11-26T22:23:13+00:00",
        "updated_at": "2025-12-08T16:44:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1909.11942",
      "title": "ELECTRA: Pre-training Text Encoders as Discriminators",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1909.11942",
      "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/electra",
        "name": "google-research/electra",
        "description": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators",
        "stars": 2365,
        "forks": 349,
        "language": "Python",
        "created_at": "2020-03-10T03:42:50+00:00",
        "updated_at": "2025-11-24T12:13:26+00:00",
        "topics": [
          "nlp",
          "deep-learning",
          "tensorflow"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1910.10683",
      "title": "T5: Exploring the Limits of Transfer Learning",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1910.10683",
      "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/text-to-text-transfer-transformer",
        "name": "google-research/text-to-text-transfer-transformer",
        "description": "Code for the paper \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\"",
        "stars": 6458,
        "forks": 788,
        "language": "Python",
        "created_at": "2019-10-17T21:45:14+00:00",
        "updated_at": "2025-12-07T18:26:33+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1906.08237",
      "title": "XLNet: Generalized Autoregressive Pretraining",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1906.08237",
      "abstract": "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking."
    },
    "repositories": [
      {
        "url": "https://github.com/zihangdai/xlnet",
        "name": "zihangdai/xlnet",
        "description": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
        "stars": 6181,
        "forks": 1165,
        "language": "Python",
        "created_at": "2019-06-19T08:16:46+00:00",
        "updated_at": "2025-12-08T07:34:58+00:00",
        "topics": [
          "tensorflow",
          "nlp",
          "deep-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2104.09864",
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2104.09864",
      "abstract": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}."
    },
    "repositories": [
      {
        "url": "https://github.com/ZhuiyiTechnology/roformer",
        "name": "ZhuiyiTechnology/roformer",
        "description": "Rotary Transformer",
        "stars": 1059,
        "forks": 59,
        "language": "Python",
        "created_at": "2021-03-22T10:27:06+00:00",
        "updated_at": "2025-12-08T22:40:39+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1901.02860",
      "title": "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1901.02860",
      "abstract": "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
    },
    "repositories": [
      {
        "url": "https://github.com/kimiyoung/transformer-xl",
        "name": "kimiyoung/transformer-xl",
        "description": null,
        "stars": 3680,
        "forks": 765,
        "language": "Python",
        "created_at": "2019-01-08T12:20:24+00:00",
        "updated_at": "2025-11-27T05:58:07+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2002.05202",
      "title": "Reformer: The Efficient Transformer",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2002.05202",
      "abstract": "Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations."
    },
    "repositories": [
      {
        "url": "https://github.com/google/trax",
        "name": "google/trax",
        "description": "Trax — Deep Learning with Clear Code and Speed",
        "stars": 8294,
        "forks": 827,
        "language": "Python",
        "created_at": "2019-10-05T15:09:14+00:00",
        "updated_at": "2025-12-08T16:44:36+00:00",
        "topics": [
          "jax",
          "numpy",
          "deep-learning",
          "deep-reinforcement-learning",
          "machine-learning",
          "transformer",
          "reinforcement-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2006.04768",
      "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2006.04768",
      "abstract": "Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/DeBERTa",
        "name": "microsoft/DeBERTa",
        "description": "The implementation of DeBERTa",
        "stars": 2177,
        "forks": 241,
        "language": "Python",
        "created_at": "2020-06-08T15:57:14+00:00",
        "updated_at": "2025-12-06T23:11:06+00:00",
        "topics": [
          "bert",
          "deeplearning",
          "representation-learning",
          "roberta",
          "language-model",
          "natural-language-understanding",
          "self-attention",
          "transformer-encoder"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1906.04341",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1906.04341",
      "abstract": "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention."
    },
    "repositories": [
      {
        "url": "https://github.com/huggingface/sentence-transformers",
        "name": "huggingface/sentence-transformers",
        "description": "State-of-the-Art Text Embeddings",
        "stars": 17978,
        "forks": 2715,
        "language": "Python",
        "created_at": "2019-07-24T10:53:51+00:00",
        "updated_at": "2025-12-10T03:08:17+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2005.14165",
      "title": "GPT-3: Language Models are Few-Shot Learners",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2005.14165",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/gpt-3",
        "name": "openai/gpt-3",
        "description": "GPT-3: Language Models are Few-Shot Learners",
        "stars": 15774,
        "forks": 2290,
        "language": null,
        "created_at": "2020-05-18T08:03:50+00:00",
        "updated_at": "2025-12-10T01:17:06+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2307.09288",
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2307.09288",
      "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs."
    },
    "repositories": [
      {
        "url": "https://github.com/meta-llama/llama",
        "name": "meta-llama/llama",
        "description": "Inference code for Llama models",
        "stars": 58976,
        "forks": 9812,
        "language": "Python",
        "created_at": "2023-02-14T09:29:12+00:00",
        "updated_at": "2025-12-09T21:12:17+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.13971",
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2302.13971",
      "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community."
    },
    "repositories": [
      {
        "url": "https://github.com/meta-llama/llama",
        "name": "meta-llama/llama",
        "description": "Inference code for Llama models",
        "stars": 58976,
        "forks": 9812,
        "language": "Python",
        "created_at": "2023-02-14T09:29:12+00:00",
        "updated_at": "2025-12-09T21:12:17+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2310.06825",
      "title": "Mistral 7B",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2310.06825",
      "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license."
    },
    "repositories": [
      {
        "url": "https://github.com/mistralai/mistral-inference",
        "name": "mistralai/mistral-inference",
        "description": "Official inference library for Mistral models",
        "stars": 10577,
        "forks": 994,
        "language": "Jupyter Notebook",
        "created_at": "2023-09-27T13:05:24+00:00",
        "updated_at": "2025-12-10T02:37:52+00:00",
        "topics": [
          "llm",
          "llm-inference",
          "mistralai"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2112.11446",
      "title": "FLAN: Finetuned Language Models Are Zero-Shot Learners",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2112.11446",
      "abstract": "Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/FLAN",
        "name": "google-research/FLAN",
        "description": null,
        "stars": 1556,
        "forks": 160,
        "language": "Python",
        "created_at": "2021-08-21T20:12:17+00:00",
        "updated_at": "2025-12-09T01:46:23+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2204.02311",
      "title": "PaLM: Scaling Language Modeling with Pathways",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2204.02311",
      "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
    },
    "repositories": [
      {
        "url": "https://github.com/lucidrains/PaLM-pytorch",
        "name": "lucidrains/PaLM-pytorch",
        "description": "Implementation of the specific Transformer architecture from PaLM - Scaling Language Modeling with Pathways",
        "stars": 827,
        "forks": 82,
        "language": "Python",
        "created_at": "2022-04-04T19:35:12+00:00",
        "updated_at": "2025-12-05T03:48:09+00:00",
        "topics": [
          "deep-learning",
          "transformers",
          "attention-mechanism",
          "artificial-general-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.15004",
      "title": "Vicuna: An Open-Source Chatbot",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2304.15004",
      "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models."
    },
    "repositories": [
      {
        "url": "https://github.com/lm-sys/FastChat",
        "name": "lm-sys/FastChat",
        "description": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.",
        "stars": 39298,
        "forks": 4778,
        "language": "Python",
        "created_at": "2023-03-19T00:18:02+00:00",
        "updated_at": "2025-12-09T21:52:34+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2106.09685",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2106.09685",
      "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/LoRA",
        "name": "microsoft/LoRA",
        "description": "Code for loralib, an implementation of \"LoRA: Low-Rank Adaptation of Large Language Models\"",
        "stars": 13038,
        "forks": 866,
        "language": "Python",
        "created_at": "2021-06-18T02:16:35+00:00",
        "updated_at": "2025-12-09T20:54:48+00:00",
        "topics": [
          "gpt-2",
          "adaptation",
          "language-model",
          "gpt-3",
          "low-rank",
          "pytorch",
          "deep-learning",
          "roberta",
          "deberta",
          "lora"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.14314",
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2305.14314",
      "abstract": "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."
    },
    "repositories": [
      {
        "url": "https://github.com/artidoro/qlora",
        "name": "artidoro/qlora",
        "description": "QLoRA: Efficient Finetuning of Quantized LLMs",
        "stars": 10782,
        "forks": 866,
        "language": "Jupyter Notebook",
        "created_at": "2023-05-11T09:30:23+00:00",
        "updated_at": "2025-12-07T03:13:19+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2104.08691",
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2104.08691",
      "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning."
    },
    "repositories": [
      {
        "url": "https://github.com/XiangLi1999/PrefixTuning",
        "name": "XiangLi1999/PrefixTuning",
        "description": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
        "stars": 956,
        "forks": 164,
        "language": "Python",
        "created_at": "2021-01-21T19:59:56+00:00",
        "updated_at": "2025-12-08T20:24:36+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2101.00190",
      "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2101.00190",
      "abstract": "Fine-tuning is the de facto way to leverage large pretrained language models to perform downstream tasks. However, it modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Prefix-tuning draws inspiration from prompting, allowing subsequent tokens to attend to this prefix as if it were \"virtual tokens\". We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We find that by learning only 0.1\\% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics unseen during training."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/prompt-tuning",
        "name": "google-research/prompt-tuning",
        "description": "Original Implementation of Prompt Tuning from Lester, et al, 2021",
        "stars": 699,
        "forks": 61,
        "language": "Python",
        "created_at": "2021-09-07T13:21:08+00:00",
        "updated_at": "2025-12-07T01:43:14+00:00",
        "topics": [
          "prompt-tuning",
          "nlp",
          "machine-learning",
          "jax",
          "flax",
          "language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2110.07602",
      "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2110.07602",
      "abstract": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2."
    },
    "repositories": [
      {
        "url": "https://github.com/THUDM/P-tuning-v2",
        "name": "THUDM/P-tuning-v2",
        "description": "An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks",
        "stars": 2069,
        "forks": 205,
        "language": "Python",
        "created_at": "2021-10-14T14:16:05+00:00",
        "updated_at": "2025-12-03T23:03:28+00:00",
        "topics": [
          "natural-language-processing",
          "prompt-tuning",
          "pretrained-language-model",
          "p-tuning",
          "parameter-efficient-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.05638",
      "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2205.05638",
      "abstract": "Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (PEFT) (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and PEFT and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new PEFT method called (IA)$^3$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6% absolute. All of the code used in our experiments is publicly available."
    },
    "repositories": [
      {
        "url": "https://github.com/QingruZhang/AdaLoRA",
        "name": "QingruZhang/AdaLoRA",
        "description": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning (ICLR 2023). ",
        "stars": 362,
        "forks": 37,
        "language": "Python",
        "created_at": "2023-05-31T01:17:08+00:00",
        "updated_at": "2025-12-08T01:39:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.10512",
      "title": "LLaMA-Adapter: Efficient Fine-tuning of Language Models",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2303.10512",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA ."
    },
    "repositories": [
      {
        "url": "https://github.com/OpenGVLab/LLaMA-Adapter",
        "name": "OpenGVLab/LLaMA-Adapter",
        "description": "[ICLR 2024] Fine-tuning LLaMA to follow Instructions within 1 Hour and 1.2M Parameters",
        "stars": 5924,
        "forks": 384,
        "language": "Python",
        "created_at": "2023-03-19T15:08:12+00:00",
        "updated_at": "2025-12-09T12:12:20+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2005.11401",
      "title": "RAG: Retrieval-Augmented Generation",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2005.11401",
      "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/DPR",
        "name": "facebookresearch/DPR",
        "description": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task.",
        "stars": 1851,
        "forks": 316,
        "language": "Python",
        "created_at": "2020-05-13T01:13:13+00:00",
        "updated_at": "2025-12-09T09:17:35+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2004.04906",
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2004.04906",
      "abstract": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/DPR",
        "name": "facebookresearch/DPR",
        "description": "Dense Passage Retriever - is a set of tools and models for open domain Q&A task.",
        "stars": 1851,
        "forks": 316,
        "language": "Python",
        "created_at": "2020-05-13T01:13:13+00:00",
        "updated_at": "2025-12-09T09:17:35+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2112.04426",
      "title": "ColBERT: Efficient Passage Search via Contextualized Late Interaction",
      "year": 2021,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/2112.04426",
      "abstract": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
    },
    "repositories": [
      {
        "url": "https://github.com/stanford-futuredata/ColBERT",
        "name": "stanford-futuredata/ColBERT",
        "description": "ColBERT: state-of-the-art neural search (SIGIR'20, TACL'21, NeurIPS'21, NAACL'22, CIKM'22, ACL'23, EMNLP'23)",
        "stars": 3724,
        "forks": 462,
        "language": "Python",
        "created_at": "2020-05-25T18:01:18+00:00",
        "updated_at": "2025-12-10T03:15:25+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.12755",
      "title": "Beir: A Heterogeneous Benchmark for IR",
      "year": 2022,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/2205.12755",
      "abstract": "Multitask learning assumes that models capable of learning from multiple tasks can achieve better quality and efficiency via knowledge transfer, a key feature of human learning. Though, state of the art ML models rely on high customization for each task and leverage size and data scale rather than scaling the number of tasks. Also, continual learning, that adds the temporal aspect to multitask, is often focused to the study of common pitfalls such as catastrophic forgetting instead of being studied at a large scale as a critical component to build the next generation artificial intelligence.We propose an evolutionary method capable of generating large scale multitask models that support the dynamic addition of new tasks. The generated multitask models are sparsely activated and integrates a task-based routing that guarantees bounded compute cost and fewer added parameters per task as the model expands.The proposed method relies on a knowledge compartmentalization technique to achieve immunity against catastrophic forgetting and other common pitfalls such as gradient interference and negative transfer. We demonstrate empirically that the proposed method can jointly solve and achieve competitive results on 69public image classification tasks, for example improving the state of the art on a competitive benchmark such as cifar10 by achieving a 15% relative error reduction compared to the best model trained on public data."
    },
    "repositories": [
      {
        "url": "https://github.com/beir-cellar/beir",
        "name": "beir-cellar/beir",
        "description": "A Heterogeneous Benchmark for Information Retrieval. Easy to use, evaluate your models across 15+ diverse IR datasets.",
        "stars": 2014,
        "forks": 225,
        "language": "Python",
        "created_at": "2021-01-18T09:55:54+00:00",
        "updated_at": "2025-12-09T05:03:08+00:00",
        "topics": [
          "nlp",
          "information-retrieval",
          "bert",
          "benchmark",
          "sentence-transformers",
          "question-generation",
          "retrieval",
          "passage-retrieval",
          "elasticsearch",
          "dpr",
          "sbert",
          "retrieval-models",
          "dataset",
          "colbert",
          "zero-shot-retrieval",
          "deep-learning",
          "pytorch",
          "llm",
          "rag"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.10496",
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2212.10496",
      "abstract": "While dense retrieval has been shown effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance label is available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings~(HyDE). Given a query, HyDE first zero-shot instructs an instruction-following language model (e.g. InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is unreal and may contain false details. Then, an unsupervised contrastively learned encoder~(e.g. Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, where similar real documents are retrieved based on vector similarity. This second step ground the generated document to the actual corpus, with the encoder's dense bottleneck filtering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers, across various tasks (e.g. web search, QA, fact verification) and languages~(e.g. sw, ko, ja)."
    },
    "repositories": [
      {
        "url": "https://github.com/AkariAsai/self-rag",
        "name": "AkariAsai/self-rag",
        "description": "This includes the original implementation of SELF-RAG: Learning to Retrieve, Generate and Critique through self-reflection by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.",
        "stars": 2262,
        "forks": 214,
        "language": "Python",
        "created_at": "2023-10-10T20:12:05+00:00",
        "updated_at": "2025-12-10T02:10:34+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2201.08239",
      "title": "Contriever: Unsupervised Dense Information Retrieval",
      "year": 2022,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/2201.08239",
      "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/contriever",
        "name": "facebookresearch/contriever",
        "description": "Contriever: Unsupervised Dense Information Retrieval with Contrastive Learning",
        "stars": 766,
        "forks": 69,
        "language": "Python",
        "created_at": "2021-12-17T17:03:53+00:00",
        "updated_at": "2025-12-09T09:17:56+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.10375",
      "title": "InPars: Data Augmentation for Information Retrieval",
      "year": 2022,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/2212.10375",
      "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://github.com/Shark-NLP/self-adaptive-ICL"
    },
    "repositories": [
      {
        "url": "https://github.com/zetaalphavector/InPars",
        "name": "zetaalphavector/InPars",
        "description": "Inquisitive Parrots for Search",
        "stars": 198,
        "forks": 24,
        "language": "Python",
        "created_at": "2022-02-02T11:28:48+00:00",
        "updated_at": "2025-10-02T21:50:16+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2103.00020",
      "title": "CLIP: Learning Transferable Visual Models From Natural Language",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2103.00020",
      "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/CLIP",
        "name": "openai/CLIP",
        "description": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image",
        "stars": 31905,
        "forks": 3859,
        "language": "Jupyter Notebook",
        "created_at": "2020-12-16T11:24:42+00:00",
        "updated_at": "2025-12-10T03:25:44+00:00",
        "topics": [
          "deep-learning",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2301.12597",
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2301.12597",
      "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
    },
    "repositories": [
      {
        "url": "https://github.com/salesforce/LAVIS",
        "name": "salesforce/LAVIS",
        "description": "LAVIS - A One-stop Library for Language-Vision Intelligence",
        "stars": 11064,
        "forks": 1087,
        "language": "Jupyter Notebook",
        "created_at": "2022-08-24T02:36:01+00:00",
        "updated_at": "2025-12-09T13:09:05+00:00",
        "topics": [
          "deep-learning",
          "deep-learning-library",
          "image-captioning",
          "salesforce",
          "vision-and-language",
          "vision-framework",
          "vision-language-pretraining",
          "vision-language-transformer",
          "visual-question-anwsering",
          "multimodal-datasets",
          "multimodal-deep-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2201.12086",
      "title": "BLIP: Bootstrapping Language-Image Pre-training",
      "year": 2022,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2201.12086",
      "abstract": "Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."
    },
    "repositories": [
      {
        "url": "https://github.com/salesforce/BLIP",
        "name": "salesforce/BLIP",
        "description": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  ",
        "stars": 5603,
        "forks": 749,
        "language": "Jupyter Notebook",
        "created_at": "2022-01-25T01:19:25+00:00",
        "updated_at": "2025-12-09T09:23:49+00:00",
        "topics": [
          "vision-language",
          "vision-and-language-pre-training",
          "image-text-retrieval",
          "image-captioning",
          "visual-question-answering",
          "visual-reasoning",
          "vision-language-transformer"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2204.14198",
      "title": "Flamingo: A Visual Language Model for Few-Shot Learning",
      "year": 2022,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2204.14198",
      "abstract": "Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."
    },
    "repositories": [
      {
        "url": "https://github.com/lucidrains/flamingo-pytorch",
        "name": "lucidrains/flamingo-pytorch",
        "description": "Implementation of 🦩 Flamingo, state-of-the-art few-shot visual question answering attention net out of Deepmind, in Pytorch",
        "stars": 1273,
        "forks": 66,
        "language": "Python",
        "created_at": "2022-04-28T15:47:33+00:00",
        "updated_at": "2025-12-08T09:15:35+00:00",
        "topics": [
          "artificial-intelligence",
          "attention-mechanism",
          "deep-learning",
          "transformers",
          "visual-question-answering"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.08485",
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2304.08485",
      "abstract": "Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."
    },
    "repositories": [
      {
        "url": "https://github.com/Vision-CAIR/MiniGPT-4",
        "name": "Vision-CAIR/MiniGPT-4",
        "description": "Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)",
        "stars": 25762,
        "forks": 2929,
        "language": "Python",
        "created_at": "2023-04-15T22:17:09+00:00",
        "updated_at": "2025-12-09T14:47:10+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.10592",
      "title": "LLaVA: Large Language and Vision Assistant",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2304.10592",
      "abstract": "The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at https://minigpt-4.github.io/."
    },
    "repositories": [
      {
        "url": "https://github.com/haotian-liu/LLaVA",
        "name": "haotian-liu/LLaVA",
        "description": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.",
        "stars": 24140,
        "forks": 2670,
        "language": "Python",
        "created_at": "2023-04-17T16:13:11+00:00",
        "updated_at": "2025-12-10T03:10:29+00:00",
        "topics": [
          "gpt-4",
          "chatbot",
          "chatgpt",
          "llama",
          "multimodal",
          "llava",
          "foundation-models",
          "instruction-tuning",
          "multi-modality",
          "visual-language-learning",
          "llama-2",
          "llama2",
          "vision-language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2310.03744",
      "title": "LLaVA-1.5: Improved Baselines with Visual Instruction Tuning",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2310.03744",
      "abstract": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available."
    },
    "repositories": [
      {
        "url": "https://github.com/haotian-liu/LLaVA",
        "name": "haotian-liu/LLaVA",
        "description": "[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.",
        "stars": 24140,
        "forks": 2670,
        "language": "Python",
        "created_at": "2023-04-17T16:13:11+00:00",
        "updated_at": "2025-12-10T03:10:29+00:00",
        "topics": [
          "gpt-4",
          "chatbot",
          "chatgpt",
          "llama",
          "multimodal",
          "llava",
          "foundation-models",
          "instruction-tuning",
          "multi-modality",
          "visual-language-learning",
          "llama-2",
          "llama2",
          "vision-language-model"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.06500",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2305.06500",
      "abstract": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip."
    },
    "repositories": [
      {
        "url": "https://github.com/salesforce/LAVIS",
        "name": "salesforce/LAVIS",
        "description": "LAVIS - A One-stop Library for Language-Vision Intelligence",
        "stars": 11064,
        "forks": 1087,
        "language": "Jupyter Notebook",
        "created_at": "2022-08-24T02:36:01+00:00",
        "updated_at": "2025-12-09T13:09:05+00:00",
        "topics": [
          "deep-learning",
          "deep-learning-library",
          "image-captioning",
          "salesforce",
          "vision-and-language",
          "vision-framework",
          "vision-language-pretraining",
          "vision-language-transformer",
          "visual-question-anwsering",
          "multimodal-datasets",
          "multimodal-deep-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2102.02779",
      "title": "ViT: An Image is Worth 16x16 Words",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2102.02779",
      "abstract": "Existing methods for vision-and-language learning typically require designing task-specific architectures and objectives for each task. For example, a multi-label answer classifier for visual question answering, a region scorer for referring expression comprehension, and a language decoder for image captioning, etc. To alleviate these hassles, in this work, we propose a unified framework that learns different tasks in a single architecture with the same language modeling objective, i.e., multimodal conditional text generation, where our models learn to generate labels in text based on the visual and textual inputs. On 7 popular vision-and-language benchmarks, including visual question answering, referring expression comprehension, visual commonsense reasoning, most of which have been previously modeled as discriminative tasks, our generative approach (with a single unified architecture) reaches comparable performance to recent task-specific state-of-the-art vision-and-language models. Moreover, our generative approach shows better generalization ability on questions that have rare answers. Also, we show that our framework allows multi-task learning in a single architecture with a single set of parameters, achieving similar performance to separately optimized single-task models. Our code is publicly available at: https://github.com/j-min/VL-T5"
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/vision_transformer",
        "name": "google-research/vision_transformer",
        "description": null,
        "stars": 12102,
        "forks": 1428,
        "language": "Jupyter Notebook",
        "created_at": "2020-10-21T12:35:02+00:00",
        "updated_at": "2025-12-10T01:29:41+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2103.14030",
      "title": "BEiT: BERT Pre-Training of Image Transformers",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2103.14030",
      "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/unilm",
        "name": "microsoft/unilm",
        "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",
        "stars": 21873,
        "forks": 2681,
        "language": "Python",
        "created_at": "2019-07-23T04:15:28+00:00",
        "updated_at": "2025-12-09T22:08:32+00:00",
        "topics": [
          "nlp",
          "pre-trained-model",
          "unilm",
          "minilm",
          "layoutlm",
          "layoutxlm",
          "beit",
          "document-ai",
          "trocr",
          "beit-3",
          "foundation-models",
          "xlm-e",
          "deepnet",
          "llm",
          "multimodal",
          "mllm",
          "kosmos",
          "kosmos-1",
          "textdiffuser",
          "bitnet"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2006.11239",
      "title": "Denoising Diffusion Probabilistic Models",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2006.11239",
      "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion"
    },
    "repositories": [
      {
        "url": "https://github.com/hojonathanho/diffusion",
        "name": "hojonathanho/diffusion",
        "description": "Denoising Diffusion Probabilistic Models",
        "stars": 4906,
        "forks": 461,
        "language": "Python",
        "created_at": "2020-06-19T00:47:02+00:00",
        "updated_at": "2025-12-10T02:44:10+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2112.10752",
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2112.10752",
      "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion ."
    },
    "repositories": [
      {
        "url": "https://github.com/CompVis/latent-diffusion",
        "name": "CompVis/latent-diffusion",
        "description": "High-Resolution Image Synthesis with Latent Diffusion Models",
        "stars": 13646,
        "forks": 1702,
        "language": "Jupyter Notebook",
        "created_at": "2021-12-20T16:56:18+00:00",
        "updated_at": "2025-12-10T03:21:41+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2105.05233",
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2105.05233",
      "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"
    },
    "repositories": [
      {
        "url": "https://github.com/openai/guided-diffusion",
        "name": "openai/guided-diffusion",
        "description": null,
        "stars": 7183,
        "forks": 887,
        "language": "Python",
        "created_at": "2021-04-27T20:27:52+00:00",
        "updated_at": "2025-12-09T21:52:49+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2010.02502",
      "title": "Denoising Diffusion Implicit Models",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2010.02502",
      "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space."
    },
    "repositories": [
      {
        "url": "https://github.com/ermongroup/ddim",
        "name": "ermongroup/ddim",
        "description": "Denoising Diffusion Implicit Models",
        "stars": 1752,
        "forks": 229,
        "language": "Python",
        "created_at": "2020-10-05T23:11:51+00:00",
        "updated_at": "2025-12-10T03:13:23+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2209.00796",
      "title": "DreamFusion: Text-to-3D using 2D Diffusion",
      "year": 2022,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2209.00796",
      "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy."
    },
    "repositories": [
      {
        "url": "https://github.com/ashawkey/stable-dreamfusion",
        "name": "ashawkey/stable-dreamfusion",
        "description": "Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.",
        "stars": 8785,
        "forks": 771,
        "language": "Python",
        "created_at": "2022-10-06T06:18:39+00:00",
        "updated_at": "2025-12-09T13:32:34+00:00",
        "topics": [
          "text-to-3d",
          "gui",
          "nerf",
          "stable-diffusion",
          "dreamfusion",
          "image-to-3d"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2208.01618",
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models",
      "year": 2022,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2208.01618",
      "abstract": "Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes. In other words, we ask: how can we use language-guided models to turn our cat into a painting, or imagine a new product based on our favorite toy? Here we present a simple approach that allows such creative freedom. Using only 3-5 images of a user-provided concept, like an object or a style, we learn to represent it through new \"words\" in the embedding space of a frozen text-to-image model. These \"words\" can be composed into natural language sentences, guiding personalized creation in an intuitive way. Notably, we find evidence that a single word embedding is sufficient for capturing unique and varied concepts. We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks.\n  Our code, data and new words will be available at: https://textual-inversion.github.io"
    },
    "repositories": [
      {
        "url": "https://github.com/google/dreambooth",
        "name": "google/dreambooth",
        "description": null,
        "stars": 1005,
        "forks": 93,
        "language": null,
        "created_at": "2023-03-01T17:10:19+00:00",
        "updated_at": "2025-12-07T08:06:09+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.05543",
      "title": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2302.05543",
      "abstract": "We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with \"zero convolutions\" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models."
    },
    "repositories": [
      {
        "url": "https://github.com/lllyasviel/ControlNet",
        "name": "lllyasviel/ControlNet",
        "description": "Let us control diffusion models!",
        "stars": 33415,
        "forks": 2992,
        "language": "Python",
        "created_at": "2023-02-01T02:43:30+00:00",
        "updated_at": "2025-12-10T03:16:16+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2307.01952",
      "title": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2307.01952",
      "abstract": "We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models"
    },
    "repositories": [
      {
        "url": "https://github.com/HumanAIGC/AnimateAnyone",
        "name": "HumanAIGC/AnimateAnyone",
        "description": "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation",
        "stars": 14794,
        "forks": 1008,
        "language": null,
        "created_at": "2023-11-28T07:55:34+00:00",
        "updated_at": "2025-12-09T10:24:14+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2312.00752",
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2312.00752",
      "abstract": "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation."
    },
    "repositories": [
      {
        "url": "https://github.com/state-spaces/mamba",
        "name": "state-spaces/mamba",
        "description": "Mamba SSM architecture",
        "stars": 16662,
        "forks": 1529,
        "language": "Python",
        "created_at": "2023-12-01T01:17:39+00:00",
        "updated_at": "2025-12-10T01:38:45+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2111.00396",
      "title": "S4: Efficiently Modeling Long Sequences",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2111.00396",
      "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors."
    },
    "repositories": [
      {
        "url": "https://github.com/state-spaces/s4",
        "name": "state-spaces/s4",
        "description": "Structured state space sequence models",
        "stars": 2794,
        "forks": 349,
        "language": "Jupyter Notebook",
        "created_at": "2021-11-03T15:33:53+00:00",
        "updated_at": "2025-12-08T03:06:34+00:00",
        "topics": [
          "state-space-models",
          "sequence-models",
          "pytorch"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2203.05556",
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2203.05556",
      "abstract": "Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial for many backbones, not only for Transformers. Specifically, after proper embeddings, simple MLP-like models can perform on par with the attention-based architectures. Overall, we highlight embeddings for numerical features as an important design aspect with good potential for further improvements in tabular DL."
    },
    "repositories": [
      {
        "url": "https://github.com/Dao-AILab/flash-attention",
        "name": "Dao-AILab/flash-attention",
        "description": "Fast and memory-efficient exact attention",
        "stars": 20993,
        "forks": 2199,
        "language": "Python",
        "created_at": "2022-05-19T21:22:06+00:00",
        "updated_at": "2025-12-10T00:38:49+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2307.08691",
      "title": "FlashAttention-2: Faster Attention with Better Parallelism",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2307.08691",
      "abstract": "Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization)."
    },
    "repositories": [
      {
        "url": "https://github.com/Dao-AILab/flash-attention",
        "name": "Dao-AILab/flash-attention",
        "description": "Fast and memory-efficient exact attention",
        "stars": 20993,
        "forks": 2199,
        "language": "Python",
        "created_at": "2022-05-19T21:22:06+00:00",
        "updated_at": "2025-12-10T00:38:49+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2108.12409",
      "title": "Linformer: Self-Attention with Linear Complexity",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2108.12409",
      "abstract": "Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark."
    },
    "repositories": [
      {
        "url": "https://github.com/tatp22/linformer-pytorch",
        "name": "tatp22/linformer-pytorch",
        "description": "My take on a practical implementation of Linformer for Pytorch.",
        "stars": 421,
        "forks": 37,
        "language": "Python",
        "created_at": "2020-06-11T15:09:22+00:00",
        "updated_at": "2025-10-20T13:16:43+00:00",
        "topics": [
          "artificial-intelligence",
          "deep-learning",
          "attention-mechanism",
          "pytorch",
          "machine-learning",
          "linformer",
          "paper"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2001.04451",
      "title": "Longformer: The Long-Document Transformer",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2001.04451",
      "abstract": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
    },
    "repositories": [
      {
        "url": "https://github.com/allenai/longformer",
        "name": "allenai/longformer",
        "description": "Longformer: The Long-Document Transformer",
        "stars": 2176,
        "forks": 288,
        "language": "Python",
        "created_at": "2020-03-31T21:07:29+00:00",
        "updated_at": "2025-12-08T13:51:15+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2203.08913",
      "title": "RetNet: Retentive Network: A Successor to Transformer",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2203.08913",
      "abstract": "Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/torchscale",
        "name": "microsoft/torchscale",
        "description": "Foundation Architecture for (M)LLMs",
        "stars": 3124,
        "forks": 222,
        "language": "Python",
        "created_at": "2022-11-17T08:55:59+00:00",
        "updated_at": "2025-12-09T09:57:27+00:00",
        "topics": [
          "computer-vision",
          "machine-learning",
          "multimodal",
          "natural-language-processing",
          "pretrained-language-model",
          "speech-processing",
          "transformer",
          "translation"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.00445",
      "title": "MRKL Systems: A modular approach to AGI",
      "year": 2022,
      "category": "cs.AI",
      "url": "https://arxiv.org/abs/2205.00445",
      "abstract": "Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation."
    },
    "repositories": [
      {
        "url": "https://github.com/langchain-ai/langchain",
        "name": "langchain-ai/langchain",
        "description": "🦜🔗 The platform for reliable agents.",
        "stars": 121514,
        "forks": 20041,
        "language": "Python",
        "created_at": "2022-10-17T02:58:36+00:00",
        "updated_at": "2025-12-10T03:17:10+00:00",
        "topics": [
          "ai",
          "anthropic",
          "gemini",
          "langchain",
          "llm",
          "openai",
          "python",
          "agents",
          "ai-agents",
          "ai-agents-framework",
          "chatgpt",
          "enterprise",
          "framework",
          "generative-ai",
          "multiagent",
          "open-source",
          "pydantic",
          "rag",
          "aiagentframework"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2210.03629",
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2210.03629",
      "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io"
    },
    "repositories": [
      {
        "url": "https://github.com/ysymyth/ReAct",
        "name": "ysymyth/ReAct",
        "description": "[ICLR 2023] ReAct: Synergizing Reasoning and Acting in Language Models",
        "stars": 3264,
        "forks": 331,
        "language": "Jupyter Notebook",
        "created_at": "2022-11-13T19:51:09+00:00",
        "updated_at": "2025-12-09T20:36:44+00:00",
        "topics": [
          "decision-making",
          "large-language-models",
          "llm",
          "prompting",
          "reasoning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.11366",
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "year": 2023,
      "category": "cs.AI",
      "url": "https://arxiv.org/abs/2303.11366",
      "abstract": "Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance."
    },
    "repositories": [
      {
        "url": "https://github.com/noahshinn/reflexion",
        "name": "noahshinn/reflexion",
        "description": "[NeurIPS 2023] Reflexion: Language Agents with Verbal Reinforcement Learning",
        "stars": 2981,
        "forks": 288,
        "language": "Python",
        "created_at": "2023-03-22T06:38:53+00:00",
        "updated_at": "2025-12-10T03:09:22+00:00",
        "topics": [
          "ai",
          "artificial-intelligence",
          "llm"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.15334",
      "title": "Gorilla: Large Language Model Connected with Massive APIs",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2305.15334",
      "abstract": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu"
    },
    "repositories": [
      {
        "url": "https://github.com/ShishirPatil/gorilla",
        "name": "ShishirPatil/gorilla",
        "description": "Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls)",
        "stars": 12603,
        "forks": 1294,
        "language": "Python",
        "created_at": "2023-05-19T00:46:45+00:00",
        "updated_at": "2025-12-08T06:24:06+00:00",
        "topics": [
          "api",
          "llm",
          "api-documentation",
          "chatgpt",
          "gpt-4-api",
          "claude-api",
          "openai-api",
          "openai-functions"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2308.08155",
      "title": "MetaGPT: Meta Programming for Multi-Agent Systems",
      "year": 2023,
      "category": "cs.AI",
      "url": "https://arxiv.org/abs/2308.08155",
      "abstract": "AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc."
    },
    "repositories": [
      {
        "url": "https://github.com/FoundationAgents/MetaGPT",
        "name": "FoundationAgents/MetaGPT",
        "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
        "stars": 60328,
        "forks": 7416,
        "language": "Python",
        "created_at": "2023-06-30T09:04:55+00:00",
        "updated_at": "2025-12-10T03:22:02+00:00",
        "topics": [
          "agent",
          "gpt",
          "llm",
          "metagpt",
          "multi-agent"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.17580",
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT",
      "year": 2023,
      "category": "cs.AI",
      "url": "https://arxiv.org/abs/2303.17580",
      "abstract": "Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence. While there are numerous AI models available for various domains and modalities, they cannot handle complicated AI tasks autonomously. Considering large language models (LLMs) have exhibited exceptional abilities in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks, with language serving as a generic interface to empower this. Based on this philosophy, we present HuggingGPT, an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards the realization of artificial general intelligence."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/JARVIS",
        "name": "microsoft/JARVIS",
        "description": "JARVIS, a system to connect LLMs with ML community. Paper: https://arxiv.org/pdf/2303.17580.pdf",
        "stars": 24488,
        "forks": 2059,
        "language": "Python",
        "created_at": "2023-03-30T02:57:12+00:00",
        "updated_at": "2025-12-10T00:51:42+00:00",
        "topics": [
          "deep-learning",
          "platform",
          "pytorch"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2106.05237",
      "title": "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2106.05237",
      "abstract": "There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art ResNet-50 model for ImageNet, which achieves 82.8% top-1 accuracy."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.04356",
      "title": "Whisper: Robust Speech Recognition via Large-Scale Weak Supervision",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2212.04356",
      "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/whisper",
        "name": "openai/whisper",
        "description": "Robust Speech Recognition via Large-Scale Weak Supervision",
        "stars": 91793,
        "forks": 11506,
        "language": "Python",
        "created_at": "2022-09-16T20:02:54+00:00",
        "updated_at": "2025-12-10T02:22:53+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.08575",
      "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2302.08575",
      "abstract": "This open access book provides a comprehensive overview of the state of the art in research and applications of Foundation Models and is intended for readers familiar with basic Natural Language Processing (NLP) concepts. Over the recent years, a revolutionary new paradigm has been developed for training models for NLP. These models are first pre-trained on large collections of text documents to acquire general syntactic knowledge and semantic information. Then, they are fine-tuned for specific tasks, which they can often solve with superhuman accuracy. When the models are large enough, they can be instructed by prompts to solve new tasks without any fine-tuning. Moreover, they can be applied to a wide range of different media and problem domains, ranging from image and video processing to robot control learning. Because they provide a blueprint for solving many tasks in artificial intelligence, they have been called Foundation Models. After a brief introduction to basic NLP models the main pre-trained language models BERT, GPT and sequence-to-sequence transformer are described, as well as the concepts of self-attention and context-sensitive embedding. Then, different approaches to improving these models are discussed, such as expanding the pre-training criteria, increasing the length of input texts, or including extra knowledge. An overview of the best-performing models for about twenty application areas is then presented, e.g., question answering, translation, story generation, dialog systems, generating images from text, etc. For each application area, the strengths and weaknesses of current models are discussed, and an outlook on further developments is given. In addition, links are provided to freely available program code. A concluding chapter summarizes the economic opportunities, mitigation of risks, and potential developments of AI."
    },
    "repositories": [
      {
        "url": "https://github.com/0nutation/SpeechGPT",
        "name": "0nutation/SpeechGPT",
        "description": "SpeechGPT Series: Speech Large Language Models",
        "stars": 1396,
        "forks": 95,
        "language": "Python",
        "created_at": "2023-05-16T15:59:40+00:00",
        "updated_at": "2025-11-30T18:33:32+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2309.03409",
      "title": "AudioPaLM: A Large Language Model for Speech Understanding",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2309.03409",
      "abstract": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro."
    },
    "repositories": [
      {
        "url": "https://github.com/lucidrains/audiolm-pytorch",
        "name": "lucidrains/audiolm-pytorch",
        "description": "Implementation of AudioLM, a SOTA Language Modeling Approach to Audio Generation out of Google Research, in Pytorch",
        "stars": 2610,
        "forks": 280,
        "language": "Python",
        "created_at": "2022-09-09T21:55:02+00:00",
        "updated_at": "2025-12-09T01:27:46+00:00",
        "topics": [
          "artificial-intelligence",
          "attention-mechanisms",
          "audio-synthesis",
          "deep-learning",
          "transformers"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2107.03374",
      "title": "Codex: Evaluating Large Language Models Trained on Code",
      "year": 2021,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2107.03374",
      "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/human-eval",
        "name": "openai/human-eval",
        "description": "Code for the paper \"Evaluating Large Language Models Trained on Code\"",
        "stars": 3048,
        "forks": 423,
        "language": "Python",
        "created_at": "2021-07-06T21:23:45+00:00",
        "updated_at": "2025-12-09T12:57:08+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2203.07814",
      "title": "Competition-Level Code Generation with AlphaCode",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2203.07814",
      "abstract": "Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions."
    },
    "repositories": [
      {
        "url": "https://github.com/google-deepmind/code_contests",
        "name": "google-deepmind/code_contests",
        "description": null,
        "stars": 2177,
        "forks": 223,
        "language": "C++",
        "created_at": "2022-01-31T09:48:14+00:00",
        "updated_at": "2025-12-09T01:42:27+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.06161",
      "title": "StarCoder: May the source be with you!",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2305.06161",
      "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license."
    },
    "repositories": [
      {
        "url": "https://github.com/bigcode-project/starcoder",
        "name": "bigcode-project/starcoder",
        "description": "Home of StarCoder: fine-tuning & inference!",
        "stars": 7478,
        "forks": 531,
        "language": "Python",
        "created_at": "2023-04-24T12:32:21+00:00",
        "updated_at": "2025-12-08T09:40:12+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2308.12950",
      "title": "Code Llama: Open Foundation Models for Code",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2308.12950",
      "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use."
    },
    "repositories": [
      {
        "url": "https://github.com/meta-llama/codellama",
        "name": "meta-llama/codellama",
        "description": "Inference code for CodeLlama models",
        "stars": 16365,
        "forks": 1946,
        "language": "Python",
        "created_at": "2023-08-24T14:25:15+00:00",
        "updated_at": "2025-12-07T04:30:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2309.10305",
      "title": "WizardCoder: Empowering Code LLMs with Evol-Instruct",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2309.10305",
      "abstract": "Large language models (LLMs) have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering. However, most powerful LLMs are closed-source or limited in their capability for languages other than English. In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. Baichuan 2 matches or outperforms other open-source models of similar size on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval. Furthermore, Baichuan 2 excels in vertical domains such as medicine and law. We will release all pre-training model checkpoints to benefit the research community in better understanding the training dynamics of Baichuan 2."
    },
    "repositories": [
      {
        "url": "https://github.com/nlpxucan/WizardLM",
        "name": "nlpxucan/WizardLM",
        "description": "LLMs build upon Evol Insturct: WizardLM, WizardCoder, WizardMath",
        "stars": 9471,
        "forks": 751,
        "language": "Python",
        "created_at": "2023-04-23T13:26:46+00:00",
        "updated_at": "2025-12-08T11:07:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2109.07958",
      "title": "MMLU: Measuring Massive Multitask Language Understanding",
      "year": 2021,
      "category": "cs.CY",
      "url": "https://arxiv.org/abs/2109.07958",
      "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web."
    },
    "repositories": [
      {
        "url": "https://github.com/hendrycks/test",
        "name": "hendrycks/test",
        "description": "Measuring Massive Multitask Language Understanding | ICLR 2021",
        "stars": 1528,
        "forks": 112,
        "language": "Python",
        "created_at": "2020-09-07T23:02:57+00:00",
        "updated_at": "2025-12-09T11:58:58+00:00",
        "topics": [
          "muti-task",
          "transfer-learning",
          "gpt-3",
          "few-shot-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.08774",
      "title": "GPT-4 Technical Report",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2303.08774",
      "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/evals",
        "name": "openai/evals",
        "description": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.",
        "stars": 17406,
        "forks": 2848,
        "language": "Python",
        "created_at": "2023-01-23T20:51:04+00:00",
        "updated_at": "2025-12-10T01:28:20+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2307.03109",
      "title": "AlpacaEval: An Automatic Evaluator for Instruction-following",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2307.03109",
      "abstract": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey."
    },
    "repositories": [
      {
        "url": "https://github.com/tatsu-lab/alpaca_eval",
        "name": "tatsu-lab/alpaca_eval",
        "description": "An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast.",
        "stars": 1924,
        "forks": 294,
        "language": "Jupyter Notebook",
        "created_at": "2023-05-25T09:35:28+00:00",
        "updated_at": "2025-12-10T03:04:52+00:00",
        "topics": [
          "deep-learning",
          "evaluation",
          "foundation-models",
          "instruction-following",
          "large-language-models",
          "leaderboard",
          "nlp",
          "rlhf"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2306.05685",
      "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2306.05685",
      "abstract": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."
    },
    "repositories": [
      {
        "url": "https://github.com/lm-sys/FastChat",
        "name": "lm-sys/FastChat",
        "description": "An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena.",
        "stars": 39298,
        "forks": 4778,
        "language": "Python",
        "created_at": "2023-03-19T00:18:02+00:00",
        "updated_at": "2025-12-09T21:52:34+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1905.07830",
      "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1905.07830",
      "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference?\n  In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models.\n  Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges."
    },
    "repositories": [
      {
        "url": "https://github.com/nyu-mll/jiant",
        "name": "nyu-mll/jiant",
        "description": "jiant is an nlp toolkit",
        "stars": 1674,
        "forks": 297,
        "language": "Python",
        "created_at": "2018-06-18T18:12:47+00:00",
        "updated_at": "2025-12-09T02:37:01+00:00",
        "topics": [
          "nlp",
          "sentence-representation",
          "bert",
          "multitask-learning",
          "transformers",
          "transfer-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1609.02907",
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "year": 2016,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1609.02907",
      "abstract": "We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin."
    },
    "repositories": [
      {
        "url": "https://github.com/tkipf/gcn",
        "name": "tkipf/gcn",
        "description": "Implementation of Graph Convolutional Networks in TensorFlow",
        "stars": 7339,
        "forks": 2013,
        "language": "Python",
        "created_at": "2016-11-11T10:59:21+00:00",
        "updated_at": "2025-12-08T15:29:00+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1710.10903",
      "title": "Graph Attention Networks",
      "year": 2017,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1710.10903",
      "abstract": "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."
    },
    "repositories": [
      {
        "url": "https://github.com/PetarV-/GAT",
        "name": "PetarV-/GAT",
        "description": "Graph Attention Networks (https://arxiv.org/abs/1710.10903)",
        "stars": 3469,
        "forks": 672,
        "language": "Python",
        "created_at": "2018-02-01T02:17:22+00:00",
        "updated_at": "2025-12-08T15:28:59+00:00",
        "topics": [
          "graph-attention-networks",
          "attention-mechanism",
          "self-attention",
          "tensorflow",
          "neural-networks",
          "python"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1706.02216",
      "title": "Inductive Representation Learning on Large Graphs",
      "year": 2017,
      "category": "cs.SI",
      "url": "https://arxiv.org/abs/1706.02216",
      "abstract": "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
    },
    "repositories": [
      {
        "url": "https://github.com/williamleif/GraphSAGE",
        "name": "williamleif/GraphSAGE",
        "description": "Representation learning on large graphs using stochastic graph convolutions.",
        "stars": 3636,
        "forks": 849,
        "language": "Python",
        "created_at": "2017-05-29T15:36:22+00:00",
        "updated_at": "2025-12-08T22:46:09+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2005.00687",
      "title": "Deep Graph Library: A Graph-Centric, Highly-Performant Package",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2005.00687",
      "abstract": "We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu ."
    },
    "repositories": [
      {
        "url": "https://github.com/dmlc/dgl",
        "name": "dmlc/dgl",
        "description": "Python package built to ease deep learning on graph, on top of existing DL frameworks.",
        "stars": 14165,
        "forks": 3056,
        "language": "Python",
        "created_at": "2018-04-20T14:49:09+00:00",
        "updated_at": "2025-12-09T23:50:11+00:00",
        "topics": [
          "deep-learning",
          "graph-neural-networks"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1903.07293",
      "title": "PyTorch Geometric: Library for Geometric Deep Learning",
      "year": 2019,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1903.07293",
      "abstract": "Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis."
    },
    "repositories": [
      {
        "url": "https://github.com/pyg-team/pytorch_geometric",
        "name": "pyg-team/pytorch_geometric",
        "description": "Graph Neural Network Library for PyTorch",
        "stars": 23255,
        "forks": 3928,
        "language": "Python",
        "created_at": "2017-10-06T16:03:03+00:00",
        "updated_at": "2025-12-09T20:09:27+00:00",
        "topics": [
          "pytorch",
          "geometric-deep-learning",
          "graph-neural-networks",
          "deep-learning",
          "graph-convolutional-networks"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1508.07909",
      "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
      "year": 2015,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1508.07909",
      "abstract": "Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively."
    },
    "repositories": [
      {
        "url": "https://github.com/lisa-groundhog/GroundHog",
        "name": "lisa-groundhog/GroundHog",
        "description": "Library for implementing RNNs with Theano",
        "stars": 613,
        "forks": 230,
        "language": "Python",
        "created_at": "2014-08-19T13:08:13+00:00",
        "updated_at": "2025-11-13T04:17:46+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1409.0473",
      "title": "Sequence to Sequence Learning with Neural Networks",
      "year": 2014,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1409.0473",
      "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
    },
    "repositories": [
      {
        "url": "https://github.com/farizrahman4u/seq2seq",
        "name": "farizrahman4u/seq2seq",
        "description": "Sequence to Sequence Learning with Keras",
        "stars": 3175,
        "forks": 837,
        "language": "Python",
        "created_at": "2015-11-07T07:37:12+00:00",
        "updated_at": "2025-12-06T23:55:06+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1706.03059",
      "title": "Depthwise Separable Convolutions for Neural Machine Translation",
      "year": 2017,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1706.03059",
      "abstract": "Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new \"super-separable\" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/google-research",
        "name": "google-research/google-research",
        "description": "Google Research",
        "stars": 36869,
        "forks": 8267,
        "language": "Jupyter Notebook",
        "created_at": "2018-10-04T18:42:48+00:00",
        "updated_at": "2025-12-10T03:17:21+00:00",
        "topics": [
          "machine-learning",
          "ai",
          "research"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2104.05556",
      "title": "Multi-Task Deep Neural Networks for Natural Language Understanding",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2104.05556",
      "abstract": "CsV$_3$Sb$_5$ is a newly discovered Kagome superconductor that attracts great interest due to its topological nontrivial band structure and the coexistence of superconductivity and charge-density-wave (CDW) with many exotic properties. Here, we report the detailed characterization of the CDW gap in high-quality CsV$_3$Sb$_5$ single crystals using high-resolution angle-resolved photoemission spectroscopy. We find that the CDW gap is strongly momentum dependent. While gapped around the $M$ point, the electronic states remain gapless around the $Γ$ point and along the $Γ$-$K$ direction. Such momentum dependence indicates that the CDW is driven by the scattering of electrons between neighboring $M$ points, where the band structure hosts multiple saddle points and the density of state diverges near the Fermi level. Our observations of the partially gapped Fermi surface and strongly momentum-dependent CDW gap not only provide a foundation for uncovering the mechanism of CDW in CsV$_3$Sb$_5$, but also shed light on the understanding of how the CDW coexists with superconductivity in this topological Kagome superconductor."
    },
    "repositories": [
      {
        "url": "https://github.com/namisan/mt-dnn",
        "name": "namisan/mt-dnn",
        "description": "Multi-Task Deep Neural Networks for Natural Language Understanding",
        "stars": 2258,
        "forks": 413,
        "language": "Python",
        "created_at": "2019-02-19T22:58:26+00:00",
        "updated_at": "2025-11-21T09:42:51+00:00",
        "topics": [
          "multi-task-learning",
          "natural-language-understanding",
          "deep-learning",
          "microsoft",
          "ranking",
          "named-entity-recognition",
          "bert",
          "machine-reading-comprehension",
          "nlp",
          "pytorch"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2109.01652",
      "title": "Finetuned Language Models Are Zero-Shot Learners",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2109.01652",
      "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/FLAN",
        "name": "google-research/FLAN",
        "description": null,
        "stars": 1556,
        "forks": 160,
        "language": "Python",
        "created_at": "2021-08-21T20:12:17+00:00",
        "updated_at": "2025-12-09T01:46:23+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2310.11511",
      "title": "Zephyr: Direct Distillation of LM Alignment",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2310.11511",
      "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."
    },
    "repositories": [
      {
        "url": "https://github.com/huggingface/alignment-handbook",
        "name": "huggingface/alignment-handbook",
        "description": "Robust recipes to align language models with human and AI preferences",
        "stars": 5444,
        "forks": 464,
        "language": "Python",
        "created_at": "2023-08-25T11:35:34+00:00",
        "updated_at": "2025-12-09T10:54:35+00:00",
        "topics": [
          "llm",
          "rlhf",
          "transformers"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.07842",
      "title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2302.07842",
      "abstract": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/FLAN",
        "name": "google-research/FLAN",
        "description": null,
        "stars": 1556,
        "forks": 160,
        "language": "Python",
        "created_at": "2021-08-21T20:12:17+00:00",
        "updated_at": "2025-12-09T01:46:23+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1606.05250",
      "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
      "year": 2016,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1606.05250",
      "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.\n  The dataset is freely available at https://stanford-qa.com"
    },
    "repositories": [
      {
        "url": "https://github.com/rajpurkar/SQuAD-explorer",
        "name": "rajpurkar/SQuAD-explorer",
        "description": "Visually Explore the Stanford Question Answering Dataset",
        "stars": 569,
        "forks": 121,
        "language": "JavaScript",
        "created_at": "2016-08-23T07:57:52+00:00",
        "updated_at": "2025-11-29T19:12:07+00:00",
        "topics": [
          "dataset",
          "visual-analysis",
          "leaderboard"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1808.07042",
      "title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
      "year": 2018,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1808.07042",
      "abstract": "Humans gather information by engaging in conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong conversational and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating there is ample room for improvement. We launch CoQA as a challenge to the community at http://stanfordnlp.github.io/coqa/"
    },
    "repositories": [
      {
        "url": "https://github.com/rajpurkar/SQuAD-explorer",
        "name": "rajpurkar/SQuAD-explorer",
        "description": "Visually Explore the Stanford Question Answering Dataset",
        "stars": 569,
        "forks": 121,
        "language": "JavaScript",
        "created_at": "2016-08-23T07:57:52+00:00",
        "updated_at": "2025-11-29T19:12:07+00:00",
        "topics": [
          "dataset",
          "visual-analysis",
          "leaderboard"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1804.09301",
      "title": "Generating Wikipedia by Summarizing Long Sequences",
      "year": 2018,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1804.09301",
      "abstract": "We present an empirical study of gender bias in coreference resolution systems. We first introduce a novel, Winograd schema-style set of minimal pair sentences that differ only by pronoun gender. With these \"Winogender schemas,\" we evaluate and confirm systematic gender bias in three publicly-available coreference resolution systems, and correlate this bias with real-world and textual gender statistics."
    },
    "repositories": [
      {
        "url": "https://github.com/tensorflow/tensor2tensor",
        "name": "tensorflow/tensor2tensor",
        "description": "Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.",
        "stars": 16801,
        "forks": 3698,
        "language": "Python",
        "created_at": "2017-06-15T16:57:39+00:00",
        "updated_at": "2025-12-10T03:32:49+00:00",
        "topics": [
          "machine-learning",
          "machine-translation",
          "deep-learning",
          "reinforcement-learning",
          "tpu"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1802.05365",
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "year": 2018,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1802.05365",
      "abstract": "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
    },
    "repositories": [
      {
        "url": "https://github.com/fastai/fastai",
        "name": "fastai/fastai",
        "description": "The fastai deep learning library",
        "stars": 27653,
        "forks": 7666,
        "language": "Jupyter Notebook",
        "created_at": "2017-09-09T17:43:36+00:00",
        "updated_at": "2025-12-10T01:57:28+00:00",
        "topics": [
          "deep-learning",
          "machine-learning",
          "pytorch",
          "python",
          "gpu",
          "fastai",
          "notebooks",
          "colab"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1905.05583",
      "title": "Billion-scale similarity search with GPUs",
      "year": 2019,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1905.05583",
      "abstract": "Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/faiss",
        "name": "facebookresearch/faiss",
        "description": "A library for efficient similarity search and clustering of dense vectors.",
        "stars": 38366,
        "forks": 4147,
        "language": "C++",
        "created_at": "2017-02-07T16:07:05+00:00",
        "updated_at": "2025-12-10T01:15:59+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2005.08100",
      "title": "Question and Answer Test-Train Overlap in Open-Domain QA Datasets",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2005.08100",
      "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research-datasets/natural-questions",
        "name": "google-research-datasets/natural-questions",
        "description": "Natural Questions (NQ) contains real user questions issued to Google search, and answers found from Wikipedia by annotators. NQ is designed for the training and evaluation of automatic question answering systems.",
        "stars": 1076,
        "forks": 157,
        "language": "Python",
        "created_at": "2019-01-22T18:37:59+00:00",
        "updated_at": "2025-12-08T03:15:36+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1512.03385",
      "title": "Deep Residual Learning for Image Recognition",
      "year": 2015,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1512.03385",
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
    },
    "repositories": [
      {
        "url": "https://github.com/KaimingHe/deep-residual-networks",
        "name": "KaimingHe/deep-residual-networks",
        "description": "Deep Residual Learning for Image Recognition ",
        "stars": 6671,
        "forks": 2232,
        "language": null,
        "created_at": "2016-02-01T13:07:00+00:00",
        "updated_at": "2025-12-09T14:40:07+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1409.1556",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "year": 2014,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1409.1556",
      "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
    },
    "repositories": [
      {
        "url": "https://github.com/machrisaa/tensorflow-vgg",
        "name": "machrisaa/tensorflow-vgg",
        "description": "VGG19 and VGG16 on Tensorflow",
        "stars": 2240,
        "forks": 1072,
        "language": "Python",
        "created_at": "2016-03-16T22:17:14+00:00",
        "updated_at": "2025-12-01T05:44:45+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1608.06993",
      "title": "Densely Connected Convolutional Networks",
      "year": 2016,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1608.06993",
      "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet ."
    },
    "repositories": [
      {
        "url": "https://github.com/liuzhuang13/DenseNet",
        "name": "liuzhuang13/DenseNet",
        "description": "Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award).",
        "stars": 4848,
        "forks": 1071,
        "language": "Lua",
        "created_at": "2016-08-24T21:32:52+00:00",
        "updated_at": "2025-12-09T17:50:25+00:00",
        "topics": [
          "deep-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1807.11164",
      "title": "YOLOX: Exceeding YOLO Series in 2021",
      "year": 2018,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1807.11164",
      "abstract": "Currently, the neural network architecture design is mostly guided by the \\emph{indirect} metric of computation complexity, i.e., FLOPs. However, the \\emph{direct} metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical \\emph{guidelines} for efficient network design. Accordingly, a new architecture is presented, called \\emph{ShuffleNet V2}. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff."
    },
    "repositories": [
      {
        "url": "https://github.com/Megvii-BaseDetection/YOLOX",
        "name": "Megvii-BaseDetection/YOLOX",
        "description": "YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with MegEngine, ONNX, TensorRT, ncnn, and OpenVINO supported. Documentation: https://yolox.readthedocs.io/",
        "stars": 10223,
        "forks": 2419,
        "language": "Python",
        "created_at": "2021-07-17T02:01:45+00:00",
        "updated_at": "2025-12-10T03:10:07+00:00",
        "topics": [
          "yolox",
          "yolov3",
          "onnx",
          "tensorrt",
          "ncnn",
          "openvino",
          "pytorch",
          "megengine",
          "object-detection",
          "yolo",
          "deep-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2010.11929",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "year": 2020,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2010.11929",
      "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/vision_transformer",
        "name": "google-research/vision_transformer",
        "description": null,
        "stars": 12102,
        "forks": 1428,
        "language": "Jupyter Notebook",
        "created_at": "2020-10-21T12:35:02+00:00",
        "updated_at": "2025-12-10T01:29:41+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2103.14899",
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "year": 2021,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2103.14899",
      "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \\url{https://github.com/IBM/CrossViT}."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/Swin-Transformer",
        "name": "microsoft/Swin-Transformer",
        "description": "This is an official implementation for \"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\".",
        "stars": 15517,
        "forks": 2202,
        "language": "Python",
        "created_at": "2021-03-25T12:42:36+00:00",
        "updated_at": "2025-12-10T02:28:18+00:00",
        "topics": [
          "swin-transformer",
          "image-classification",
          "object-detection",
          "semantic-segmentation",
          "imagenet",
          "mscoco",
          "ade20k",
          "mask-rcnn"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.01917",
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "year": 2022,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2205.01917",
      "abstract": "Exploring large-scale pretrained foundation models is of significant interest in computer vision because these models can be quickly transferred to many downstream tasks. This paper presents Contrastive Captioner (CoCa), a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with contrastive loss and captioning loss, thereby subsuming model capabilities from contrastive approaches like CLIP and generative methods like SimVLM. In contrast to standard encoder-decoder transformers where all decoder layers attend to encoder outputs, CoCa omits cross-attention in the first half of decoder layers to encode unimodal text representations, and cascades the remaining decoder layers which cross-attend to the image encoder for multimodal image-text representations. We apply a contrastive loss between unimodal image and text embeddings, in addition to a captioning loss on the multimodal decoder outputs which predicts text tokens autoregressively. By sharing the same computational graph, the two training objectives are computed efficiently with minimal overhead. CoCa is pretrained end-to-end and from scratch on both web-scale alt-text data and annotated images by treating all labels simply as text, seamlessly unifying natural language supervision for representation learning. Empirically, CoCa achieves state-of-the-art performance with zero-shot transfer or minimal task-specific adaptation on a broad range of downstream tasks, spanning visual recognition (ImageNet, Kinetics-400/600/700, Moments-in-Time), crossmodal retrieval (MSCOCO, Flickr30K, MSR-VTT), multimodal understanding (VQA, SNLI-VE, NLVR2), and image captioning (MSCOCO, NoCaps). Notably on ImageNet classification, CoCa obtains 86.3% zero-shot top-1 accuracy, 90.6% with a frozen encoder and learned classification head, and new state-of-the-art 91.0% top-1 accuracy on ImageNet with a finetuned encoder."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/mae",
        "name": "facebookresearch/mae",
        "description": "PyTorch implementation of MAE https//arxiv.org/abs/2111.06377",
        "stars": 8127,
        "forks": 1338,
        "language": "Python",
        "created_at": "2021-12-06T21:29:09+00:00",
        "updated_at": "2025-12-08T11:18:32+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2304.02643",
      "title": "Segment Anything",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2304.02643",
      "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/segment-anything",
        "name": "facebookresearch/segment-anything",
        "description": "The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.",
        "stars": 52837,
        "forks": 6170,
        "language": "Jupyter Notebook",
        "created_at": "2023-03-23T17:03:03+00:00",
        "updated_at": "2025-12-10T02:17:01+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2309.16588",
      "title": "Segment Anything in Medical Images",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2309.16588",
      "abstract": "Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing."
    },
    "repositories": [
      {
        "url": "https://github.com/bowang-lab/MedSAM",
        "name": "bowang-lab/MedSAM",
        "description": "Segment Anything in Medical Images",
        "stars": 3974,
        "forks": 546,
        "language": "Jupyter Notebook",
        "created_at": "2023-04-22T19:57:25+00:00",
        "updated_at": "2025-12-10T02:17:01+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1412.6980",
      "title": "Adam: A Method for Stochastic Optimization",
      "year": 2014,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1412.6980",
      "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
    },
    "repositories": [
      {
        "url": "https://github.com/pytorch/pytorch",
        "name": "pytorch/pytorch",
        "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "stars": 95727,
        "forks": 26159,
        "language": "Python",
        "created_at": "2016-08-13T05:26:41+00:00",
        "updated_at": "2025-12-10T03:21:16+00:00",
        "topics": [
          "neural-network",
          "autograd",
          "gpu",
          "numpy",
          "deep-learning",
          "tensor",
          "python",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1711.05101",
      "title": "Decoupled Weight Decay Regularization",
      "year": 2017,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1711.05101",
      "abstract": "L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it \"weight decay\" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW"
    },
    "repositories": [
      {
        "url": "https://github.com/pytorch/pytorch",
        "name": "pytorch/pytorch",
        "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "stars": 95727,
        "forks": 26159,
        "language": "Python",
        "created_at": "2016-08-13T05:26:41+00:00",
        "updated_at": "2025-12-10T03:21:16+00:00",
        "topics": [
          "neural-network",
          "autograd",
          "gpu",
          "numpy",
          "deep-learning",
          "tensor",
          "python",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2002.04745",
      "title": "Lookahead Optimizer: k steps forward, 1 step back",
      "year": 2020,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2002.04745",
      "abstract": "The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications."
    },
    "repositories": [
      {
        "url": "https://github.com/alphadl/lookahead.pytorch",
        "name": "alphadl/lookahead.pytorch",
        "description": "lookahead optimizer (Lookahead Optimizer: k steps forward, 1 step back) for pytorch ",
        "stars": 338,
        "forks": 64,
        "language": "Python",
        "created_at": "2019-07-25T11:15:29+00:00",
        "updated_at": "2025-12-08T06:06:14+00:00",
        "topics": [
          "pytorch",
          "optimizer",
          "lookahead"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1910.02054",
      "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
      "year": 2019,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1910.02054",
      "abstract": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy."
    },
    "repositories": [
      {
        "url": "https://github.com/deepspeedai/DeepSpeed",
        "name": "deepspeedai/DeepSpeed",
        "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
        "stars": 40956,
        "forks": 4657,
        "language": "Python",
        "created_at": "2020-01-23T18:35:18+00:00",
        "updated_at": "2025-12-10T02:14:27+00:00",
        "topics": [
          "deep-learning",
          "pytorch",
          "gpu",
          "machine-learning",
          "billion-parameters",
          "data-parallelism",
          "model-parallelism",
          "inference",
          "pipeline-parallelism",
          "compression",
          "mixture-of-experts",
          "trillion-parameters",
          "zero"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2205.05198",
      "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
      "year": 2022,
      "category": "cs.DC",
      "url": "https://arxiv.org/abs/2205.05198",
      "abstract": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."
    },
    "repositories": [
      {
        "url": "https://github.com/pytorch/pytorch",
        "name": "pytorch/pytorch",
        "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "stars": 95727,
        "forks": 26159,
        "language": "Python",
        "created_at": "2016-08-13T05:26:41+00:00",
        "updated_at": "2025-12-10T03:21:16+00:00",
        "topics": [
          "neural-network",
          "autograd",
          "gpu",
          "numpy",
          "deep-learning",
          "tensor",
          "python",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1301.3781",
      "title": "Efficient Estimation of Word Representations in Vector Space",
      "year": 2013,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1301.3781",
      "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities."
    },
    "repositories": [
      {
        "url": "https://github.com/tmikolov/word2vec",
        "name": "tmikolov/word2vec",
        "description": "Automatically exported from code.google.com/p/word2vec",
        "stars": 1572,
        "forks": 544,
        "language": "C",
        "created_at": "2015-08-12T11:04:28+00:00",
        "updated_at": "2025-12-04T15:35:42+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1607.01759",
      "title": "Enriching Word Vectors with Subword Information",
      "year": 2016,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1607.01759",
      "abstract": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fastText",
        "name": "facebookresearch/fastText",
        "description": "Library for fast text representation and classification.",
        "stars": 26445,
        "forks": 4815,
        "language": "HTML",
        "created_at": "2016-07-16T13:38:42+00:00",
        "updated_at": "2025-12-10T02:17:18+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2101.00027",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2101.00027",
      "abstract": "Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction."
    },
    "repositories": [
      {
        "url": "https://github.com/princeton-nlp/SimCSE",
        "name": "princeton-nlp/SimCSE",
        "description": "[EMNLP 2021] SimCSE: Simple Contrastive Learning of Sentence Embeddings https://arxiv.org/abs/2104.08821",
        "stars": 3623,
        "forks": 533,
        "language": "Python",
        "created_at": "2021-04-16T02:57:04+00:00",
        "updated_at": "2025-12-09T12:09:24+00:00",
        "topics": [
          "nlp",
          "sentence-embeddings"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2212.03533",
      "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
      "year": 2022,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2212.03533",
      "abstract": "This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters."
    },
    "repositories": [
      {
        "url": "https://github.com/microsoft/unilm",
        "name": "microsoft/unilm",
        "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities",
        "stars": 21873,
        "forks": 2681,
        "language": "Python",
        "created_at": "2019-07-23T04:15:28+00:00",
        "updated_at": "2025-12-09T22:08:32+00:00",
        "topics": [
          "nlp",
          "pre-trained-model",
          "unilm",
          "minilm",
          "layoutlm",
          "layoutxlm",
          "beit",
          "document-ai",
          "trocr",
          "beit-3",
          "foundation-models",
          "xlm-e",
          "deepnet",
          "llm",
          "multimodal",
          "mllm",
          "kosmos",
          "kosmos-1",
          "textdiffuser",
          "bitnet"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1905.09418",
      "title": "BERT Rediscovers the Classical NLP Pipeline",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1905.09418",
      "abstract": "Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU."
    },
    "repositories": [
      {
        "url": "https://github.com/clarkkev/attention-analysis",
        "name": "clarkkev/attention-analysis",
        "description": null,
        "stars": 469,
        "forks": 82,
        "language": "Jupyter Notebook",
        "created_at": "2019-06-07T21:36:34+00:00",
        "updated_at": "2025-11-21T09:42:57+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2004.03270",
      "title": "Analyzing the Structure of Attention in a Transformer Language Model",
      "year": 2020,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2004.03270",
      "abstract": "This article is a collection of several memories for a special issue of SIGMA devoted to Dmitry Borisovich Fuchs."
    },
    "repositories": [
      {
        "url": "https://github.com/clarkkev/attention-analysis",
        "name": "clarkkev/attention-analysis",
        "description": null,
        "stars": 469,
        "forks": 82,
        "language": "Jupyter Notebook",
        "created_at": "2019-06-07T21:36:34+00:00",
        "updated_at": "2025-11-21T09:42:57+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1904.12848",
      "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1904.12848",
      "abstract": "Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda."
    },
    "repositories": [
      {
        "url": "https://github.com/jasonwei20/eda_nlp",
        "name": "jasonwei20/eda_nlp",
        "description": "Data augmentation for NLP, presented at EMNLP 2019",
        "stars": 1649,
        "forks": 317,
        "language": "Python",
        "created_at": "2018-12-27T02:02:36+00:00",
        "updated_at": "2025-11-23T17:01:48+00:00",
        "topics": [
          "nlp",
          "data-augmentation",
          "text-classification",
          "synonyms",
          "embeddings",
          "sentence",
          "classification",
          "rnn",
          "cnn",
          "swap",
          "position"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2302.13007",
      "title": "Synthetic Data Generation with Large Language Models",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2302.13007",
      "abstract": "Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely-used strategy to mitigate such challenges is to perform data augmentation to better capture the data invariance and increase the sample size. However, current text data augmentation methods either can't ensure the correct labeling of the generated data (lacking faithfulness) or can't ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models, especially the development of ChatGPT, which demonstrated improved language comprehension abilities, in this work, we propose a text data augmentation approach based on ChatGPT (named AugGPT). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/google-research",
        "name": "google-research/google-research",
        "description": "Google Research",
        "stars": 36869,
        "forks": 8267,
        "language": "Jupyter Notebook",
        "created_at": "2018-10-04T18:42:48+00:00",
        "updated_at": "2025-12-10T03:17:21+00:00",
        "topics": [
          "machine-learning",
          "ai",
          "research"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.07185",
      "title": "LIMA: Less Is More for Alignment",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2305.07185",
      "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale."
    },
    "repositories": [
      {
        "url": "https://github.com/meta-llama/llama",
        "name": "meta-llama/llama",
        "description": "Inference code for Llama models",
        "stars": 58976,
        "forks": 9812,
        "language": "Python",
        "created_at": "2023-02-14T09:29:12+00:00",
        "updated_at": "2025-12-09T21:12:17+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2206.01861",
      "title": "LLM.int8(): 8-bit Matrix Multiplication for Transformers",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2206.01861",
      "abstract": "How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency."
    },
    "repositories": [
      {
        "url": "https://github.com/bitsandbytes-foundation/bitsandbytes",
        "name": "bitsandbytes-foundation/bitsandbytes",
        "description": "Accessible large language models via k-bit quantization for PyTorch.",
        "stars": 7804,
        "forks": 798,
        "language": "Python",
        "created_at": "2021-06-04T00:10:34+00:00",
        "updated_at": "2025-12-09T18:33:06+00:00",
        "topics": [
          "llm",
          "machine-learning",
          "pytorch",
          "qlora",
          "quantization"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2211.10438",
      "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization",
      "year": 2022,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2211.10438",
      "abstract": "Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant."
    },
    "repositories": [
      {
        "url": "https://github.com/mit-han-lab/smoothquant",
        "name": "mit-han-lab/smoothquant",
        "description": "[ICML 2023] SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
        "stars": 1567,
        "forks": 190,
        "language": "Python",
        "created_at": "2022-11-17T17:27:49+00:00",
        "updated_at": "2025-12-08T09:07:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1503.02531",
      "title": "Distilling the Knowledge in a Neural Network",
      "year": 2015,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1503.02531",
      "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
    },
    "repositories": [
      {
        "url": "https://github.com/haitongli/knowledge-distillation-pytorch",
        "name": "haitongli/knowledge-distillation-pytorch",
        "description": "A PyTorch implementation for exploring deep and shallow knowledge distillation (KD) experiments with flexibility",
        "stars": 1974,
        "forks": 351,
        "language": "Python",
        "created_at": "2018-03-09T23:58:31+00:00",
        "updated_at": "2025-12-04T08:09:58+00:00",
        "topics": [
          "pytorch",
          "knowledge-distillation",
          "deep-neural-networks",
          "cifar10",
          "model-compression",
          "dark-knowledge",
          "computer-vision"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1909.10351",
      "title": "DistilBERT: a distilled version of BERT",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1909.10351",
      "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny-BERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pretraining and task-specific learning stages. This framework ensures that TinyBERT can capture he general-domain as well as the task-specific knowledge in BERT.\n  TinyBERT with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERTBASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT with 4 layers is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only about 28% parameters and about 31% inference time of them. Moreover, TinyBERT with 6 layers performs on-par with its teacher BERTBASE."
    },
    "repositories": [
      {
        "url": "https://github.com/huggingface/transformers",
        "name": "huggingface/transformers",
        "description": "🤗 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
        "stars": 153664,
        "forks": 31358,
        "language": "Python",
        "created_at": "2018-10-29T13:56:00+00:00",
        "updated_at": "2025-12-10T03:34:07+00:00",
        "topics": [
          "nlp",
          "natural-language-processing",
          "pytorch",
          "pytorch-transformers",
          "transformer",
          "model-hub",
          "pretrained-models",
          "speech-recognition",
          "hacktoberfest",
          "python",
          "machine-learning",
          "deep-learning",
          "audio",
          "deepseek",
          "gemma",
          "glm",
          "llm",
          "qwen",
          "vlm"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1910.01108",
      "title": "TinyBERT: Distilling BERT for Natural Language Understanding",
      "year": 2019,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/1910.01108",
      "abstract": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
    },
    "repositories": [
      {
        "url": "https://github.com/huawei-noah/Pretrained-Language-Model",
        "name": "huawei-noah/Pretrained-Language-Model",
        "description": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
        "stars": 3150,
        "forks": 642,
        "language": "Python",
        "created_at": "2019-12-02T14:26:04+00:00",
        "updated_at": "2025-12-07T08:30:52+00:00",
        "topics": [
          "knowledge-distillation",
          "model-compression",
          "quantization",
          "pretrained-models",
          "large-scale-distributed"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1312.5602",
      "title": "Playing Atari with Deep Reinforcement Learning",
      "year": 2013,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1312.5602",
      "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
    },
    "repositories": [
      {
        "url": "https://github.com/google-deepmind/dqn",
        "name": "google-deepmind/dqn",
        "description": "Lua/Torch implementation of DQN (Nature, 2015)",
        "stars": 618,
        "forks": 165,
        "language": "Lua",
        "created_at": "2017-04-06T15:46:46+00:00",
        "updated_at": "2025-12-08T22:37:29+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1707.06347",
      "title": "Proximal Policy Optimization Algorithms",
      "year": 2017,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1707.06347",
      "abstract": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time."
    },
    "repositories": [
      {
        "url": "https://github.com/openai/baselines",
        "name": "openai/baselines",
        "description": "OpenAI Baselines: high-quality implementations of reinforcement learning algorithms",
        "stars": 16588,
        "forks": 4951,
        "language": "Python",
        "created_at": "2017-05-24T01:58:13+00:00",
        "updated_at": "2025-12-09T18:39:18+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1801.01290",
      "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning",
      "year": 2018,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1801.01290",
      "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds."
    },
    "repositories": [
      {
        "url": "https://github.com/rail-berkeley/softlearning",
        "name": "rail-berkeley/softlearning",
        "description": "Softlearning is a reinforcement learning framework for training maximum entropy policies in continuous domains. Includes the official implementation of the Soft Actor-Critic algorithm.",
        "stars": 1377,
        "forks": 250,
        "language": "Python",
        "created_at": "2018-12-03T05:55:54+00:00",
        "updated_at": "2025-12-09T10:52:27+00:00",
        "topics": [
          "reinforcement-learning",
          "soft-actor-critic",
          "deep-learning",
          "deep-reinforcement-learning",
          "deep-neural-networks",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1910.01741",
      "title": "Dream to Control: Learning Behaviors by Latent Imagination",
      "year": 2019,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1910.01741",
      "abstract": "Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and identify variational autoencoders, used by previous investigations, as the cause of the divergence. Following these findings, we propose effective techniques to improve training stability. This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home."
    },
    "repositories": [
      {
        "url": "https://github.com/danijar/dreamer",
        "name": "danijar/dreamer",
        "description": "Dream to Control: Learning Behaviors by Latent Imagination",
        "stars": 570,
        "forks": 113,
        "language": "Python",
        "created_at": "2020-01-27T18:19:56+00:00",
        "updated_at": "2025-12-07T05:07:45+00:00",
        "topics": [
          "reinforcement-learning",
          "artificial-intelligence",
          "deep-learning",
          "robotics",
          "world-models"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1703.04691",
      "title": "Neural Message Passing for Quantum Chemistry",
      "year": 2017,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1703.04691",
      "abstract": "We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, a ReLU activation function and conditioning is performed by applying multiple convolutional filters in parallel to separate time series which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally as well as conditionally for financial time series forecasting using the S&P500, the volatility index, the CBOE interest rate and several exchange rates and extensively compare it to the performance of the well-known autoregressive model and a long-short term memory network. We show that a convolutional network is well-suited for regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, is a time-efficient and easy to implement alternative to recurrent-type networks and tends to outperform linear and recurrent models."
    },
    "repositories": [
      {
        "url": "https://github.com/priba/nmp_qc",
        "name": "priba/nmp_qc",
        "description": "Our own implementation of Neural Message Passing for Computer Vision paper",
        "stars": 344,
        "forks": 84,
        "language": "Python",
        "created_at": "2017-04-19T19:10:59+00:00",
        "updated_at": "2025-11-26T07:39:04+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2305.10425",
      "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
      "year": 2023,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/2305.10425",
      "abstract": "Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice."
    },
    "repositories": [
      {
        "url": "https://github.com/KimMeen/Time-LLM",
        "name": "KimMeen/Time-LLM",
        "description": "[ICLR 2024] Official implementation of \" 🦙 Time-LLM: Time Series Forecasting by Reprogramming Large Language Models\"",
        "stars": 2410,
        "forks": 424,
        "language": "Python",
        "created_at": "2024-01-20T01:26:30+00:00",
        "updated_at": "2025-12-09T16:39:36+00:00",
        "topics": [
          "cross-modal-learning",
          "cross-modality",
          "deep-learning",
          "language-model",
          "large-language-models",
          "machine-learning",
          "multimodal-deep-learning",
          "multimodal-time-series",
          "prompt-tuning",
          "time-series",
          "time-series-analysis",
          "time-series-forecast",
          "time-series-forecasting"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1606.07792",
      "title": "Wide & Deep Learning for Recommender Systems",
      "year": 2016,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/1606.07792",
      "abstract": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow."
    },
    "repositories": [
      {
        "url": "https://github.com/tensorflow/models",
        "name": "tensorflow/models",
        "description": "Models and examples built with TensorFlow",
        "stars": 77680,
        "forks": 45382,
        "language": "Python",
        "created_at": "2016-02-05T01:15:20+00:00",
        "updated_at": "2025-12-09T12:19:31+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1708.05031",
      "title": "Neural Collaborative Filtering",
      "year": 2017,
      "category": "cs.IR",
      "url": "https://arxiv.org/abs/1708.05031",
      "abstract": "In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation -- collaborative filtering -- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering -- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance."
    },
    "repositories": [
      {
        "url": "https://github.com/hexiangnan/neural_collaborative_filtering",
        "name": "hexiangnan/neural_collaborative_filtering",
        "description": "Neural Collaborative Filtering",
        "stars": 1865,
        "forks": 666,
        "language": "Python",
        "created_at": "2017-02-02T02:55:58+00:00",
        "updated_at": "2025-12-04T15:04:17+00:00",
        "topics": [
          "deep-learning",
          "recommender-system",
          "collaborative-filtering"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1603.04467",
      "title": "TensorFlow: A System for Large-Scale Machine Learning",
      "year": 2016,
      "category": "cs.DC",
      "url": "https://arxiv.org/abs/1603.04467",
      "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
    },
    "repositories": [
      {
        "url": "https://github.com/tensorflow/tensorflow",
        "name": "tensorflow/tensorflow",
        "description": "An Open Source Machine Learning Framework for Everyone",
        "stars": 192725,
        "forks": 75036,
        "language": "C++",
        "created_at": "2015-11-07T01:19:20+00:00",
        "updated_at": "2025-12-10T03:12:49+00:00",
        "topics": [
          "tensorflow",
          "machine-learning",
          "python",
          "deep-learning",
          "deep-neural-networks",
          "neural-network",
          "ml",
          "distributed"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1912.01703",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "year": 2019,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1912.01703",
      "abstract": "Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.\n  In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\n  We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks."
    },
    "repositories": [
      {
        "url": "https://github.com/pytorch/pytorch",
        "name": "pytorch/pytorch",
        "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
        "stars": 95727,
        "forks": 26159,
        "language": "Python",
        "created_at": "2016-08-13T05:26:41+00:00",
        "updated_at": "2025-12-10T03:21:16+00:00",
        "topics": [
          "neural-network",
          "autograd",
          "gpu",
          "numpy",
          "deep-learning",
          "tensor",
          "python",
          "machine-learning"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1902.05188",
      "title": "JAX: Composable transformations of Python+NumPy programs",
      "year": 2019,
      "category": "cs.MS",
      "url": "https://arxiv.org/abs/1902.05188",
      "abstract": "Future radio surveys will generate catalogues of tens of millions of radio sources, for which redshift estimates will be essential to achieve many of the science goals. However, spectroscopic data will be available for only a small fraction of these sources, and in most cases even the optical and infrared photometry will be of limited quality. Furthermore, radio sources tend to be at higher redshift than most optical sources and so a significant fraction of radio sources hosts differ from those for which most photometric redshift templates are designed. We therefore need to develop new techniques for estimating the redshifts of radio sources. As a starting point in this process, we evaluate a number of machine-learning techniques for estimating redshift, together with a conventional template-fitting technique. We pay special attention to how the performance is affected by the incompleteness of the training sample and by sparseness of the parameter space or by limited availability of ancillary multi-wavelength data. As expected, we find that the quality of the photometric-redshift degrades as the quality of the photometry decreases, but that even with the limited quality of photometry available for all sky-surveys, useful redshift information is available for the majority of sources, particularly at low redshift. We find that a template-fitting technique performs best with high-quality and almost complete multi-band photometry, especially if radio sources that are also X-ray emitting are treated separately. When we reduced the quality of photometry to match that available for the EMU all-sky radio survey, the quality of the template-fitting degraded and became comparable to some of the machine learning methods. Machine learning techniques currently perform better at low redshift than at high redshift, because of incompleteness of the currently available training data at high redshifts."
    },
    "repositories": [
      {
        "url": "https://github.com/jax-ml/jax",
        "name": "jax-ml/jax",
        "description": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more",
        "stars": 34268,
        "forks": 3294,
        "language": "Python",
        "created_at": "2018-10-25T21:25:02+00:00",
        "updated_at": "2025-12-10T01:50:23+00:00",
        "topics": [
          "jax"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1711.00489",
      "title": "Horovod: fast and easy distributed deep learning in TensorFlow",
      "year": 2017,
      "category": "cs.LG",
      "url": "https://arxiv.org/abs/1711.00489",
      "abstract": "It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate $ε$ and scaling the batch size $B \\propto ε$. Finally, one can increase the momentum coefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under 30 minutes."
    },
    "repositories": [
      {
        "url": "https://github.com/horovod/horovod",
        "name": "horovod/horovod",
        "description": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.",
        "stars": 14642,
        "forks": 2257,
        "language": "Python",
        "created_at": "2017-08-09T19:39:59+00:00",
        "updated_at": "2025-12-09T16:11:25+00:00",
        "topics": [
          "tensorflow",
          "uber",
          "machine-learning",
          "machinelearning",
          "mpi",
          "baidu",
          "deep-learning",
          "deeplearning",
          "keras",
          "pytorch",
          "mxnet",
          "spark",
          "ray"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1506.02640",
      "title": "Faster R-CNN: Towards Real-Time Object Detection",
      "year": 2015,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1506.02640",
      "abstract": "We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset."
    },
    "repositories": [
      {
        "url": "https://github.com/rbgirshick/py-faster-rcnn",
        "name": "rbgirshick/py-faster-rcnn",
        "description": "Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version",
        "stars": 8278,
        "forks": 4104,
        "language": "Python",
        "created_at": "2015-09-25T21:04:08+00:00",
        "updated_at": "2025-12-10T02:16:59+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "1612.03144",
      "title": "Feature Pyramid Networks for Object Detection",
      "year": 2016,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/1612.03144",
      "abstract": "Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/Detectron",
        "name": "facebookresearch/Detectron",
        "description": "FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.",
        "stars": 26396,
        "forks": 5435,
        "language": "Python",
        "created_at": "2017-10-05T17:32:00+00:00",
        "updated_at": "2025-12-09T14:24:09+00:00",
        "topics": []
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2005.12872",
      "title": "EfficientDet: Scalable and Efficient Object Detection",
      "year": 2020,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2005.12872",
      "abstract": "We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr."
    },
    "repositories": [
      {
        "url": "https://github.com/google/automl",
        "name": "google/automl",
        "description": "Google Brain AutoML",
        "stars": 6430,
        "forks": 1460,
        "language": "Jupyter Notebook",
        "created_at": "2020-03-12T03:52:47+00:00",
        "updated_at": "2025-12-09T00:37:46+00:00",
        "topics": [
          "automl",
          "efficientdet",
          "object-detection",
          "efficientnet",
          "efficientnetv2"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2303.11331",
      "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training",
      "year": 2023,
      "category": "cs.CV",
      "url": "https://arxiv.org/abs/2303.11331",
      "abstract": "We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open & accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest & best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive performance. To facilitate open access and open research, we release the complete suite of EVA-02 to the community at https://github.com/baaivision/EVA/tree/master/EVA-02."
    },
    "repositories": [
      {
        "url": "https://github.com/IDEA-Research/GroundingDINO",
        "name": "IDEA-Research/GroundingDINO",
        "description": "[ECCV 2024] Official implementation of the paper \"Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection\"",
        "stars": 9401,
        "forks": 976,
        "language": "Python",
        "created_at": "2023-03-09T06:14:41+00:00",
        "updated_at": "2025-12-10T02:33:13+00:00",
        "topics": [
          "object-detection",
          "open-world",
          "open-world-detection",
          "vision-language",
          "vision-language-transformer"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2106.07447",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2106.07447",
      "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2109.04908",
      "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
      "year": 2021,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2109.04908",
      "abstract": "This paper addresses the issues of unmanned aerial vehicle (UAV) indoor navigation, specifically in areas where GPS and magnetometer sensor measurements are unavailable or unreliable. The proposed solution is to use an error state extended Kalman filter (ES -EKF) in the context of multi-sensor fusion. Its implementation is adapted to fuse measurements from multiple sensor sources and the state model is extended to account for sensor drift and possible calibration inaccuracies. Experimental validation is performed by fusing IMU data obtained from the PixHawk 2.1 flight controller with pose measurements from LiDAR Cartographer SLAM, visual odometry provided by the Intel T265 camera and position measurements from the Pozyx UWB indoor positioning system. The estimated odometry from ES-EKF is validated against ground truth data from the Optitrack motion capture system and its use in a position control loop to stabilize the UAV is demonstrated."
    },
    "repositories": [
      {
        "url": "https://github.com/facebookresearch/fairseq",
        "name": "facebookresearch/fairseq",
        "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
        "stars": 32022,
        "forks": 6633,
        "language": "Python",
        "created_at": "2017-08-29T16:26:12+00:00",
        "updated_at": "2025-12-09T11:59:54+00:00",
        "topics": [
          "python",
          "pytorch",
          "artificial-intelligence"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2307.00109",
      "title": "Med-PaLM 2: Towards Expert-Level Medical Question Answering",
      "year": 2023,
      "category": "cs.CL",
      "url": "https://arxiv.org/abs/2307.00109",
      "abstract": "The goal of this paper is to present an approach to Hod Pair Capturing (HPC). $HPC$ is the most outstanding open problem of descriptive inner model theory. More specifically, we introduce two principles, the Direct Limit Independence and the Bounded Direct Limits, and show that they together imply HPC."
    },
    "repositories": [
      {
        "url": "https://github.com/google-research/google-research",
        "name": "google-research/google-research",
        "description": "Google Research",
        "stars": 36869,
        "forks": 8267,
        "language": "Jupyter Notebook",
        "created_at": "2018-10-04T18:42:48+00:00",
        "updated_at": "2025-12-10T03:17:21+00:00",
        "topics": [
          "machine-learning",
          "ai",
          "research"
        ]
      }
    ]
  },
  {
    "paper": {
      "arxiv_id": "2306.04190",
      "title": "ChemCrow: Augmenting large-language models with chemistry tools",
      "year": 2023,
      "category": "cs.AI",
      "url": "https://arxiv.org/abs/2306.04190",
      "abstract": "The interest in employing automatic speech recognition (ASR) in applications for reading practice has been growing in recent years. In a previous study, we presented an ASR-based Dutch reading tutor application that was developed to provide instantaneous feedback to first-graders learning to read. We saw that ASR has potential at this stage of the reading process, as the results suggested that pupils made progress in reading accuracy and fluency by using the software. In the current study, we used children's speech from an existing corpus (JASMIN) to develop two new ASR systems, and compared the results to those of the previous study. We analyze correct/incorrect classification of the ASR systems using human transcripts at word level, by means of evaluation measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC), precision, recall and F-measures. We observe improvements for the newly developed ASR systems regarding the agreement with human-based judgment and correct rejection (CR). The accuracy of the ASR systems varies for different reading tasks and word types. Our results suggest that, in the current configuration, it is difficult to classify isolated words. We discuss these results, possible ways to improve our systems and avenues for future research."
    },
    "repositories": [
      {
        "url": "https://github.com/ur-whitelab/chemcrow-public",
        "name": "ur-whitelab/chemcrow-public",
        "description": "Chemcrow",
        "stars": 845,
        "forks": 134,
        "language": "Python",
        "created_at": "2023-06-04T15:59:05+00:00",
        "updated_at": "2025-12-05T03:11:56+00:00",
        "topics": []
      }
    ]
  }
]