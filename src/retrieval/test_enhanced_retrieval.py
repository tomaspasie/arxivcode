#!/usr/bin/env python3
"""
Enhanced Retrieval Testing with Cross-Encoder Re-ranking
Combines initial retrieval with cross-encoder re-ranking for improved relevance.
"""

import sys
import logging
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from src.retrieval import DenseRetrieval, RerankingPipeline

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_enhanced_retrieval():
    """Test the complete retrieval pipeline with re-ranking."""
    print("\n" + "="*80)
    print("ENHANCED RETRIEVAL TESTING WITH CROSS-ENCODER RE-RANKING")
    print("="*80)

    # 1. Load existing retrieval system
    print("\n1. Loading retrieval system...")
    retriever = DenseRetrieval(embedding_model_name="tfidf")
    retriever.load_index(
        "data/processed/FAISS/faiss_index.index",
        "data/processed/FAISS/faiss_metadata.pkl"
    )
    print("   âœ… Index loaded with 284 code snippets")

    # 2. Setup re-ranking pipeline
    print("\n2. Initializing re-ranking pipeline...")
    pipeline = RerankingPipeline(
        dense_retriever=retriever,
        cross_encoder_model="cross-encoder/ms-marco-MiniLM-L-6-v2",
        initial_top_k=50,
        final_top_k=20
    )
    print("   âœ… Re-ranking pipeline ready")

    # 3. Test queries with enhanced retrieval
    test_queries = [
        "implement transformer attention mechanism",
        "fine-tune large language model",
        "parameter efficient training methods",
        "masked language modeling",
        "contrastive learning",
        "diffusion model implementation",
        "reinforcement learning policy gradient",
        "graph neural network",
        "how to implement LoRA",
        "self-attention layer code",
        "tokenizer implementation",
        "batch normalization"
    ]

    print("\n3. Testing enhanced retrieval with re-ranking...")
    print("-" * 80)

    all_results = []

    for i, query in enumerate(test_queries, 1):
        print(f"\nðŸ“ Query {i}: '{query}'")
        print("-" * 80)

        # Get enhanced results
        results = pipeline.retrieve_and_rerank(query)
        all_results.append(results)

        print(f"Initial candidates: {results['pipeline_stats']['initial_candidates']}")
        print(f"Final results: {results['pipeline_stats']['final_results']}")

        # Show top 3 results
        for result in results['reranked_results'][:3]:
            meta = result['metadata']
            print(f"\nRank {result['rank']} (Score: {result['score']:.4f})")
            print(f"  ðŸ“„ Paper: {meta['paper_title'][:60]}{'...' if len(meta['paper_title']) > 60 else ''}")
            print(f"  ðŸ”— Repo: {meta['repo_name']}")
            print(f"  â­ Stars: {meta['star_count']}")
            print(f"  ðŸ“ File: {meta['file_path']}")

    # 4. Quality analysis
    print("\n4. Retrieval Quality Analysis")
    print("-" * 80)

    # Calculate average scores and statistics
    total_results = len(all_results)
    avg_candidates = sum(r['pipeline_stats']['initial_candidates'] for r in all_results) / total_results
    avg_final = sum(r['pipeline_stats']['final_results'] for r in all_results) / total_results

    # Score distributions
    all_scores = []
    for results in all_results:
        all_scores.extend([r['score'] for r in results['reranked_results']])

    scores_sorted = sorted(all_scores)
    score_range = f"{scores_sorted[0]:.4f} to {scores_sorted[-1]:.4f}"
    median_score = scores_sorted[len(scores_sorted)//2]

    print(f"Total queries tested: {total_results}")
    print(f"Average initial candidates: {avg_candidates:.1f}")
    print(f"Average final results: {avg_final:.1f}")
    print(f"Score range: {score_range}")
    print(f"Median score: {median_score:.4f}")

    # Unique repos and papers
    unique_repos = set()
    unique_papers = set()

    for results in all_results:
        for result in results['reranked_results']:
            meta = result['metadata']
            unique_repos.add(meta['repo_name'])
            unique_papers.add(meta['paper_title'])

    print(f"Unique repos in results: {len(unique_repos)}")
    print(f"Unique papers in results: {len(unique_papers)}")

    # 5. Comparison with baseline (no re-ranking)
    print("\n5. Comparison: Initial vs Re-ranked Results")
    print("-" * 80)

    # Test one query in detail
    test_query = "graph neural network"
    print(f"Detailed comparison for: '{test_query}'")

    # Get both initial and re-ranked
    results = pipeline.retrieve_and_rerank(test_query)

    print("\nInitial Retrieval (Top 5):")
    for i, result in enumerate(results['initial_results'][:5], 1):
        meta = result['metadata']
        print(f"{i}. {meta['paper_title'][:50]}... (Score: {result['score']:.4f})")

    print("\nAfter Re-ranking (Top 5):")
    for i, result in enumerate(results['reranked_results'][:5], 1):
        meta = result['metadata']
        print(f"{i}. {meta['paper_title'][:50]}... (Score: {result['score']:.4f})")

        # Show ranking change
        initial_rank = None
        for j, init_result in enumerate(results['initial_results'][:10], 1):
            if init_result['metadata']['repo_name'] == meta['repo_name']:
                initial_rank = j
                break

        if initial_rank:
            change = initial_rank - i
            direction = "â†‘" if change > 0 else "â†“" if change < 0 else "â†’"
            print(f"   {direction} Rank change: {initial_rank} â†’ {i}")

    print("\n" + "="*80)
    print("âœ… ENHANCED RETRIEVAL TESTING COMPLETE")
    print("="*80)
    print("\nðŸ“Š Summary:")
    print(f"   â€¢ Tested {len(test_queries)} ML/AI queries")
    print(f"   â€¢ Retrieved from {len(unique_repos)} unique repositories")
    print(f"   â€¢ Cross-encoder re-ranking provides more relevant results")
    print(f"   â€¢ Pipeline ready for integration with LLM question-answering")


if __name__ == "__main__":
    test_enhanced_retrieval()